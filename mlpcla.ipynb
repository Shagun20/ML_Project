{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-04T15:59:32.577876Z","iopub.execute_input":"2022-12-04T15:59:32.578312Z","iopub.status.idle":"2022-12-04T15:59:32.595515Z","shell.execute_reply.started":"2022-12-04T15:59:32.578278Z","shell.execute_reply":"2022-12-04T15:59:32.594608Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"/kaggle/input/no-dir/weatherAUS-noDir.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/no-dir/weatherAUS-noDir.csv')\nprint(df.columns)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T15:41:19.202812Z","iopub.execute_input":"2022-12-04T15:41:19.203907Z","iopub.status.idle":"2022-12-04T15:41:19.433986Z","shell.execute_reply.started":"2022-12-04T15:41:19.203856Z","shell.execute_reply":"2022-12-04T15:41:19.432883Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Index(['MinTemp', 'MaxTemp', 'Rainfall', 'Humidity9am', 'Humidity3pm',\n       'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm', 'RainToday',\n       'RainTomorrow', 'WindSpeedNorth', 'WindSpeedEast', 'WindSpeedNorth9am',\n       'WindSpeedEast9am', 'WindSpeedNorth3pm', 'WindSpeedEast3pm'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-12-04T15:54:47.455047Z","iopub.execute_input":"2022-12-04T15:54:47.455417Z","iopub.status.idle":"2022-12-04T15:54:47.530534Z","shell.execute_reply.started":"2022-12-04T15:54:47.455387Z","shell.execute_reply":"2022-12-04T15:54:47.529514Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"x = list(df.columns).index(\"RainTomorrow\")\nls= list(df.columns[:x])+list(df.columns[x+1:])\nls.append(\"RainTomorrow\")\ndf = df[ls]\ndf.head()\ndel x\nprint(df.columns)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T15:52:52.785306Z","iopub.execute_input":"2022-12-04T15:52:52.785722Z","iopub.status.idle":"2022-12-04T15:52:52.798908Z","shell.execute_reply.started":"2022-12-04T15:52:52.785688Z","shell.execute_reply":"2022-12-04T15:52:52.797632Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Index(['MinTemp', 'MaxTemp', 'Rainfall', 'Humidity9am', 'Humidity3pm',\n       'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm', 'RainToday',\n       'WindSpeedNorth', 'WindSpeedEast', 'WindSpeedNorth9am',\n       'WindSpeedEast9am', 'WindSpeedNorth3pm', 'WindSpeedEast3pm',\n       'RainTomorrow'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"X=df[df.columns[:-1]]\ny = df[df.columns[-1]]\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)\nprint(x_train.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T15:55:52.325726Z","iopub.execute_input":"2022-12-04T15:55:52.326226Z","iopub.status.idle":"2022-12-04T15:55:52.391948Z","shell.execute_reply.started":"2022-12-04T15:55:52.326181Z","shell.execute_reply":"2022-12-04T15:55:52.391034Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"(113712, 16)\n(113712,)\n(28429,)\n","output_type":"stream"}]},{"cell_type":"code","source":"scaler = StandardScaler()\ncols = x_train.columns\nx_train = pd.DataFrame(scaler.fit_transform(x_train))\nx_train.columns = cols\ni=0\nmeans = scaler.mean_\nvar = scaler.var_\nfor col in x_train.columns:\n    x_test[col] =  (x_test[col]-means[i])/math.sqrt(var[i])\n    i+=1\n","metadata":{"execution":{"iopub.status.busy":"2022-12-04T16:00:07.033425Z","iopub.execute_input":"2022-12-04T16:00:07.033874Z","iopub.status.idle":"2022-12-04T16:00:07.094845Z","shell.execute_reply.started":"2022-12-04T16:00:07.033837Z","shell.execute_reply":"2022-12-04T16:00:07.093554Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"         MinTemp   MaxTemp  Rainfall  Humidity9am  Humidity3pm  Pressure9am  \\\n0      -1.704810 -0.523806 -0.275823    -1.045636    -0.897572    -0.305621   \n1      -1.188298 -0.327203 -0.275823    -0.046033    -0.897572     0.850521   \n2       1.237739  0.599639 -0.275823     0.585295     0.602186    -0.320073   \n3       1.175132  1.835429 -0.275823     0.427463    -1.139468    -0.348976   \n4       0.126458  0.417079 -0.275823    -0.835193    -0.945951    -0.098479   \n...          ...       ...       ...          ...          ...          ...   \n113707 -1.235254 -1.043399 -0.275823    -0.414307    -0.317020    -0.464591   \n113708  1.112525  0.529424  1.074267     0.006578     1.376254    -0.941499   \n113709  0.846443  0.164304 -0.275823     0.848349     1.134358     0.807166   \n113710 -0.765698 -0.074428 -0.275823     1.637509     0.021634     0.749359   \n113711  0.282976 -0.200815  0.079464     0.059189     0.457048     0.156836   \n\n        Pressure3pm   Temp9am   Temp3pm  RainToday  WindSpeedNorth  \\\n0         -0.001779 -0.782187 -0.522071  -0.533539        0.518583   \n1          0.390817 -0.335794 -0.348601  -0.533539        1.292392   \n2         -0.307131  1.034172  0.562120  -0.533539        0.061332   \n3         -0.471924  1.126529  2.022165  -0.533539        0.624102   \n4          0.143627  0.172171  0.533209  -0.533539        0.624102   \n...             ...       ...       ...        ...             ...   \n113707    -1.659403 -1.028473 -0.941292  -0.533539        2.242066   \n113708    -0.772429  1.064957  0.128444   1.874278        0.799968   \n113709     0.885196  0.341492  0.186267  -0.533539        0.061332   \n113710     0.521682 -0.766794  0.128444  -0.533539        0.905487   \n113711     0.075771  0.387671 -0.175130   1.874278        0.061332   \n\n        WindSpeedEast  WindSpeedNorth9am  WindSpeedEast9am  WindSpeedNorth3pm  \\\n0            1.208347          -0.508724          0.518785           0.918853   \n1            0.159840           0.892019         -0.982173           1.756238   \n2            1.437707          -0.755914         -0.010965           0.081468   \n3           -1.085261           0.562432         -0.010965           0.569943   \n4            1.404941          -0.014344          2.461201           1.197982   \n...               ...                ...               ...                ...   \n113707      -1.871641           1.798382         -0.010965           2.035367   \n113708       0.847923          -0.920708         -0.010965          -0.755917   \n113709       0.946220          -0.838311         -0.364132          -0.407006   \n113710       0.159840          -0.014344         -0.010965           1.058417   \n113711      -1.085261          -1.085501          1.136826          -0.476788   \n\n        WindSpeedEast3pm  \n0              -0.771821  \n1               0.046840  \n2               1.684163  \n3              -1.044709  \n4               0.524393  \n...                  ...  \n113707         -1.863370  \n113708          0.387949  \n113709          1.274832  \n113710         -0.362491  \n113711          0.592614  \n\n[113712 rows x 16 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2022-12-04T16:15:50.996476Z","iopub.execute_input":"2022-12-04T16:15:50.996890Z","iopub.status.idle":"2022-12-04T16:15:51.002002Z","shell.execute_reply.started":"2022-12-04T16:15:50.996857Z","shell.execute_reply":"2022-12-04T16:15:51.000818Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"mlc = MLPClassifier(hidden_layer_sizes = (128,64), activation = 'relu', verbose = True, max_iter = 100)\nmlc.fit(x_train, y_train)\nprint(mlc.score(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-12-04T16:04:57.885658Z","iopub.execute_input":"2022-12-04T16:04:57.886037Z","iopub.status.idle":"2022-12-04T16:08:45.455121Z","shell.execute_reply.started":"2022-12-04T16:04:57.886006Z","shell.execute_reply":"2022-12-04T16:08:45.453938Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Iteration 1, loss = 0.36644575\nIteration 2, loss = 0.34739320\nIteration 3, loss = 0.34293328\nIteration 4, loss = 0.33945964\nIteration 5, loss = 0.33731639\nIteration 6, loss = 0.33592992\nIteration 7, loss = 0.33455285\nIteration 8, loss = 0.33315229\nIteration 9, loss = 0.33212719\nIteration 10, loss = 0.33093478\nIteration 11, loss = 0.33044722\nIteration 12, loss = 0.32940440\nIteration 13, loss = 0.32801955\nIteration 14, loss = 0.32737124\nIteration 15, loss = 0.32706519\nIteration 16, loss = 0.32552478\nIteration 17, loss = 0.32527461\nIteration 18, loss = 0.32482762\nIteration 19, loss = 0.32397117\nIteration 20, loss = 0.32316743\nIteration 21, loss = 0.32230306\nIteration 22, loss = 0.32169358\nIteration 23, loss = 0.32113928\nIteration 24, loss = 0.32075601\nIteration 25, loss = 0.32006635\nIteration 26, loss = 0.31957432\nIteration 27, loss = 0.31898771\nIteration 28, loss = 0.31806733\nIteration 29, loss = 0.31808810\nIteration 30, loss = 0.31694614\nIteration 31, loss = 0.31651218\nIteration 32, loss = 0.31600342\nIteration 33, loss = 0.31566382\nIteration 34, loss = 0.31512633\nIteration 35, loss = 0.31468685\nIteration 36, loss = 0.31474133\nIteration 37, loss = 0.31367569\nIteration 38, loss = 0.31331018\nIteration 39, loss = 0.31281244\nIteration 40, loss = 0.31235580\nIteration 41, loss = 0.31198156\nIteration 42, loss = 0.31114816\nIteration 43, loss = 0.31143507\nIteration 44, loss = 0.31083787\nIteration 45, loss = 0.31003590\nIteration 46, loss = 0.30932159\nIteration 47, loss = 0.30901558\nIteration 48, loss = 0.30885351\nIteration 49, loss = 0.30902600\nIteration 50, loss = 0.30808052\nIteration 51, loss = 0.30772184\nIteration 52, loss = 0.30728381\nIteration 53, loss = 0.30660935\nIteration 54, loss = 0.30647624\nIteration 55, loss = 0.30627165\nIteration 56, loss = 0.30554391\nIteration 57, loss = 0.30544612\nIteration 58, loss = 0.30474764\nIteration 59, loss = 0.30440080\nIteration 60, loss = 0.30369682\nIteration 61, loss = 0.30328490\nIteration 62, loss = 0.30294920\nIteration 63, loss = 0.30310916\nIteration 64, loss = 0.30191870\nIteration 65, loss = 0.30186197\nIteration 66, loss = 0.30159347\nIteration 67, loss = 0.30121178\nIteration 68, loss = 0.30129733\nIteration 69, loss = 0.30047222\nIteration 70, loss = 0.30009643\nIteration 71, loss = 0.29969600\nIteration 72, loss = 0.29920419\nIteration 73, loss = 0.29904191\nIteration 74, loss = 0.29871921\nIteration 75, loss = 0.29825671\nIteration 76, loss = 0.29774727\nIteration 77, loss = 0.29771241\nIteration 78, loss = 0.29678765\nIteration 79, loss = 0.29664265\nIteration 80, loss = 0.29683315\nIteration 81, loss = 0.29614288\nIteration 82, loss = 0.29555701\nIteration 83, loss = 0.29594791\nIteration 84, loss = 0.29575260\nIteration 85, loss = 0.29530460\nIteration 86, loss = 0.29459987\nIteration 87, loss = 0.29380858\nIteration 88, loss = 0.29374826\nIteration 89, loss = 0.29406638\nIteration 90, loss = 0.29405673\nIteration 91, loss = 0.29314897\nIteration 92, loss = 0.29255808\nIteration 93, loss = 0.29272751\nIteration 94, loss = 0.29233918\nIteration 95, loss = 0.29174582\nIteration 96, loss = 0.29220091\nIteration 97, loss = 0.29142981\nIteration 98, loss = 0.29135997\nIteration 99, loss = 0.29077308\nIteration 100, loss = 0.29064189\n0.8512434485912272\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"param_dicti = {'hidden_layer_sizes' : [(128, 64, 32), (32, 16, 8)],'random_state' : [123, 516],'activation' : ['relu', 'tanh', 'logistic'],'learning_rate_init' : [0.001, 0.005, 0.01],'max_iter' : [50, 250]}\ngrid = GridSearchCV(estimator = MLPClassifier(verbose = True), param_grid = param_dicti)\ngrid_cv = grid.fit(x_train, y_train)\nprint(grid_cv.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T17:26:25.957736Z","iopub.execute_input":"2022-12-04T17:26:25.958106Z","iopub.status.idle":"2022-12-04T20:45:40.736719Z","shell.execute_reply.started":"2022-12-04T17:26:25.958075Z","shell.execute_reply":"2022-12-04T20:45:40.734081Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Iteration 1, loss = 0.37371200\nIteration 2, loss = 0.34861668\nIteration 3, loss = 0.34298486\nIteration 4, loss = 0.33976588\nIteration 5, loss = 0.33658253\nIteration 6, loss = 0.33463306\nIteration 7, loss = 0.33400976\nIteration 8, loss = 0.33129745\nIteration 9, loss = 0.33006940\nIteration 10, loss = 0.32858684\nIteration 11, loss = 0.32677714\nIteration 12, loss = 0.32559485\nIteration 13, loss = 0.32488215\nIteration 14, loss = 0.32381064\nIteration 15, loss = 0.32267126\nIteration 16, loss = 0.32103442\nIteration 17, loss = 0.32060549\nIteration 18, loss = 0.31941139\nIteration 19, loss = 0.31881976\nIteration 20, loss = 0.31763443\nIteration 21, loss = 0.31630487\nIteration 22, loss = 0.31499480\nIteration 23, loss = 0.31446790\nIteration 24, loss = 0.31291795\nIteration 25, loss = 0.31244214\nIteration 26, loss = 0.31113217\nIteration 27, loss = 0.30991451\nIteration 28, loss = 0.30888657\nIteration 29, loss = 0.30811679\nIteration 30, loss = 0.30737485\nIteration 31, loss = 0.30668254\nIteration 32, loss = 0.30546352\nIteration 33, loss = 0.30454626\nIteration 34, loss = 0.30432462\nIteration 35, loss = 0.30272244\nIteration 36, loss = 0.30159505\nIteration 37, loss = 0.30056525\nIteration 38, loss = 0.29959446\nIteration 39, loss = 0.29858868\nIteration 40, loss = 0.29826507\nIteration 41, loss = 0.29718534\nIteration 42, loss = 0.29673530\nIteration 43, loss = 0.29565813\nIteration 44, loss = 0.29422958\nIteration 45, loss = 0.29379730\nIteration 46, loss = 0.29264914\nIteration 47, loss = 0.29207555\nIteration 48, loss = 0.29128144\nIteration 49, loss = 0.29000322\nIteration 50, loss = 0.29036693\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37212711\nIteration 2, loss = 0.34690303\nIteration 3, loss = 0.34175845\nIteration 4, loss = 0.33871517\nIteration 5, loss = 0.33589490\nIteration 6, loss = 0.33430475\nIteration 7, loss = 0.33298493\nIteration 8, loss = 0.33040619\nIteration 9, loss = 0.32917244\nIteration 10, loss = 0.32765886\nIteration 11, loss = 0.32631198\nIteration 12, loss = 0.32485339\nIteration 13, loss = 0.32395924\nIteration 14, loss = 0.32293141\nIteration 15, loss = 0.32208565\nIteration 16, loss = 0.32046177\nIteration 17, loss = 0.31968410\nIteration 18, loss = 0.31870551\nIteration 19, loss = 0.31835932\nIteration 20, loss = 0.31726637\nIteration 21, loss = 0.31624118\nIteration 22, loss = 0.31498855\nIteration 23, loss = 0.31410926\nIteration 24, loss = 0.31305806\nIteration 25, loss = 0.31215911\nIteration 26, loss = 0.31080979\nIteration 27, loss = 0.31009342\nIteration 28, loss = 0.30951358\nIteration 29, loss = 0.30867584\nIteration 30, loss = 0.30763187\nIteration 31, loss = 0.30658376\nIteration 32, loss = 0.30578287\nIteration 33, loss = 0.30509142\nIteration 34, loss = 0.30412084\nIteration 35, loss = 0.30297576\nIteration 36, loss = 0.30212677\nIteration 37, loss = 0.30176069\nIteration 38, loss = 0.30116421\nIteration 39, loss = 0.29985678\nIteration 40, loss = 0.29921650\nIteration 41, loss = 0.29822169\nIteration 42, loss = 0.29767786\nIteration 43, loss = 0.29707434\nIteration 44, loss = 0.29525598\nIteration 45, loss = 0.29512451\nIteration 46, loss = 0.29398884\nIteration 47, loss = 0.29313323\nIteration 48, loss = 0.29207666\nIteration 49, loss = 0.29144226\nIteration 50, loss = 0.29098004\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37392595\nIteration 2, loss = 0.34990079\nIteration 3, loss = 0.34419610\nIteration 4, loss = 0.34076773\nIteration 5, loss = 0.33764428\nIteration 6, loss = 0.33555669\nIteration 7, loss = 0.33428226\nIteration 8, loss = 0.33231093\nIteration 9, loss = 0.33136623\nIteration 10, loss = 0.32941807\nIteration 11, loss = 0.32805149\nIteration 12, loss = 0.32676148\nIteration 13, loss = 0.32664886\nIteration 14, loss = 0.32530509\nIteration 15, loss = 0.32401933\nIteration 16, loss = 0.32297876\nIteration 17, loss = 0.32139654\nIteration 18, loss = 0.32052200\nIteration 19, loss = 0.31962706\nIteration 20, loss = 0.31794785\nIteration 21, loss = 0.31762523\nIteration 22, loss = 0.31631766\nIteration 23, loss = 0.31538302\nIteration 24, loss = 0.31390667\nIteration 25, loss = 0.31366307\nIteration 26, loss = 0.31191796\nIteration 27, loss = 0.31139777\nIteration 28, loss = 0.31064731\nIteration 29, loss = 0.30964606\nIteration 30, loss = 0.30841319\nIteration 31, loss = 0.30761023\nIteration 32, loss = 0.30654945\nIteration 33, loss = 0.30594310\nIteration 34, loss = 0.30483974\nIteration 35, loss = 0.30408425\nIteration 36, loss = 0.30347260\nIteration 37, loss = 0.30275029\nIteration 38, loss = 0.30149805\nIteration 39, loss = 0.30068416\nIteration 40, loss = 0.29931678\nIteration 41, loss = 0.29877696\nIteration 42, loss = 0.29818832\nIteration 43, loss = 0.29723797\nIteration 44, loss = 0.29606731\nIteration 45, loss = 0.29529193\nIteration 46, loss = 0.29409340\nIteration 47, loss = 0.29385949\nIteration 48, loss = 0.29328340\nIteration 49, loss = 0.29185625\nIteration 50, loss = 0.29085635\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37351721\nIteration 2, loss = 0.34909893\nIteration 3, loss = 0.34316763\nIteration 4, loss = 0.33940861\nIteration 5, loss = 0.33704912\nIteration 6, loss = 0.33466645\nIteration 7, loss = 0.33373175\nIteration 8, loss = 0.33126416\nIteration 9, loss = 0.33046149\nIteration 10, loss = 0.32873096\nIteration 11, loss = 0.32766661\nIteration 12, loss = 0.32608385\nIteration 13, loss = 0.32581699\nIteration 14, loss = 0.32453561\nIteration 15, loss = 0.32345267\nIteration 16, loss = 0.32289026\nIteration 17, loss = 0.32104803\nIteration 18, loss = 0.32024362\nIteration 19, loss = 0.31902929\nIteration 20, loss = 0.31837498\nIteration 21, loss = 0.31762188\nIteration 22, loss = 0.31675000\nIteration 23, loss = 0.31571328\nIteration 24, loss = 0.31434500\nIteration 25, loss = 0.31367334\nIteration 26, loss = 0.31264033\nIteration 27, loss = 0.31199486\nIteration 28, loss = 0.31095108\nIteration 29, loss = 0.31022473\nIteration 30, loss = 0.30914822\nIteration 31, loss = 0.30804153\nIteration 32, loss = 0.30645466\nIteration 33, loss = 0.30591673\nIteration 34, loss = 0.30513060\nIteration 35, loss = 0.30381697\nIteration 36, loss = 0.30347074\nIteration 37, loss = 0.30251566\nIteration 38, loss = 0.30121677\nIteration 39, loss = 0.30026965\nIteration 40, loss = 0.29933574\nIteration 41, loss = 0.29865612\nIteration 42, loss = 0.29759289\nIteration 43, loss = 0.29647285\nIteration 44, loss = 0.29567120\nIteration 45, loss = 0.29511358\nIteration 46, loss = 0.29395998\nIteration 47, loss = 0.29296842\nIteration 48, loss = 0.29199460\nIteration 49, loss = 0.29217631\nIteration 50, loss = 0.29034878\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37392952\nIteration 2, loss = 0.34953675\nIteration 3, loss = 0.34293534\nIteration 4, loss = 0.33968502\nIteration 5, loss = 0.33698325\nIteration 6, loss = 0.33459751\nIteration 7, loss = 0.33337362\nIteration 8, loss = 0.33096725\nIteration 9, loss = 0.32999288\nIteration 10, loss = 0.32860035\nIteration 11, loss = 0.32725897\nIteration 12, loss = 0.32563384\nIteration 13, loss = 0.32498528\nIteration 14, loss = 0.32374893\nIteration 15, loss = 0.32285850\nIteration 16, loss = 0.32209395\nIteration 17, loss = 0.32040175\nIteration 18, loss = 0.31905333\nIteration 19, loss = 0.31813099\nIteration 20, loss = 0.31698613\nIteration 21, loss = 0.31682869\nIteration 22, loss = 0.31548006\nIteration 23, loss = 0.31376794\nIteration 24, loss = 0.31280491\nIteration 25, loss = 0.31259082\nIteration 26, loss = 0.31122846\nIteration 27, loss = 0.31062809\nIteration 28, loss = 0.30997851\nIteration 29, loss = 0.30845471\nIteration 30, loss = 0.30773528\nIteration 31, loss = 0.30697114\nIteration 32, loss = 0.30548424\nIteration 33, loss = 0.30470178\nIteration 34, loss = 0.30368186\nIteration 35, loss = 0.30302750\nIteration 36, loss = 0.30154884\nIteration 37, loss = 0.30125900\nIteration 38, loss = 0.30052756\nIteration 39, loss = 0.29937941\nIteration 40, loss = 0.29782760\nIteration 41, loss = 0.29725314\nIteration 42, loss = 0.29610068\nIteration 43, loss = 0.29516201\nIteration 44, loss = 0.29440775\nIteration 45, loss = 0.29329082\nIteration 46, loss = 0.29216575\nIteration 47, loss = 0.29153980\nIteration 48, loss = 0.29044564\nIteration 49, loss = 0.29036295\nIteration 50, loss = 0.28870824\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37703890\nIteration 2, loss = 0.34863198\nIteration 3, loss = 0.34204963\nIteration 4, loss = 0.33928587\nIteration 5, loss = 0.33723324\nIteration 6, loss = 0.33459579\nIteration 7, loss = 0.33272303\nIteration 8, loss = 0.33187236\nIteration 9, loss = 0.33003156\nIteration 10, loss = 0.32885428\nIteration 11, loss = 0.32749383\nIteration 12, loss = 0.32646937\nIteration 13, loss = 0.32510683\nIteration 14, loss = 0.32425146\nIteration 15, loss = 0.32269478\nIteration 16, loss = 0.32178717\nIteration 17, loss = 0.32075831\nIteration 18, loss = 0.31987084\nIteration 19, loss = 0.31839094\nIteration 20, loss = 0.31760246\nIteration 21, loss = 0.31632233\nIteration 22, loss = 0.31512148\nIteration 23, loss = 0.31401464\nIteration 24, loss = 0.31279954\nIteration 25, loss = 0.31189595\nIteration 26, loss = 0.31147819\nIteration 27, loss = 0.31025865\nIteration 28, loss = 0.30921746\nIteration 29, loss = 0.30781235\nIteration 30, loss = 0.30752346\nIteration 31, loss = 0.30637655\nIteration 32, loss = 0.30540891\nIteration 33, loss = 0.30467376\nIteration 34, loss = 0.30353966\nIteration 35, loss = 0.30277210\nIteration 36, loss = 0.30197051\nIteration 37, loss = 0.30061157\nIteration 38, loss = 0.29982425\nIteration 39, loss = 0.29961711\nIteration 40, loss = 0.29831180\nIteration 41, loss = 0.29716635\nIteration 42, loss = 0.29653347\nIteration 43, loss = 0.29553855\nIteration 44, loss = 0.29463706\nIteration 45, loss = 0.29435401\nIteration 46, loss = 0.29295941\nIteration 47, loss = 0.29269036\nIteration 48, loss = 0.29140645\nIteration 49, loss = 0.29075792\nIteration 50, loss = 0.29091904\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37585107\nIteration 2, loss = 0.34697579\nIteration 3, loss = 0.34038160\nIteration 4, loss = 0.33821271\nIteration 5, loss = 0.33556148\nIteration 6, loss = 0.33340195\nIteration 7, loss = 0.33187249\nIteration 8, loss = 0.33057314\nIteration 9, loss = 0.32920129\nIteration 10, loss = 0.32787972\nIteration 11, loss = 0.32712618\nIteration 12, loss = 0.32575109\nIteration 13, loss = 0.32426244\nIteration 14, loss = 0.32389602\nIteration 15, loss = 0.32175008\nIteration 16, loss = 0.32076079\nIteration 17, loss = 0.31992265\nIteration 18, loss = 0.31863120\nIteration 19, loss = 0.31759738\nIteration 20, loss = 0.31640087\nIteration 21, loss = 0.31525293\nIteration 22, loss = 0.31431758\nIteration 23, loss = 0.31353598\nIteration 24, loss = 0.31230715\nIteration 25, loss = 0.31071185\nIteration 26, loss = 0.31038336\nIteration 27, loss = 0.30910407\nIteration 28, loss = 0.30842330\nIteration 29, loss = 0.30781624\nIteration 30, loss = 0.30637822\nIteration 31, loss = 0.30539130\nIteration 32, loss = 0.30447565\nIteration 33, loss = 0.30352807\nIteration 34, loss = 0.30269752\nIteration 35, loss = 0.30242067\nIteration 36, loss = 0.30088552\nIteration 37, loss = 0.30051943\nIteration 38, loss = 0.29944533\nIteration 39, loss = 0.29886838\nIteration 40, loss = 0.29795578\nIteration 41, loss = 0.29673916\nIteration 42, loss = 0.29525951\nIteration 43, loss = 0.29467617\nIteration 44, loss = 0.29395016\nIteration 45, loss = 0.29322577\nIteration 46, loss = 0.29226515\nIteration 47, loss = 0.29185984\nIteration 48, loss = 0.29047270\nIteration 49, loss = 0.28995887\nIteration 50, loss = 0.28912931\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37798522\nIteration 2, loss = 0.34843128\nIteration 3, loss = 0.34266813\nIteration 4, loss = 0.33992523\nIteration 5, loss = 0.33795816\nIteration 6, loss = 0.33587738\nIteration 7, loss = 0.33406121\nIteration 8, loss = 0.33237042\nIteration 9, loss = 0.33154567\nIteration 10, loss = 0.33010312\nIteration 11, loss = 0.32857588\nIteration 12, loss = 0.32712373\nIteration 13, loss = 0.32650347\nIteration 14, loss = 0.32554202\nIteration 15, loss = 0.32425101\nIteration 16, loss = 0.32280978\nIteration 17, loss = 0.32261673\nIteration 18, loss = 0.32087989\nIteration 19, loss = 0.31996346\nIteration 20, loss = 0.31867925\nIteration 21, loss = 0.31793783\nIteration 22, loss = 0.31683788\nIteration 23, loss = 0.31598437\nIteration 24, loss = 0.31474077\nIteration 25, loss = 0.31383630\nIteration 26, loss = 0.31204360\nIteration 27, loss = 0.31095042\nIteration 28, loss = 0.31052891\nIteration 29, loss = 0.30904448\nIteration 30, loss = 0.30833125\nIteration 31, loss = 0.30716043\nIteration 32, loss = 0.30614201\nIteration 33, loss = 0.30541585\nIteration 34, loss = 0.30374793\nIteration 35, loss = 0.30302201\nIteration 36, loss = 0.30189581\nIteration 37, loss = 0.30174840\nIteration 38, loss = 0.30041890\nIteration 39, loss = 0.30025615\nIteration 40, loss = 0.29835523\nIteration 41, loss = 0.29820533\nIteration 42, loss = 0.29690418\nIteration 43, loss = 0.29569337\nIteration 44, loss = 0.29469477\nIteration 45, loss = 0.29445521\nIteration 46, loss = 0.29345639\nIteration 47, loss = 0.29204819\nIteration 48, loss = 0.29199822\nIteration 49, loss = 0.29047194\nIteration 50, loss = 0.28955324\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37719464\nIteration 2, loss = 0.34799239\nIteration 3, loss = 0.34218473\nIteration 4, loss = 0.33920146\nIteration 5, loss = 0.33692604\nIteration 6, loss = 0.33486459\nIteration 7, loss = 0.33216051\nIteration 8, loss = 0.33135662\nIteration 9, loss = 0.32995020\nIteration 10, loss = 0.32852547\nIteration 11, loss = 0.32753012\nIteration 12, loss = 0.32603462\nIteration 13, loss = 0.32533053\nIteration 14, loss = 0.32418066\nIteration 15, loss = 0.32341332\nIteration 16, loss = 0.32204131\nIteration 17, loss = 0.32115945\nIteration 18, loss = 0.31992656\nIteration 19, loss = 0.31873767\nIteration 20, loss = 0.31787094\nIteration 21, loss = 0.31736352\nIteration 22, loss = 0.31555180\nIteration 23, loss = 0.31515110\nIteration 24, loss = 0.31368771\nIteration 25, loss = 0.31300651\nIteration 26, loss = 0.31194569\nIteration 27, loss = 0.31090707\nIteration 28, loss = 0.31056608\nIteration 29, loss = 0.30930160\nIteration 30, loss = 0.30825776\nIteration 31, loss = 0.30708097\nIteration 32, loss = 0.30648171\nIteration 33, loss = 0.30578315\nIteration 34, loss = 0.30403429\nIteration 35, loss = 0.30322384\nIteration 36, loss = 0.30289349\nIteration 37, loss = 0.30224462\nIteration 38, loss = 0.30159807\nIteration 39, loss = 0.30097490\nIteration 40, loss = 0.29961617\nIteration 41, loss = 0.29886557\nIteration 42, loss = 0.29796387\nIteration 43, loss = 0.29716227\nIteration 44, loss = 0.29585977\nIteration 45, loss = 0.29609138\nIteration 46, loss = 0.29486960\nIteration 47, loss = 0.29350477\nIteration 48, loss = 0.29287384\nIteration 49, loss = 0.29251194\nIteration 50, loss = 0.29149066\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37859262\nIteration 2, loss = 0.34847518\nIteration 3, loss = 0.34261773\nIteration 4, loss = 0.34018490\nIteration 5, loss = 0.33761344\nIteration 6, loss = 0.33543466\nIteration 7, loss = 0.33329723\nIteration 8, loss = 0.33193189\nIteration 9, loss = 0.32987800\nIteration 10, loss = 0.32881132\nIteration 11, loss = 0.32748336\nIteration 12, loss = 0.32613817\nIteration 13, loss = 0.32500796\nIteration 14, loss = 0.32403341\nIteration 15, loss = 0.32306450\nIteration 16, loss = 0.32191182\nIteration 17, loss = 0.32081500\nIteration 18, loss = 0.31970399\nIteration 19, loss = 0.31870161\nIteration 20, loss = 0.31710995\nIteration 21, loss = 0.31666468\nIteration 22, loss = 0.31512553\nIteration 23, loss = 0.31457006\nIteration 24, loss = 0.31349458\nIteration 25, loss = 0.31260938\nIteration 26, loss = 0.31112262\nIteration 27, loss = 0.31012319\nIteration 28, loss = 0.30956017\nIteration 29, loss = 0.30812239\nIteration 30, loss = 0.30755859\nIteration 31, loss = 0.30636151\nIteration 32, loss = 0.30543059\nIteration 33, loss = 0.30437053\nIteration 34, loss = 0.30300545\nIteration 35, loss = 0.30221256\nIteration 36, loss = 0.30148723\nIteration 37, loss = 0.30126684\nIteration 38, loss = 0.29985563\nIteration 39, loss = 0.29934051\nIteration 40, loss = 0.29770502\nIteration 41, loss = 0.29677883\nIteration 42, loss = 0.29628630\nIteration 43, loss = 0.29508835\nIteration 44, loss = 0.29445797\nIteration 45, loss = 0.29331969\nIteration 46, loss = 0.29308349\nIteration 47, loss = 0.29185374\nIteration 48, loss = 0.29099784\nIteration 49, loss = 0.29019197\nIteration 50, loss = 0.28816973\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37371200\nIteration 2, loss = 0.34861668\nIteration 3, loss = 0.34298486\nIteration 4, loss = 0.33976588\nIteration 5, loss = 0.33658253\nIteration 6, loss = 0.33463306\nIteration 7, loss = 0.33400976\nIteration 8, loss = 0.33129745\nIteration 9, loss = 0.33006940\nIteration 10, loss = 0.32858684\nIteration 11, loss = 0.32677714\nIteration 12, loss = 0.32559485\nIteration 13, loss = 0.32488215\nIteration 14, loss = 0.32381064\nIteration 15, loss = 0.32267126\nIteration 16, loss = 0.32103442\nIteration 17, loss = 0.32060549\nIteration 18, loss = 0.31941139\nIteration 19, loss = 0.31881976\nIteration 20, loss = 0.31763443\nIteration 21, loss = 0.31630487\nIteration 22, loss = 0.31499480\nIteration 23, loss = 0.31446790\nIteration 24, loss = 0.31291795\nIteration 25, loss = 0.31244214\nIteration 26, loss = 0.31113217\nIteration 27, loss = 0.30991451\nIteration 28, loss = 0.30888657\nIteration 29, loss = 0.30811679\nIteration 30, loss = 0.30737485\nIteration 31, loss = 0.30668254\nIteration 32, loss = 0.30546352\nIteration 33, loss = 0.30454626\nIteration 34, loss = 0.30432462\nIteration 35, loss = 0.30272244\nIteration 36, loss = 0.30159505\nIteration 37, loss = 0.30056525\nIteration 38, loss = 0.29959446\nIteration 39, loss = 0.29858868\nIteration 40, loss = 0.29826507\nIteration 41, loss = 0.29718534\nIteration 42, loss = 0.29673530\nIteration 43, loss = 0.29565813\nIteration 44, loss = 0.29422958\nIteration 45, loss = 0.29379730\nIteration 46, loss = 0.29264914\nIteration 47, loss = 0.29207555\nIteration 48, loss = 0.29128144\nIteration 49, loss = 0.29000322\nIteration 50, loss = 0.29036693\nIteration 51, loss = 0.28931732\nIteration 52, loss = 0.28849132\nIteration 53, loss = 0.28726769\nIteration 54, loss = 0.28633632\nIteration 55, loss = 0.28600378\nIteration 56, loss = 0.28516924\nIteration 57, loss = 0.28455014\nIteration 58, loss = 0.28374361\nIteration 59, loss = 0.28359302\nIteration 60, loss = 0.28259251\nIteration 61, loss = 0.28174805\nIteration 62, loss = 0.27966205\nIteration 63, loss = 0.27934397\nIteration 64, loss = 0.27980219\nIteration 65, loss = 0.27758452\nIteration 66, loss = 0.27776562\nIteration 67, loss = 0.27729533\nIteration 68, loss = 0.27674387\nIteration 69, loss = 0.27521971\nIteration 70, loss = 0.27458339\nIteration 71, loss = 0.27420836\nIteration 72, loss = 0.27338000\nIteration 73, loss = 0.27255926\nIteration 74, loss = 0.27194715\nIteration 75, loss = 0.27149765\nIteration 76, loss = 0.27084384\nIteration 77, loss = 0.27056109\nIteration 78, loss = 0.26970563\nIteration 79, loss = 0.26917976\nIteration 80, loss = 0.26824950\nIteration 81, loss = 0.26777476\nIteration 82, loss = 0.26730919\nIteration 83, loss = 0.26615505\nIteration 84, loss = 0.26561864\nIteration 85, loss = 0.26511476\nIteration 86, loss = 0.26401016\nIteration 87, loss = 0.26387889\nIteration 88, loss = 0.26346380\nIteration 89, loss = 0.26272390\nIteration 90, loss = 0.26226572\nIteration 91, loss = 0.26157957\nIteration 92, loss = 0.26113618\nIteration 93, loss = 0.26114487\nIteration 94, loss = 0.25959596\nIteration 95, loss = 0.25922689\nIteration 96, loss = 0.25877818\nIteration 97, loss = 0.25844437\nIteration 98, loss = 0.25747236\nIteration 99, loss = 0.25764116\nIteration 100, loss = 0.25727577\nIteration 101, loss = 0.25618426\nIteration 102, loss = 0.25665681\nIteration 103, loss = 0.25544208\nIteration 104, loss = 0.25476670\nIteration 105, loss = 0.25371311\nIteration 106, loss = 0.25447522\nIteration 107, loss = 0.25354248\nIteration 108, loss = 0.25242269\nIteration 109, loss = 0.25252299\nIteration 110, loss = 0.25259235\nIteration 111, loss = 0.25164704\nIteration 112, loss = 0.25143358\nIteration 113, loss = 0.25078151\nIteration 114, loss = 0.24951493\nIteration 115, loss = 0.24981217\nIteration 116, loss = 0.24963710\nIteration 117, loss = 0.24880775\nIteration 118, loss = 0.24965619\nIteration 119, loss = 0.24842390\nIteration 120, loss = 0.24789167\nIteration 121, loss = 0.24697340\nIteration 122, loss = 0.24653959\nIteration 123, loss = 0.24602195\nIteration 124, loss = 0.24660297\nIteration 125, loss = 0.24549928\nIteration 126, loss = 0.24519268\nIteration 127, loss = 0.24459646\nIteration 128, loss = 0.24430795\nIteration 129, loss = 0.24388788\nIteration 130, loss = 0.24408847\nIteration 131, loss = 0.24368831\nIteration 132, loss = 0.24331578\nIteration 133, loss = 0.24213431\nIteration 134, loss = 0.24178952\nIteration 135, loss = 0.24200015\nIteration 136, loss = 0.24108272\nIteration 137, loss = 0.24127664\nIteration 138, loss = 0.24020555\nIteration 139, loss = 0.24056273\nIteration 140, loss = 0.24032095\nIteration 141, loss = 0.23888783\nIteration 142, loss = 0.23910203\nIteration 143, loss = 0.23939491\nIteration 144, loss = 0.23853751\nIteration 145, loss = 0.23866695\nIteration 146, loss = 0.23799067\nIteration 147, loss = 0.23760860\nIteration 148, loss = 0.23776540\nIteration 149, loss = 0.23808353\nIteration 150, loss = 0.23634495\nIteration 151, loss = 0.23711208\nIteration 152, loss = 0.23682842\nIteration 153, loss = 0.23597739\nIteration 154, loss = 0.23546939\nIteration 155, loss = 0.23518972\nIteration 156, loss = 0.23574458\nIteration 157, loss = 0.23495363\nIteration 158, loss = 0.23451798\nIteration 159, loss = 0.23399137\nIteration 160, loss = 0.23418933\nIteration 161, loss = 0.23417212\nIteration 162, loss = 0.23258774\nIteration 163, loss = 0.23262388\nIteration 164, loss = 0.23255654\nIteration 165, loss = 0.23155005\nIteration 166, loss = 0.23195016\nIteration 167, loss = 0.23088313\nIteration 168, loss = 0.23152617\nIteration 169, loss = 0.23113774\nIteration 170, loss = 0.23111800\nIteration 171, loss = 0.23049761\nIteration 172, loss = 0.23014114\nIteration 173, loss = 0.22971272\nIteration 174, loss = 0.23003898\nIteration 175, loss = 0.22966506\nIteration 176, loss = 0.22971654\nIteration 177, loss = 0.23001796\nIteration 178, loss = 0.22958825\nIteration 179, loss = 0.22832082\nIteration 180, loss = 0.22878082\nIteration 181, loss = 0.22791627\nIteration 182, loss = 0.22805670\nIteration 183, loss = 0.22788222\nIteration 184, loss = 0.22796633\nIteration 185, loss = 0.22667551\nIteration 186, loss = 0.22660199\nIteration 187, loss = 0.22568885\nIteration 188, loss = 0.22616678\nIteration 189, loss = 0.22653403\nIteration 190, loss = 0.22544516\nIteration 191, loss = 0.22536242\nIteration 192, loss = 0.22487941\nIteration 193, loss = 0.22478786\nIteration 194, loss = 0.22399072\nIteration 195, loss = 0.22469420\nIteration 196, loss = 0.22468010\nIteration 197, loss = 0.22328325\nIteration 198, loss = 0.22353968\nIteration 199, loss = 0.22266910\nIteration 200, loss = 0.22287847\nIteration 201, loss = 0.22315203\nIteration 202, loss = 0.22316980\nIteration 203, loss = 0.22281888\nIteration 204, loss = 0.22308068\nIteration 205, loss = 0.22147087\nIteration 206, loss = 0.22263849\nIteration 207, loss = 0.22169689\nIteration 208, loss = 0.22091334\nIteration 209, loss = 0.22152217\nIteration 210, loss = 0.22081956\nIteration 211, loss = 0.22073342\nIteration 212, loss = 0.22027149\nIteration 213, loss = 0.21921266\nIteration 214, loss = 0.22066313\nIteration 215, loss = 0.22024438\nIteration 216, loss = 0.21952244\nIteration 217, loss = 0.21936717\nIteration 218, loss = 0.21901900\nIteration 219, loss = 0.21857619\nIteration 220, loss = 0.21895612\nIteration 221, loss = 0.21866235\nIteration 222, loss = 0.21880411\nIteration 223, loss = 0.21801332\nIteration 224, loss = 0.21759642\nIteration 225, loss = 0.21849398\nIteration 226, loss = 0.21816081\nIteration 227, loss = 0.21816804\nIteration 228, loss = 0.21635228\nIteration 229, loss = 0.21678291\nIteration 230, loss = 0.21629148\nIteration 231, loss = 0.21622880\nIteration 232, loss = 0.21671460\nIteration 233, loss = 0.21693920\nIteration 234, loss = 0.21652600\nIteration 235, loss = 0.21611548\nIteration 236, loss = 0.21513200\nIteration 237, loss = 0.21581643\nIteration 238, loss = 0.21569882\nIteration 239, loss = 0.21456667\nIteration 240, loss = 0.21554274\nIteration 241, loss = 0.21433681\nIteration 242, loss = 0.21362517\nIteration 243, loss = 0.21348204\nIteration 244, loss = 0.21434877\nIteration 245, loss = 0.21298966\nIteration 246, loss = 0.21391029\nIteration 247, loss = 0.21376030\nIteration 248, loss = 0.21374005\nIteration 249, loss = 0.21281301\nIteration 250, loss = 0.21391555\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37212711\nIteration 2, loss = 0.34690303\nIteration 3, loss = 0.34175845\nIteration 4, loss = 0.33871517\nIteration 5, loss = 0.33589490\nIteration 6, loss = 0.33430475\nIteration 7, loss = 0.33298493\nIteration 8, loss = 0.33040619\nIteration 9, loss = 0.32917244\nIteration 10, loss = 0.32765886\nIteration 11, loss = 0.32631198\nIteration 12, loss = 0.32485339\nIteration 13, loss = 0.32395924\nIteration 14, loss = 0.32293141\nIteration 15, loss = 0.32208565\nIteration 16, loss = 0.32046177\nIteration 17, loss = 0.31968410\nIteration 18, loss = 0.31870551\nIteration 19, loss = 0.31835932\nIteration 20, loss = 0.31726637\nIteration 21, loss = 0.31624118\nIteration 22, loss = 0.31498855\nIteration 23, loss = 0.31410926\nIteration 24, loss = 0.31305806\nIteration 25, loss = 0.31215911\nIteration 26, loss = 0.31080979\nIteration 27, loss = 0.31009342\nIteration 28, loss = 0.30951358\nIteration 29, loss = 0.30867584\nIteration 30, loss = 0.30763187\nIteration 31, loss = 0.30658376\nIteration 32, loss = 0.30578287\nIteration 33, loss = 0.30509142\nIteration 34, loss = 0.30412084\nIteration 35, loss = 0.30297576\nIteration 36, loss = 0.30212677\nIteration 37, loss = 0.30176069\nIteration 38, loss = 0.30116421\nIteration 39, loss = 0.29985678\nIteration 40, loss = 0.29921650\nIteration 41, loss = 0.29822169\nIteration 42, loss = 0.29767786\nIteration 43, loss = 0.29707434\nIteration 44, loss = 0.29525598\nIteration 45, loss = 0.29512451\nIteration 46, loss = 0.29398884\nIteration 47, loss = 0.29313323\nIteration 48, loss = 0.29207666\nIteration 49, loss = 0.29144226\nIteration 50, loss = 0.29098004\nIteration 51, loss = 0.29015317\nIteration 52, loss = 0.28987123\nIteration 53, loss = 0.28852813\nIteration 54, loss = 0.28801192\nIteration 55, loss = 0.28736830\nIteration 56, loss = 0.28640600\nIteration 57, loss = 0.28650166\nIteration 58, loss = 0.28483071\nIteration 59, loss = 0.28469127\nIteration 60, loss = 0.28362995\nIteration 61, loss = 0.28301625\nIteration 62, loss = 0.28208982\nIteration 63, loss = 0.28096058\nIteration 64, loss = 0.28088723\nIteration 65, loss = 0.27997423\nIteration 66, loss = 0.27934195\nIteration 67, loss = 0.27893520\nIteration 68, loss = 0.27844464\nIteration 69, loss = 0.27752644\nIteration 70, loss = 0.27660846\nIteration 71, loss = 0.27586804\nIteration 72, loss = 0.27517837\nIteration 73, loss = 0.27466677\nIteration 74, loss = 0.27427715\nIteration 75, loss = 0.27294800\nIteration 76, loss = 0.27333043\nIteration 77, loss = 0.27207419\nIteration 78, loss = 0.27209011\nIteration 79, loss = 0.27099155\nIteration 80, loss = 0.27048964\nIteration 81, loss = 0.27015024\nIteration 82, loss = 0.26865058\nIteration 83, loss = 0.26888444\nIteration 84, loss = 0.26751946\nIteration 85, loss = 0.26670657\nIteration 86, loss = 0.26689399\nIteration 87, loss = 0.26618645\nIteration 88, loss = 0.26497901\nIteration 89, loss = 0.26415000\nIteration 90, loss = 0.26485791\nIteration 91, loss = 0.26403607\nIteration 92, loss = 0.26327861\nIteration 93, loss = 0.26215210\nIteration 94, loss = 0.26216054\nIteration 95, loss = 0.26032052\nIteration 96, loss = 0.26117487\nIteration 97, loss = 0.26021270\nIteration 98, loss = 0.25949528\nIteration 99, loss = 0.25912743\nIteration 100, loss = 0.25884085\nIteration 101, loss = 0.25773956\nIteration 102, loss = 0.25827464\nIteration 103, loss = 0.25677800\nIteration 104, loss = 0.25646037\nIteration 105, loss = 0.25566591\nIteration 106, loss = 0.25552791\nIteration 107, loss = 0.25504934\nIteration 108, loss = 0.25434979\nIteration 109, loss = 0.25387146\nIteration 110, loss = 0.25369729\nIteration 111, loss = 0.25256058\nIteration 112, loss = 0.25218353\nIteration 113, loss = 0.25138850\nIteration 114, loss = 0.25145671\nIteration 115, loss = 0.25117666\nIteration 116, loss = 0.25117209\nIteration 117, loss = 0.24960751\nIteration 118, loss = 0.24973919\nIteration 119, loss = 0.24943224\nIteration 120, loss = 0.24845084\nIteration 121, loss = 0.24814837\nIteration 122, loss = 0.24820875\nIteration 123, loss = 0.24729920\nIteration 124, loss = 0.24717371\nIteration 125, loss = 0.24649107\nIteration 126, loss = 0.24552923\nIteration 127, loss = 0.24553821\nIteration 128, loss = 0.24540097\nIteration 129, loss = 0.24523145\nIteration 130, loss = 0.24427719\nIteration 131, loss = 0.24395256\nIteration 132, loss = 0.24367958\nIteration 133, loss = 0.24385582\nIteration 134, loss = 0.24318015\nIteration 135, loss = 0.24297054\nIteration 136, loss = 0.24129472\nIteration 137, loss = 0.24124258\nIteration 138, loss = 0.24048965\nIteration 139, loss = 0.24093449\nIteration 140, loss = 0.24103700\nIteration 141, loss = 0.23995509\nIteration 142, loss = 0.23992550\nIteration 143, loss = 0.23988948\nIteration 144, loss = 0.23933980\nIteration 145, loss = 0.23882885\nIteration 146, loss = 0.23846144\nIteration 147, loss = 0.23804373\nIteration 148, loss = 0.23768669\nIteration 149, loss = 0.23711016\nIteration 150, loss = 0.23753174\nIteration 151, loss = 0.23607061\nIteration 152, loss = 0.23692321\nIteration 153, loss = 0.23703544\nIteration 154, loss = 0.23538363\nIteration 155, loss = 0.23502184\nIteration 156, loss = 0.23437853\nIteration 157, loss = 0.23565450\nIteration 158, loss = 0.23539029\nIteration 159, loss = 0.23469977\nIteration 160, loss = 0.23374439\nIteration 161, loss = 0.23362419\nIteration 162, loss = 0.23315103\nIteration 163, loss = 0.23232234\nIteration 164, loss = 0.23288241\nIteration 165, loss = 0.23181632\nIteration 166, loss = 0.23241396\nIteration 167, loss = 0.23199115\nIteration 168, loss = 0.23105387\nIteration 169, loss = 0.23128790\nIteration 170, loss = 0.23097350\nIteration 171, loss = 0.23039155\nIteration 172, loss = 0.22989507\nIteration 173, loss = 0.22986018\nIteration 174, loss = 0.22971469\nIteration 175, loss = 0.22877706\nIteration 176, loss = 0.22971190\nIteration 177, loss = 0.22923077\nIteration 178, loss = 0.22864352\nIteration 179, loss = 0.22825357\nIteration 180, loss = 0.22739154\nIteration 181, loss = 0.22674878\nIteration 182, loss = 0.22730751\nIteration 183, loss = 0.22702635\nIteration 184, loss = 0.22629765\nIteration 185, loss = 0.22678585\nIteration 186, loss = 0.22714008\nIteration 187, loss = 0.22704907\nIteration 188, loss = 0.22541420\nIteration 189, loss = 0.22562543\nIteration 190, loss = 0.22559391\nIteration 191, loss = 0.22458395\nIteration 192, loss = 0.22509557\nIteration 193, loss = 0.22519281\nIteration 194, loss = 0.22408497\nIteration 195, loss = 0.22442494\nIteration 196, loss = 0.22382122\nIteration 197, loss = 0.22282433\nIteration 198, loss = 0.22289695\nIteration 199, loss = 0.22306060\nIteration 200, loss = 0.22343886\nIteration 201, loss = 0.22193859\nIteration 202, loss = 0.22098042\nIteration 203, loss = 0.22145936\nIteration 204, loss = 0.22291037\nIteration 205, loss = 0.22216251\nIteration 206, loss = 0.22150165\nIteration 207, loss = 0.21962371\nIteration 208, loss = 0.22050947\nIteration 209, loss = 0.21976871\nIteration 210, loss = 0.21995273\nIteration 211, loss = 0.22026325\nIteration 212, loss = 0.21947418\nIteration 213, loss = 0.21988433\nIteration 214, loss = 0.21895325\nIteration 215, loss = 0.21918375\nIteration 216, loss = 0.21788860\nIteration 217, loss = 0.21888081\nIteration 218, loss = 0.21860043\nIteration 219, loss = 0.21791458\nIteration 220, loss = 0.21779394\nIteration 221, loss = 0.21816289\nIteration 222, loss = 0.21783340\nIteration 223, loss = 0.21660999\nIteration 224, loss = 0.21792111\nIteration 225, loss = 0.21640778\nIteration 226, loss = 0.21622711\nIteration 227, loss = 0.21655701\nIteration 228, loss = 0.21598097\nIteration 229, loss = 0.21627810\nIteration 230, loss = 0.21552363\nIteration 231, loss = 0.21503141\nIteration 232, loss = 0.21484462\nIteration 233, loss = 0.21514210\nIteration 234, loss = 0.21581763\nIteration 235, loss = 0.21494171\nIteration 236, loss = 0.21443425\nIteration 237, loss = 0.21407375\nIteration 238, loss = 0.21345846\nIteration 239, loss = 0.21491092\nIteration 240, loss = 0.21398532\nIteration 241, loss = 0.21311105\nIteration 242, loss = 0.21432791\nIteration 243, loss = 0.21291799\nIteration 244, loss = 0.21204483\nIteration 245, loss = 0.21327533\nIteration 246, loss = 0.21229737\nIteration 247, loss = 0.21281295\nIteration 248, loss = 0.21234969\nIteration 249, loss = 0.21176842\nIteration 250, loss = 0.21166845\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37392595\nIteration 2, loss = 0.34990079\nIteration 3, loss = 0.34419610\nIteration 4, loss = 0.34076773\nIteration 5, loss = 0.33764428\nIteration 6, loss = 0.33555669\nIteration 7, loss = 0.33428226\nIteration 8, loss = 0.33231093\nIteration 9, loss = 0.33136623\nIteration 10, loss = 0.32941807\nIteration 11, loss = 0.32805149\nIteration 12, loss = 0.32676148\nIteration 13, loss = 0.32664886\nIteration 14, loss = 0.32530509\nIteration 15, loss = 0.32401933\nIteration 16, loss = 0.32297876\nIteration 17, loss = 0.32139654\nIteration 18, loss = 0.32052200\nIteration 19, loss = 0.31962706\nIteration 20, loss = 0.31794785\nIteration 21, loss = 0.31762523\nIteration 22, loss = 0.31631766\nIteration 23, loss = 0.31538302\nIteration 24, loss = 0.31390667\nIteration 25, loss = 0.31366307\nIteration 26, loss = 0.31191796\nIteration 27, loss = 0.31139777\nIteration 28, loss = 0.31064731\nIteration 29, loss = 0.30964606\nIteration 30, loss = 0.30841319\nIteration 31, loss = 0.30761023\nIteration 32, loss = 0.30654945\nIteration 33, loss = 0.30594310\nIteration 34, loss = 0.30483974\nIteration 35, loss = 0.30408425\nIteration 36, loss = 0.30347260\nIteration 37, loss = 0.30275029\nIteration 38, loss = 0.30149805\nIteration 39, loss = 0.30068416\nIteration 40, loss = 0.29931678\nIteration 41, loss = 0.29877696\nIteration 42, loss = 0.29818832\nIteration 43, loss = 0.29723797\nIteration 44, loss = 0.29606731\nIteration 45, loss = 0.29529193\nIteration 46, loss = 0.29409340\nIteration 47, loss = 0.29385949\nIteration 48, loss = 0.29328340\nIteration 49, loss = 0.29185625\nIteration 50, loss = 0.29085635\nIteration 51, loss = 0.29013153\nIteration 52, loss = 0.28954575\nIteration 53, loss = 0.28841515\nIteration 54, loss = 0.28786146\nIteration 55, loss = 0.28769712\nIteration 56, loss = 0.28633546\nIteration 57, loss = 0.28585370\nIteration 58, loss = 0.28476615\nIteration 59, loss = 0.28359785\nIteration 60, loss = 0.28252721\nIteration 61, loss = 0.28292751\nIteration 62, loss = 0.28180816\nIteration 63, loss = 0.28119548\nIteration 64, loss = 0.28100698\nIteration 65, loss = 0.27997494\nIteration 66, loss = 0.27893131\nIteration 67, loss = 0.27851792\nIteration 68, loss = 0.27683227\nIteration 69, loss = 0.27613747\nIteration 70, loss = 0.27505489\nIteration 71, loss = 0.27517009\nIteration 72, loss = 0.27554537\nIteration 73, loss = 0.27444612\nIteration 74, loss = 0.27311214\nIteration 75, loss = 0.27234889\nIteration 76, loss = 0.27253035\nIteration 77, loss = 0.27081440\nIteration 78, loss = 0.27117689\nIteration 79, loss = 0.26951726\nIteration 80, loss = 0.26874364\nIteration 81, loss = 0.26893977\nIteration 82, loss = 0.26808749\nIteration 83, loss = 0.26663969\nIteration 84, loss = 0.26754043\nIteration 85, loss = 0.26596535\nIteration 86, loss = 0.26516232\nIteration 87, loss = 0.26481264\nIteration 88, loss = 0.26452929\nIteration 89, loss = 0.26396401\nIteration 90, loss = 0.26273751\nIteration 91, loss = 0.26190761\nIteration 92, loss = 0.26280286\nIteration 93, loss = 0.26134321\nIteration 94, loss = 0.26061135\nIteration 95, loss = 0.26066930\nIteration 96, loss = 0.26014314\nIteration 97, loss = 0.25951913\nIteration 98, loss = 0.25920932\nIteration 99, loss = 0.25784333\nIteration 100, loss = 0.25860488\nIteration 101, loss = 0.25752390\nIteration 102, loss = 0.25694869\nIteration 103, loss = 0.25671567\nIteration 104, loss = 0.25618722\nIteration 105, loss = 0.25533813\nIteration 106, loss = 0.25468917\nIteration 107, loss = 0.25477632\nIteration 108, loss = 0.25364951\nIteration 109, loss = 0.25392146\nIteration 110, loss = 0.25383426\nIteration 111, loss = 0.25277069\nIteration 112, loss = 0.25192386\nIteration 113, loss = 0.25160341\nIteration 114, loss = 0.25143491\nIteration 115, loss = 0.24972600\nIteration 116, loss = 0.25177012\nIteration 117, loss = 0.24992786\nIteration 118, loss = 0.24930845\nIteration 119, loss = 0.24841180\nIteration 120, loss = 0.24832849\nIteration 121, loss = 0.24785900\nIteration 122, loss = 0.24815255\nIteration 123, loss = 0.24693666\nIteration 124, loss = 0.24678512\nIteration 125, loss = 0.24692225\nIteration 126, loss = 0.24586081\nIteration 127, loss = 0.24515679\nIteration 128, loss = 0.24556462\nIteration 129, loss = 0.24539458\nIteration 130, loss = 0.24468451\nIteration 131, loss = 0.24358944\nIteration 132, loss = 0.24359984\nIteration 133, loss = 0.24235503\nIteration 134, loss = 0.24242546\nIteration 135, loss = 0.24279563\nIteration 136, loss = 0.24290039\nIteration 137, loss = 0.24157838\nIteration 138, loss = 0.24232115\nIteration 139, loss = 0.24075696\nIteration 140, loss = 0.24112435\nIteration 141, loss = 0.23996636\nIteration 142, loss = 0.23881657\nIteration 143, loss = 0.23979959\nIteration 144, loss = 0.23879350\nIteration 145, loss = 0.23899434\nIteration 146, loss = 0.23803258\nIteration 147, loss = 0.23708489\nIteration 148, loss = 0.23814636\nIteration 149, loss = 0.23736041\nIteration 150, loss = 0.23683120\nIteration 151, loss = 0.23701286\nIteration 152, loss = 0.23616543\nIteration 153, loss = 0.23618940\nIteration 154, loss = 0.23555833\nIteration 155, loss = 0.23445724\nIteration 156, loss = 0.23593439\nIteration 157, loss = 0.23436891\nIteration 158, loss = 0.23366310\nIteration 159, loss = 0.23399092\nIteration 160, loss = 0.23416739\nIteration 161, loss = 0.23347316\nIteration 162, loss = 0.23431682\nIteration 163, loss = 0.23296718\nIteration 164, loss = 0.23201835\nIteration 165, loss = 0.23172228\nIteration 166, loss = 0.23143945\nIteration 167, loss = 0.23098957\nIteration 168, loss = 0.23161542\nIteration 169, loss = 0.23064655\nIteration 170, loss = 0.23157767\nIteration 171, loss = 0.23005761\nIteration 172, loss = 0.23055217\nIteration 173, loss = 0.22874788\nIteration 174, loss = 0.22918959\nIteration 175, loss = 0.22895915\nIteration 176, loss = 0.22888382\nIteration 177, loss = 0.22785052\nIteration 178, loss = 0.22754670\nIteration 179, loss = 0.22816359\nIteration 180, loss = 0.22774985\nIteration 181, loss = 0.22798324\nIteration 182, loss = 0.22730321\nIteration 183, loss = 0.22660168\nIteration 184, loss = 0.22609785\nIteration 185, loss = 0.22620031\nIteration 186, loss = 0.22614840\nIteration 187, loss = 0.22610225\nIteration 188, loss = 0.22614676\nIteration 189, loss = 0.22403052\nIteration 190, loss = 0.22514171\nIteration 191, loss = 0.22402818\nIteration 192, loss = 0.22437220\nIteration 193, loss = 0.22487634\nIteration 194, loss = 0.22439106\nIteration 195, loss = 0.22417541\nIteration 196, loss = 0.22422538\nIteration 197, loss = 0.22303198\nIteration 198, loss = 0.22369388\nIteration 199, loss = 0.22280781\nIteration 200, loss = 0.22159777\nIteration 201, loss = 0.22202376\nIteration 202, loss = 0.22262679\nIteration 203, loss = 0.22195055\nIteration 204, loss = 0.22215883\nIteration 205, loss = 0.22154972\nIteration 206, loss = 0.22138416\nIteration 207, loss = 0.22157769\nIteration 208, loss = 0.22202561\nIteration 209, loss = 0.21906522\nIteration 210, loss = 0.21924167\nIteration 211, loss = 0.21982570\nIteration 212, loss = 0.21986977\nIteration 213, loss = 0.21943187\nIteration 214, loss = 0.21814857\nIteration 215, loss = 0.21932557\nIteration 216, loss = 0.21883779\nIteration 217, loss = 0.21874306\nIteration 218, loss = 0.21800898\nIteration 219, loss = 0.21759137\nIteration 220, loss = 0.21803501\nIteration 221, loss = 0.21728943\nIteration 222, loss = 0.21626595\nIteration 223, loss = 0.21774133\nIteration 224, loss = 0.21714235\nIteration 225, loss = 0.21847762\nIteration 226, loss = 0.21605453\nIteration 227, loss = 0.21676633\nIteration 228, loss = 0.21554302\nIteration 229, loss = 0.21629039\nIteration 230, loss = 0.21691366\nIteration 231, loss = 0.21567881\nIteration 232, loss = 0.21637639\nIteration 233, loss = 0.21549865\nIteration 234, loss = 0.21531190\nIteration 235, loss = 0.21481961\nIteration 236, loss = 0.21460677\nIteration 237, loss = 0.21556230\nIteration 238, loss = 0.21415116\nIteration 239, loss = 0.21383338\nIteration 240, loss = 0.21379917\nIteration 241, loss = 0.21506228\nIteration 242, loss = 0.21320947\nIteration 243, loss = 0.21316406\nIteration 244, loss = 0.21248292\nIteration 245, loss = 0.21180191\nIteration 246, loss = 0.21401297\nIteration 247, loss = 0.21266314\nIteration 248, loss = 0.21315947\nIteration 249, loss = 0.21233491\nIteration 250, loss = 0.21177235\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37351721\nIteration 2, loss = 0.34909893\nIteration 3, loss = 0.34316763\nIteration 4, loss = 0.33940861\nIteration 5, loss = 0.33704912\nIteration 6, loss = 0.33466645\nIteration 7, loss = 0.33373175\nIteration 8, loss = 0.33126416\nIteration 9, loss = 0.33046149\nIteration 10, loss = 0.32873096\nIteration 11, loss = 0.32766661\nIteration 12, loss = 0.32608385\nIteration 13, loss = 0.32581699\nIteration 14, loss = 0.32453561\nIteration 15, loss = 0.32345267\nIteration 16, loss = 0.32289026\nIteration 17, loss = 0.32104803\nIteration 18, loss = 0.32024362\nIteration 19, loss = 0.31902929\nIteration 20, loss = 0.31837498\nIteration 21, loss = 0.31762188\nIteration 22, loss = 0.31675000\nIteration 23, loss = 0.31571328\nIteration 24, loss = 0.31434500\nIteration 25, loss = 0.31367334\nIteration 26, loss = 0.31264033\nIteration 27, loss = 0.31199486\nIteration 28, loss = 0.31095108\nIteration 29, loss = 0.31022473\nIteration 30, loss = 0.30914822\nIteration 31, loss = 0.30804153\nIteration 32, loss = 0.30645466\nIteration 33, loss = 0.30591673\nIteration 34, loss = 0.30513060\nIteration 35, loss = 0.30381697\nIteration 36, loss = 0.30347074\nIteration 37, loss = 0.30251566\nIteration 38, loss = 0.30121677\nIteration 39, loss = 0.30026965\nIteration 40, loss = 0.29933574\nIteration 41, loss = 0.29865612\nIteration 42, loss = 0.29759289\nIteration 43, loss = 0.29647285\nIteration 44, loss = 0.29567120\nIteration 45, loss = 0.29511358\nIteration 46, loss = 0.29395998\nIteration 47, loss = 0.29296842\nIteration 48, loss = 0.29199460\nIteration 49, loss = 0.29217631\nIteration 50, loss = 0.29034878\nIteration 51, loss = 0.28988399\nIteration 52, loss = 0.28884965\nIteration 53, loss = 0.28810270\nIteration 54, loss = 0.28744895\nIteration 55, loss = 0.28677969\nIteration 56, loss = 0.28646118\nIteration 57, loss = 0.28558824\nIteration 58, loss = 0.28410080\nIteration 59, loss = 0.28296099\nIteration 60, loss = 0.28208436\nIteration 61, loss = 0.28177710\nIteration 62, loss = 0.28122695\nIteration 63, loss = 0.28006971\nIteration 64, loss = 0.27928695\nIteration 65, loss = 0.27964145\nIteration 66, loss = 0.27795672\nIteration 67, loss = 0.27718478\nIteration 68, loss = 0.27597225\nIteration 69, loss = 0.27609302\nIteration 70, loss = 0.27397996\nIteration 71, loss = 0.27466813\nIteration 72, loss = 0.27362282\nIteration 73, loss = 0.27272222\nIteration 74, loss = 0.27248734\nIteration 75, loss = 0.27192706\nIteration 76, loss = 0.27171739\nIteration 77, loss = 0.27020130\nIteration 78, loss = 0.26908215\nIteration 79, loss = 0.26893317\nIteration 80, loss = 0.26892868\nIteration 81, loss = 0.26801349\nIteration 82, loss = 0.26771005\nIteration 83, loss = 0.26676438\nIteration 84, loss = 0.26607002\nIteration 85, loss = 0.26526185\nIteration 86, loss = 0.26461192\nIteration 87, loss = 0.26445636\nIteration 88, loss = 0.26352868\nIteration 89, loss = 0.26297140\nIteration 90, loss = 0.26226281\nIteration 91, loss = 0.26147978\nIteration 92, loss = 0.26204596\nIteration 93, loss = 0.26090694\nIteration 94, loss = 0.25938931\nIteration 95, loss = 0.25932176\nIteration 96, loss = 0.25870694\nIteration 97, loss = 0.25908901\nIteration 98, loss = 0.25817392\nIteration 99, loss = 0.25788436\nIteration 100, loss = 0.25777559\nIteration 101, loss = 0.25679971\nIteration 102, loss = 0.25610274\nIteration 103, loss = 0.25581332\nIteration 104, loss = 0.25532964\nIteration 105, loss = 0.25350236\nIteration 106, loss = 0.25339068\nIteration 107, loss = 0.25307639\nIteration 108, loss = 0.25377262\nIteration 109, loss = 0.25178916\nIteration 110, loss = 0.25215753\nIteration 111, loss = 0.25097898\nIteration 112, loss = 0.25140594\nIteration 113, loss = 0.25108206\nIteration 114, loss = 0.25071765\nIteration 115, loss = 0.25031730\nIteration 116, loss = 0.24859965\nIteration 117, loss = 0.24884257\nIteration 118, loss = 0.24860722\nIteration 119, loss = 0.24803611\nIteration 120, loss = 0.24711165\nIteration 121, loss = 0.24809302\nIteration 122, loss = 0.24747623\nIteration 123, loss = 0.24705072\nIteration 124, loss = 0.24622359\nIteration 125, loss = 0.24592509\nIteration 126, loss = 0.24516059\nIteration 127, loss = 0.24443647\nIteration 128, loss = 0.24424658\nIteration 129, loss = 0.24437361\nIteration 130, loss = 0.24438685\nIteration 131, loss = 0.24282564\nIteration 132, loss = 0.24290052\nIteration 133, loss = 0.24222667\nIteration 134, loss = 0.24192597\nIteration 135, loss = 0.24099834\nIteration 136, loss = 0.24170638\nIteration 137, loss = 0.24027212\nIteration 138, loss = 0.24141667\nIteration 139, loss = 0.23921248\nIteration 140, loss = 0.24018871\nIteration 141, loss = 0.23922915\nIteration 142, loss = 0.23908255\nIteration 143, loss = 0.23927017\nIteration 144, loss = 0.23785729\nIteration 145, loss = 0.23822393\nIteration 146, loss = 0.23837848\nIteration 147, loss = 0.23809771\nIteration 148, loss = 0.23639925\nIteration 149, loss = 0.23604237\nIteration 150, loss = 0.23589011\nIteration 151, loss = 0.23548374\nIteration 152, loss = 0.23495839\nIteration 153, loss = 0.23503313\nIteration 154, loss = 0.23426877\nIteration 155, loss = 0.23339339\nIteration 156, loss = 0.23363033\nIteration 157, loss = 0.23286407\nIteration 158, loss = 0.23361834\nIteration 159, loss = 0.23321967\nIteration 160, loss = 0.23220220\nIteration 161, loss = 0.23209541\nIteration 162, loss = 0.23289912\nIteration 163, loss = 0.23191777\nIteration 164, loss = 0.23244963\nIteration 165, loss = 0.23052560\nIteration 166, loss = 0.22984473\nIteration 167, loss = 0.22944590\nIteration 168, loss = 0.23012891\nIteration 169, loss = 0.22942510\nIteration 170, loss = 0.22945190\nIteration 171, loss = 0.22857904\nIteration 172, loss = 0.22753896\nIteration 173, loss = 0.22789515\nIteration 174, loss = 0.22766858\nIteration 175, loss = 0.22748121\nIteration 176, loss = 0.22768119\nIteration 177, loss = 0.22809044\nIteration 178, loss = 0.22647802\nIteration 179, loss = 0.22610297\nIteration 180, loss = 0.22667018\nIteration 181, loss = 0.22502127\nIteration 182, loss = 0.22588457\nIteration 183, loss = 0.22525630\nIteration 184, loss = 0.22547758\nIteration 185, loss = 0.22505898\nIteration 186, loss = 0.22553485\nIteration 187, loss = 0.22420718\nIteration 188, loss = 0.22389199\nIteration 189, loss = 0.22414523\nIteration 190, loss = 0.22414973\nIteration 191, loss = 0.22427242\nIteration 192, loss = 0.22329782\nIteration 193, loss = 0.22277965\nIteration 194, loss = 0.22170432\nIteration 195, loss = 0.22227994\nIteration 196, loss = 0.22156397\nIteration 197, loss = 0.22180869\nIteration 198, loss = 0.22090679\nIteration 199, loss = 0.22095626\nIteration 200, loss = 0.22068784\nIteration 201, loss = 0.22113906\nIteration 202, loss = 0.22100296\nIteration 203, loss = 0.22080930\nIteration 204, loss = 0.22032828\nIteration 205, loss = 0.21906785\nIteration 206, loss = 0.21911713\nIteration 207, loss = 0.21857820\nIteration 208, loss = 0.21939862\nIteration 209, loss = 0.21831969\nIteration 210, loss = 0.21757227\nIteration 211, loss = 0.21838054\nIteration 212, loss = 0.21846432\nIteration 213, loss = 0.21834529\nIteration 214, loss = 0.21734481\nIteration 215, loss = 0.21725800\nIteration 216, loss = 0.21716908\nIteration 217, loss = 0.21724363\nIteration 218, loss = 0.21594440\nIteration 219, loss = 0.21675141\nIteration 220, loss = 0.21633546\nIteration 221, loss = 0.21610311\nIteration 222, loss = 0.21570899\nIteration 223, loss = 0.21483158\nIteration 224, loss = 0.21655513\nIteration 225, loss = 0.21527667\nIteration 226, loss = 0.21462451\nIteration 227, loss = 0.21416734\nIteration 228, loss = 0.21571239\nIteration 229, loss = 0.21434912\nIteration 230, loss = 0.21408099\nIteration 231, loss = 0.21322493\nIteration 232, loss = 0.21306267\nIteration 233, loss = 0.21334698\nIteration 234, loss = 0.21300840\nIteration 235, loss = 0.21293831\nIteration 236, loss = 0.21279461\nIteration 237, loss = 0.21318374\nIteration 238, loss = 0.21246560\nIteration 239, loss = 0.21221538\nIteration 240, loss = 0.21193960\nIteration 241, loss = 0.21208865\nIteration 242, loss = 0.21106264\nIteration 243, loss = 0.21126251\nIteration 244, loss = 0.21145192\nIteration 245, loss = 0.21091094\nIteration 246, loss = 0.21096859\nIteration 247, loss = 0.21045668\nIteration 248, loss = 0.21001984\nIteration 249, loss = 0.20982403\nIteration 250, loss = 0.20974502\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37392952\nIteration 2, loss = 0.34953675\nIteration 3, loss = 0.34293534\nIteration 4, loss = 0.33968502\nIteration 5, loss = 0.33698325\nIteration 6, loss = 0.33459751\nIteration 7, loss = 0.33337362\nIteration 8, loss = 0.33096725\nIteration 9, loss = 0.32999288\nIteration 10, loss = 0.32860035\nIteration 11, loss = 0.32725897\nIteration 12, loss = 0.32563384\nIteration 13, loss = 0.32498528\nIteration 14, loss = 0.32374893\nIteration 15, loss = 0.32285850\nIteration 16, loss = 0.32209395\nIteration 17, loss = 0.32040175\nIteration 18, loss = 0.31905333\nIteration 19, loss = 0.31813099\nIteration 20, loss = 0.31698613\nIteration 21, loss = 0.31682869\nIteration 22, loss = 0.31548006\nIteration 23, loss = 0.31376794\nIteration 24, loss = 0.31280491\nIteration 25, loss = 0.31259082\nIteration 26, loss = 0.31122846\nIteration 27, loss = 0.31062809\nIteration 28, loss = 0.30997851\nIteration 29, loss = 0.30845471\nIteration 30, loss = 0.30773528\nIteration 31, loss = 0.30697114\nIteration 32, loss = 0.30548424\nIteration 33, loss = 0.30470178\nIteration 34, loss = 0.30368186\nIteration 35, loss = 0.30302750\nIteration 36, loss = 0.30154884\nIteration 37, loss = 0.30125900\nIteration 38, loss = 0.30052756\nIteration 39, loss = 0.29937941\nIteration 40, loss = 0.29782760\nIteration 41, loss = 0.29725314\nIteration 42, loss = 0.29610068\nIteration 43, loss = 0.29516201\nIteration 44, loss = 0.29440775\nIteration 45, loss = 0.29329082\nIteration 46, loss = 0.29216575\nIteration 47, loss = 0.29153980\nIteration 48, loss = 0.29044564\nIteration 49, loss = 0.29036295\nIteration 50, loss = 0.28870824\nIteration 51, loss = 0.28795041\nIteration 52, loss = 0.28735750\nIteration 53, loss = 0.28547695\nIteration 54, loss = 0.28566330\nIteration 55, loss = 0.28496307\nIteration 56, loss = 0.28445002\nIteration 57, loss = 0.28307983\nIteration 58, loss = 0.28163779\nIteration 59, loss = 0.28112893\nIteration 60, loss = 0.28051279\nIteration 61, loss = 0.27964060\nIteration 62, loss = 0.27924395\nIteration 63, loss = 0.27764294\nIteration 64, loss = 0.27720326\nIteration 65, loss = 0.27755175\nIteration 66, loss = 0.27590350\nIteration 67, loss = 0.27530884\nIteration 68, loss = 0.27439800\nIteration 69, loss = 0.27419949\nIteration 70, loss = 0.27202435\nIteration 71, loss = 0.27221354\nIteration 72, loss = 0.27212807\nIteration 73, loss = 0.27095745\nIteration 74, loss = 0.27038740\nIteration 75, loss = 0.27039645\nIteration 76, loss = 0.26901873\nIteration 77, loss = 0.26855081\nIteration 78, loss = 0.26864303\nIteration 79, loss = 0.26672052\nIteration 80, loss = 0.26660269\nIteration 81, loss = 0.26626128\nIteration 82, loss = 0.26533500\nIteration 83, loss = 0.26534643\nIteration 84, loss = 0.26447113\nIteration 85, loss = 0.26276139\nIteration 86, loss = 0.26329822\nIteration 87, loss = 0.26267630\nIteration 88, loss = 0.26173260\nIteration 89, loss = 0.26103909\nIteration 90, loss = 0.26037786\nIteration 91, loss = 0.26070482\nIteration 92, loss = 0.25896075\nIteration 93, loss = 0.25840909\nIteration 94, loss = 0.25811977\nIteration 95, loss = 0.25711828\nIteration 96, loss = 0.25734549\nIteration 97, loss = 0.25634581\nIteration 98, loss = 0.25602236\nIteration 99, loss = 0.25569955\nIteration 100, loss = 0.25509414\nIteration 101, loss = 0.25438515\nIteration 102, loss = 0.25472827\nIteration 103, loss = 0.25410053\nIteration 104, loss = 0.25312014\nIteration 105, loss = 0.25258887\nIteration 106, loss = 0.25307010\nIteration 107, loss = 0.25196614\nIteration 108, loss = 0.25167507\nIteration 109, loss = 0.25016677\nIteration 110, loss = 0.25004117\nIteration 111, loss = 0.25042601\nIteration 112, loss = 0.24898367\nIteration 113, loss = 0.24931753\nIteration 114, loss = 0.24908045\nIteration 115, loss = 0.24768329\nIteration 116, loss = 0.24761193\nIteration 117, loss = 0.24654820\nIteration 118, loss = 0.24680858\nIteration 119, loss = 0.24625972\nIteration 120, loss = 0.24552760\nIteration 121, loss = 0.24560192\nIteration 122, loss = 0.24585228\nIteration 123, loss = 0.24462440\nIteration 124, loss = 0.24432991\nIteration 125, loss = 0.24364680\nIteration 126, loss = 0.24365794\nIteration 127, loss = 0.24351770\nIteration 128, loss = 0.24283569\nIteration 129, loss = 0.24237210\nIteration 130, loss = 0.24199779\nIteration 131, loss = 0.24175802\nIteration 132, loss = 0.24052437\nIteration 133, loss = 0.24058384\nIteration 134, loss = 0.23990147\nIteration 135, loss = 0.23953605\nIteration 136, loss = 0.24001503\nIteration 137, loss = 0.23965144\nIteration 138, loss = 0.23838911\nIteration 139, loss = 0.23826355\nIteration 140, loss = 0.23833702\nIteration 141, loss = 0.23718966\nIteration 142, loss = 0.23725802\nIteration 143, loss = 0.23736720\nIteration 144, loss = 0.23648013\nIteration 145, loss = 0.23578851\nIteration 146, loss = 0.23713639\nIteration 147, loss = 0.23558662\nIteration 148, loss = 0.23540587\nIteration 149, loss = 0.23498010\nIteration 150, loss = 0.23395603\nIteration 151, loss = 0.23498778\nIteration 152, loss = 0.23281457\nIteration 153, loss = 0.23354842\nIteration 154, loss = 0.23271171\nIteration 155, loss = 0.23247472\nIteration 156, loss = 0.23323096\nIteration 157, loss = 0.23213099\nIteration 158, loss = 0.23201358\nIteration 159, loss = 0.23157445\nIteration 160, loss = 0.23129289\nIteration 161, loss = 0.23035211\nIteration 162, loss = 0.23061300\nIteration 163, loss = 0.23066779\nIteration 164, loss = 0.23012863\nIteration 165, loss = 0.22932430\nIteration 166, loss = 0.22858763\nIteration 167, loss = 0.22870437\nIteration 168, loss = 0.22838610\nIteration 169, loss = 0.22776623\nIteration 170, loss = 0.22904028\nIteration 171, loss = 0.22817850\nIteration 172, loss = 0.22699141\nIteration 173, loss = 0.22668425\nIteration 174, loss = 0.22758294\nIteration 175, loss = 0.22701142\nIteration 176, loss = 0.22691398\nIteration 177, loss = 0.22611542\nIteration 178, loss = 0.22533964\nIteration 179, loss = 0.22569831\nIteration 180, loss = 0.22431795\nIteration 181, loss = 0.22496816\nIteration 182, loss = 0.22448985\nIteration 183, loss = 0.22484716\nIteration 184, loss = 0.22333676\nIteration 185, loss = 0.22360108\nIteration 186, loss = 0.22389057\nIteration 187, loss = 0.22378903\nIteration 188, loss = 0.22260184\nIteration 189, loss = 0.22353529\nIteration 190, loss = 0.22243583\nIteration 191, loss = 0.22196353\nIteration 192, loss = 0.22274278\nIteration 193, loss = 0.22231670\nIteration 194, loss = 0.22182614\nIteration 195, loss = 0.22215588\nIteration 196, loss = 0.22082634\nIteration 197, loss = 0.22072345\nIteration 198, loss = 0.22054416\nIteration 199, loss = 0.22036270\nIteration 200, loss = 0.22038891\nIteration 201, loss = 0.22003182\nIteration 202, loss = 0.21963673\nIteration 203, loss = 0.22007214\nIteration 204, loss = 0.21939388\nIteration 205, loss = 0.21899597\nIteration 206, loss = 0.21873601\nIteration 207, loss = 0.21770040\nIteration 208, loss = 0.21908254\nIteration 209, loss = 0.21867507\nIteration 210, loss = 0.21767903\nIteration 211, loss = 0.21820703\nIteration 212, loss = 0.21698777\nIteration 213, loss = 0.21759350\nIteration 214, loss = 0.21656594\nIteration 215, loss = 0.21591272\nIteration 216, loss = 0.21709830\nIteration 217, loss = 0.21593799\nIteration 218, loss = 0.21649929\nIteration 219, loss = 0.21552800\nIteration 220, loss = 0.21579526\nIteration 221, loss = 0.21476648\nIteration 222, loss = 0.21529127\nIteration 223, loss = 0.21501166\nIteration 224, loss = 0.21430592\nIteration 225, loss = 0.21489956\nIteration 226, loss = 0.21374097\nIteration 227, loss = 0.21463677\nIteration 228, loss = 0.21405195\nIteration 229, loss = 0.21334803\nIteration 230, loss = 0.21288383\nIteration 231, loss = 0.21369249\nIteration 232, loss = 0.21372878\nIteration 233, loss = 0.21313172\nIteration 234, loss = 0.21245240\nIteration 235, loss = 0.21243138\nIteration 236, loss = 0.21204738\nIteration 237, loss = 0.21291375\nIteration 238, loss = 0.21157819\nIteration 239, loss = 0.21178015\nIteration 240, loss = 0.21098939\nIteration 241, loss = 0.21107179\nIteration 242, loss = 0.21035083\nIteration 243, loss = 0.21055961\nIteration 244, loss = 0.21105326\nIteration 245, loss = 0.20978336\nIteration 246, loss = 0.21068717\nIteration 247, loss = 0.20970028\nIteration 248, loss = 0.21031491\nIteration 249, loss = 0.20913099\nIteration 250, loss = 0.20839416\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37703890\nIteration 2, loss = 0.34863198\nIteration 3, loss = 0.34204963\nIteration 4, loss = 0.33928587\nIteration 5, loss = 0.33723324\nIteration 6, loss = 0.33459579\nIteration 7, loss = 0.33272303\nIteration 8, loss = 0.33187236\nIteration 9, loss = 0.33003156\nIteration 10, loss = 0.32885428\nIteration 11, loss = 0.32749383\nIteration 12, loss = 0.32646937\nIteration 13, loss = 0.32510683\nIteration 14, loss = 0.32425146\nIteration 15, loss = 0.32269478\nIteration 16, loss = 0.32178717\nIteration 17, loss = 0.32075831\nIteration 18, loss = 0.31987084\nIteration 19, loss = 0.31839094\nIteration 20, loss = 0.31760246\nIteration 21, loss = 0.31632233\nIteration 22, loss = 0.31512148\nIteration 23, loss = 0.31401464\nIteration 24, loss = 0.31279954\nIteration 25, loss = 0.31189595\nIteration 26, loss = 0.31147819\nIteration 27, loss = 0.31025865\nIteration 28, loss = 0.30921746\nIteration 29, loss = 0.30781235\nIteration 30, loss = 0.30752346\nIteration 31, loss = 0.30637655\nIteration 32, loss = 0.30540891\nIteration 33, loss = 0.30467376\nIteration 34, loss = 0.30353966\nIteration 35, loss = 0.30277210\nIteration 36, loss = 0.30197051\nIteration 37, loss = 0.30061157\nIteration 38, loss = 0.29982425\nIteration 39, loss = 0.29961711\nIteration 40, loss = 0.29831180\nIteration 41, loss = 0.29716635\nIteration 42, loss = 0.29653347\nIteration 43, loss = 0.29553855\nIteration 44, loss = 0.29463706\nIteration 45, loss = 0.29435401\nIteration 46, loss = 0.29295941\nIteration 47, loss = 0.29269036\nIteration 48, loss = 0.29140645\nIteration 49, loss = 0.29075792\nIteration 50, loss = 0.29091904\nIteration 51, loss = 0.28951063\nIteration 52, loss = 0.28859038\nIteration 53, loss = 0.28784620\nIteration 54, loss = 0.28728018\nIteration 55, loss = 0.28657286\nIteration 56, loss = 0.28607819\nIteration 57, loss = 0.28572875\nIteration 58, loss = 0.28423191\nIteration 59, loss = 0.28315767\nIteration 60, loss = 0.28284361\nIteration 61, loss = 0.28210940\nIteration 62, loss = 0.28140812\nIteration 63, loss = 0.28173004\nIteration 64, loss = 0.27994945\nIteration 65, loss = 0.27929736\nIteration 66, loss = 0.27886520\nIteration 67, loss = 0.27809593\nIteration 68, loss = 0.27684797\nIteration 69, loss = 0.27703749\nIteration 70, loss = 0.27663256\nIteration 71, loss = 0.27540082\nIteration 72, loss = 0.27457802\nIteration 73, loss = 0.27425480\nIteration 74, loss = 0.27324663\nIteration 75, loss = 0.27342794\nIteration 76, loss = 0.27279355\nIteration 77, loss = 0.27209617\nIteration 78, loss = 0.27147111\nIteration 79, loss = 0.27120325\nIteration 80, loss = 0.27018085\nIteration 81, loss = 0.27013377\nIteration 82, loss = 0.26942365\nIteration 83, loss = 0.26847740\nIteration 84, loss = 0.26795804\nIteration 85, loss = 0.26773646\nIteration 86, loss = 0.26762476\nIteration 87, loss = 0.26619650\nIteration 88, loss = 0.26523922\nIteration 89, loss = 0.26580514\nIteration 90, loss = 0.26471718\nIteration 91, loss = 0.26455960\nIteration 92, loss = 0.26350373\nIteration 93, loss = 0.26319525\nIteration 94, loss = 0.26309846\nIteration 95, loss = 0.26248499\nIteration 96, loss = 0.26163250\nIteration 97, loss = 0.26107975\nIteration 98, loss = 0.26022352\nIteration 99, loss = 0.26078427\nIteration 100, loss = 0.26061711\nIteration 101, loss = 0.25872675\nIteration 102, loss = 0.25948281\nIteration 103, loss = 0.25833863\nIteration 104, loss = 0.25722105\nIteration 105, loss = 0.25803652\nIteration 106, loss = 0.25696326\nIteration 107, loss = 0.25602930\nIteration 108, loss = 0.25600619\nIteration 109, loss = 0.25580488\nIteration 110, loss = 0.25524214\nIteration 111, loss = 0.25391247\nIteration 112, loss = 0.25374233\nIteration 113, loss = 0.25378541\nIteration 114, loss = 0.25349718\nIteration 115, loss = 0.25212203\nIteration 116, loss = 0.25163561\nIteration 117, loss = 0.25212464\nIteration 118, loss = 0.25164752\nIteration 119, loss = 0.25110328\nIteration 120, loss = 0.25009173\nIteration 121, loss = 0.24992799\nIteration 122, loss = 0.25014625\nIteration 123, loss = 0.24908555\nIteration 124, loss = 0.24938074\nIteration 125, loss = 0.24752920\nIteration 126, loss = 0.24838434\nIteration 127, loss = 0.24696246\nIteration 128, loss = 0.24737314\nIteration 129, loss = 0.24730302\nIteration 130, loss = 0.24676889\nIteration 131, loss = 0.24591213\nIteration 132, loss = 0.24590835\nIteration 133, loss = 0.24548115\nIteration 134, loss = 0.24465290\nIteration 135, loss = 0.24487788\nIteration 136, loss = 0.24456872\nIteration 137, loss = 0.24426043\nIteration 138, loss = 0.24344567\nIteration 139, loss = 0.24367836\nIteration 140, loss = 0.24321483\nIteration 141, loss = 0.24259409\nIteration 142, loss = 0.24168991\nIteration 143, loss = 0.24147173\nIteration 144, loss = 0.24201531\nIteration 145, loss = 0.24140005\nIteration 146, loss = 0.24107010\nIteration 147, loss = 0.24040441\nIteration 148, loss = 0.24023909\nIteration 149, loss = 0.23939169\nIteration 150, loss = 0.24044139\nIteration 151, loss = 0.23949250\nIteration 152, loss = 0.23969849\nIteration 153, loss = 0.23870333\nIteration 154, loss = 0.23825706\nIteration 155, loss = 0.23831925\nIteration 156, loss = 0.23824251\nIteration 157, loss = 0.23670634\nIteration 158, loss = 0.23643371\nIteration 159, loss = 0.23714812\nIteration 160, loss = 0.23711701\nIteration 161, loss = 0.23637458\nIteration 162, loss = 0.23530450\nIteration 163, loss = 0.23643704\nIteration 164, loss = 0.23482258\nIteration 165, loss = 0.23547889\nIteration 166, loss = 0.23475896\nIteration 167, loss = 0.23472401\nIteration 168, loss = 0.23472029\nIteration 169, loss = 0.23503864\nIteration 170, loss = 0.23273108\nIteration 171, loss = 0.23404592\nIteration 172, loss = 0.23291044\nIteration 173, loss = 0.23284479\nIteration 174, loss = 0.23324362\nIteration 175, loss = 0.23200673\nIteration 176, loss = 0.23298794\nIteration 177, loss = 0.23186747\nIteration 178, loss = 0.23124915\nIteration 179, loss = 0.23129763\nIteration 180, loss = 0.23111011\nIteration 181, loss = 0.23092624\nIteration 182, loss = 0.23120753\nIteration 183, loss = 0.23045432\nIteration 184, loss = 0.23041745\nIteration 185, loss = 0.22997275\nIteration 186, loss = 0.22935780\nIteration 187, loss = 0.22933138\nIteration 188, loss = 0.22985313\nIteration 189, loss = 0.22874749\nIteration 190, loss = 0.22856175\nIteration 191, loss = 0.22845868\nIteration 192, loss = 0.22915577\nIteration 193, loss = 0.22874794\nIteration 194, loss = 0.22804150\nIteration 195, loss = 0.22752522\nIteration 196, loss = 0.22729317\nIteration 197, loss = 0.22640859\nIteration 198, loss = 0.22734282\nIteration 199, loss = 0.22685429\nIteration 200, loss = 0.22657128\nIteration 201, loss = 0.22669082\nIteration 202, loss = 0.22575414\nIteration 203, loss = 0.22517725\nIteration 204, loss = 0.22528849\nIteration 205, loss = 0.22532214\nIteration 206, loss = 0.22440064\nIteration 207, loss = 0.22458533\nIteration 208, loss = 0.22572231\nIteration 209, loss = 0.22426999\nIteration 210, loss = 0.22435826\nIteration 211, loss = 0.22375239\nIteration 212, loss = 0.22348941\nIteration 213, loss = 0.22402066\nIteration 214, loss = 0.22343840\nIteration 215, loss = 0.22303626\nIteration 216, loss = 0.22281806\nIteration 217, loss = 0.22175893\nIteration 218, loss = 0.22284614\nIteration 219, loss = 0.22263548\nIteration 220, loss = 0.22207383\nIteration 221, loss = 0.22245262\nIteration 222, loss = 0.22147710\nIteration 223, loss = 0.22202808\nIteration 224, loss = 0.22173029\nIteration 225, loss = 0.22097598\nIteration 226, loss = 0.22163410\nIteration 227, loss = 0.22066483\nIteration 228, loss = 0.21948474\nIteration 229, loss = 0.22028207\nIteration 230, loss = 0.21954209\nIteration 231, loss = 0.21911356\nIteration 232, loss = 0.21960449\nIteration 233, loss = 0.22012603\nIteration 234, loss = 0.21957635\nIteration 235, loss = 0.21881724\nIteration 236, loss = 0.21795743\nIteration 237, loss = 0.21882977\nIteration 238, loss = 0.21887188\nIteration 239, loss = 0.21837940\nIteration 240, loss = 0.21850050\nIteration 241, loss = 0.21771867\nIteration 242, loss = 0.21874919\nIteration 243, loss = 0.21678511\nIteration 244, loss = 0.21695215\nIteration 245, loss = 0.21739497\nIteration 246, loss = 0.21716094\nIteration 247, loss = 0.21751344\nIteration 248, loss = 0.21669292\nIteration 249, loss = 0.21623208\nIteration 250, loss = 0.21572034\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37585107\nIteration 2, loss = 0.34697579\nIteration 3, loss = 0.34038160\nIteration 4, loss = 0.33821271\nIteration 5, loss = 0.33556148\nIteration 6, loss = 0.33340195\nIteration 7, loss = 0.33187249\nIteration 8, loss = 0.33057314\nIteration 9, loss = 0.32920129\nIteration 10, loss = 0.32787972\nIteration 11, loss = 0.32712618\nIteration 12, loss = 0.32575109\nIteration 13, loss = 0.32426244\nIteration 14, loss = 0.32389602\nIteration 15, loss = 0.32175008\nIteration 16, loss = 0.32076079\nIteration 17, loss = 0.31992265\nIteration 18, loss = 0.31863120\nIteration 19, loss = 0.31759738\nIteration 20, loss = 0.31640087\nIteration 21, loss = 0.31525293\nIteration 22, loss = 0.31431758\nIteration 23, loss = 0.31353598\nIteration 24, loss = 0.31230715\nIteration 25, loss = 0.31071185\nIteration 26, loss = 0.31038336\nIteration 27, loss = 0.30910407\nIteration 28, loss = 0.30842330\nIteration 29, loss = 0.30781624\nIteration 30, loss = 0.30637822\nIteration 31, loss = 0.30539130\nIteration 32, loss = 0.30447565\nIteration 33, loss = 0.30352807\nIteration 34, loss = 0.30269752\nIteration 35, loss = 0.30242067\nIteration 36, loss = 0.30088552\nIteration 37, loss = 0.30051943\nIteration 38, loss = 0.29944533\nIteration 39, loss = 0.29886838\nIteration 40, loss = 0.29795578\nIteration 41, loss = 0.29673916\nIteration 42, loss = 0.29525951\nIteration 43, loss = 0.29467617\nIteration 44, loss = 0.29395016\nIteration 45, loss = 0.29322577\nIteration 46, loss = 0.29226515\nIteration 47, loss = 0.29185984\nIteration 48, loss = 0.29047270\nIteration 49, loss = 0.28995887\nIteration 50, loss = 0.28912931\nIteration 51, loss = 0.28808981\nIteration 52, loss = 0.28742229\nIteration 53, loss = 0.28691975\nIteration 54, loss = 0.28628145\nIteration 55, loss = 0.28588809\nIteration 56, loss = 0.28516184\nIteration 57, loss = 0.28392411\nIteration 58, loss = 0.28371998\nIteration 59, loss = 0.28223998\nIteration 60, loss = 0.28183582\nIteration 61, loss = 0.28139623\nIteration 62, loss = 0.28019181\nIteration 63, loss = 0.28057392\nIteration 64, loss = 0.27901440\nIteration 65, loss = 0.27833612\nIteration 66, loss = 0.27764736\nIteration 67, loss = 0.27695009\nIteration 68, loss = 0.27602464\nIteration 69, loss = 0.27568233\nIteration 70, loss = 0.27579117\nIteration 71, loss = 0.27522955\nIteration 72, loss = 0.27365031\nIteration 73, loss = 0.27366269\nIteration 74, loss = 0.27287396\nIteration 75, loss = 0.27246933\nIteration 76, loss = 0.27163510\nIteration 77, loss = 0.27129608\nIteration 78, loss = 0.27133336\nIteration 79, loss = 0.27045685\nIteration 80, loss = 0.26967847\nIteration 81, loss = 0.26909947\nIteration 82, loss = 0.26855379\nIteration 83, loss = 0.26783564\nIteration 84, loss = 0.26764647\nIteration 85, loss = 0.26733461\nIteration 86, loss = 0.26687209\nIteration 87, loss = 0.26634846\nIteration 88, loss = 0.26445847\nIteration 89, loss = 0.26558504\nIteration 90, loss = 0.26435986\nIteration 91, loss = 0.26421335\nIteration 92, loss = 0.26348805\nIteration 93, loss = 0.26250608\nIteration 94, loss = 0.26235325\nIteration 95, loss = 0.26136576\nIteration 96, loss = 0.26145195\nIteration 97, loss = 0.26048547\nIteration 98, loss = 0.25991040\nIteration 99, loss = 0.25967180\nIteration 100, loss = 0.25933915\nIteration 101, loss = 0.25825835\nIteration 102, loss = 0.25872185\nIteration 103, loss = 0.25822744\nIteration 104, loss = 0.25684702\nIteration 105, loss = 0.25860579\nIteration 106, loss = 0.25641334\nIteration 107, loss = 0.25550473\nIteration 108, loss = 0.25507669\nIteration 109, loss = 0.25598095\nIteration 110, loss = 0.25574900\nIteration 111, loss = 0.25384774\nIteration 112, loss = 0.25355794\nIteration 113, loss = 0.25279867\nIteration 114, loss = 0.25351819\nIteration 115, loss = 0.25255499\nIteration 116, loss = 0.25162979\nIteration 117, loss = 0.25118743\nIteration 118, loss = 0.25091764\nIteration 119, loss = 0.25146241\nIteration 120, loss = 0.25017922\nIteration 121, loss = 0.25068394\nIteration 122, loss = 0.24949692\nIteration 123, loss = 0.24913837\nIteration 124, loss = 0.24941326\nIteration 125, loss = 0.24854639\nIteration 126, loss = 0.24823550\nIteration 127, loss = 0.24761210\nIteration 128, loss = 0.24734922\nIteration 129, loss = 0.24607253\nIteration 130, loss = 0.24719804\nIteration 131, loss = 0.24596332\nIteration 132, loss = 0.24569892\nIteration 133, loss = 0.24570594\nIteration 134, loss = 0.24495676\nIteration 135, loss = 0.24545246\nIteration 136, loss = 0.24409444\nIteration 137, loss = 0.24427835\nIteration 138, loss = 0.24347799\nIteration 139, loss = 0.24383231\nIteration 140, loss = 0.24342017\nIteration 141, loss = 0.24291943\nIteration 142, loss = 0.24256848\nIteration 143, loss = 0.24133479\nIteration 144, loss = 0.24196073\nIteration 145, loss = 0.24088722\nIteration 146, loss = 0.24037730\nIteration 147, loss = 0.24055624\nIteration 148, loss = 0.24042973\nIteration 149, loss = 0.24030377\nIteration 150, loss = 0.24048507\nIteration 151, loss = 0.23954481\nIteration 152, loss = 0.23910174\nIteration 153, loss = 0.23811561\nIteration 154, loss = 0.23809675\nIteration 155, loss = 0.23877001\nIteration 156, loss = 0.23801669\nIteration 157, loss = 0.23719792\nIteration 158, loss = 0.23648070\nIteration 159, loss = 0.23658559\nIteration 160, loss = 0.23574828\nIteration 161, loss = 0.23638747\nIteration 162, loss = 0.23670257\nIteration 163, loss = 0.23533104\nIteration 164, loss = 0.23562103\nIteration 165, loss = 0.23517557\nIteration 166, loss = 0.23460324\nIteration 167, loss = 0.23444993\nIteration 168, loss = 0.23374333\nIteration 169, loss = 0.23456944\nIteration 170, loss = 0.23306190\nIteration 171, loss = 0.23347457\nIteration 172, loss = 0.23269085\nIteration 173, loss = 0.23306733\nIteration 174, loss = 0.23355636\nIteration 175, loss = 0.23144627\nIteration 176, loss = 0.23219205\nIteration 177, loss = 0.23171591\nIteration 178, loss = 0.23165315\nIteration 179, loss = 0.23082908\nIteration 180, loss = 0.23062032\nIteration 181, loss = 0.23057591\nIteration 182, loss = 0.23042646\nIteration 183, loss = 0.22961024\nIteration 184, loss = 0.22970518\nIteration 185, loss = 0.22895530\nIteration 186, loss = 0.22991969\nIteration 187, loss = 0.22996341\nIteration 188, loss = 0.22927193\nIteration 189, loss = 0.22790993\nIteration 190, loss = 0.22814647\nIteration 191, loss = 0.22749723\nIteration 192, loss = 0.22767562\nIteration 193, loss = 0.22834399\nIteration 194, loss = 0.22694174\nIteration 195, loss = 0.22677506\nIteration 196, loss = 0.22685051\nIteration 197, loss = 0.22691747\nIteration 198, loss = 0.22731633\nIteration 199, loss = 0.22700278\nIteration 200, loss = 0.22576885\nIteration 201, loss = 0.22586385\nIteration 202, loss = 0.22633312\nIteration 203, loss = 0.22543780\nIteration 204, loss = 0.22462413\nIteration 205, loss = 0.22546519\nIteration 206, loss = 0.22514415\nIteration 207, loss = 0.22370079\nIteration 208, loss = 0.22522824\nIteration 209, loss = 0.22437749\nIteration 210, loss = 0.22393630\nIteration 211, loss = 0.22329103\nIteration 212, loss = 0.22277792\nIteration 213, loss = 0.22325689\nIteration 214, loss = 0.22289891\nIteration 215, loss = 0.22184844\nIteration 216, loss = 0.22338208\nIteration 217, loss = 0.22176065\nIteration 218, loss = 0.22144075\nIteration 219, loss = 0.22127759\nIteration 220, loss = 0.22144747\nIteration 221, loss = 0.22092977\nIteration 222, loss = 0.22129960\nIteration 223, loss = 0.22080208\nIteration 224, loss = 0.22035580\nIteration 225, loss = 0.22070834\nIteration 226, loss = 0.22117377\nIteration 227, loss = 0.22012301\nIteration 228, loss = 0.22033490\nIteration 229, loss = 0.21959134\nIteration 230, loss = 0.21980457\nIteration 231, loss = 0.21857209\nIteration 232, loss = 0.21872740\nIteration 233, loss = 0.21885560\nIteration 234, loss = 0.21901578\nIteration 235, loss = 0.21905458\nIteration 236, loss = 0.21851269\nIteration 237, loss = 0.21914734\nIteration 238, loss = 0.21727396\nIteration 239, loss = 0.21774604\nIteration 240, loss = 0.21699391\nIteration 241, loss = 0.21783869\nIteration 242, loss = 0.21760954\nIteration 243, loss = 0.21703397\nIteration 244, loss = 0.21705196\nIteration 245, loss = 0.21781022\nIteration 246, loss = 0.21548901\nIteration 247, loss = 0.21636949\nIteration 248, loss = 0.21623444\nIteration 249, loss = 0.21619966\nIteration 250, loss = 0.21541894\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37798522\nIteration 2, loss = 0.34843128\nIteration 3, loss = 0.34266813\nIteration 4, loss = 0.33992523\nIteration 5, loss = 0.33795816\nIteration 6, loss = 0.33587738\nIteration 7, loss = 0.33406121\nIteration 8, loss = 0.33237042\nIteration 9, loss = 0.33154567\nIteration 10, loss = 0.33010312\nIteration 11, loss = 0.32857588\nIteration 12, loss = 0.32712373\nIteration 13, loss = 0.32650347\nIteration 14, loss = 0.32554202\nIteration 15, loss = 0.32425101\nIteration 16, loss = 0.32280978\nIteration 17, loss = 0.32261673\nIteration 18, loss = 0.32087989\nIteration 19, loss = 0.31996346\nIteration 20, loss = 0.31867925\nIteration 21, loss = 0.31793783\nIteration 22, loss = 0.31683788\nIteration 23, loss = 0.31598437\nIteration 24, loss = 0.31474077\nIteration 25, loss = 0.31383630\nIteration 26, loss = 0.31204360\nIteration 27, loss = 0.31095042\nIteration 28, loss = 0.31052891\nIteration 29, loss = 0.30904448\nIteration 30, loss = 0.30833125\nIteration 31, loss = 0.30716043\nIteration 32, loss = 0.30614201\nIteration 33, loss = 0.30541585\nIteration 34, loss = 0.30374793\nIteration 35, loss = 0.30302201\nIteration 36, loss = 0.30189581\nIteration 37, loss = 0.30174840\nIteration 38, loss = 0.30041890\nIteration 39, loss = 0.30025615\nIteration 40, loss = 0.29835523\nIteration 41, loss = 0.29820533\nIteration 42, loss = 0.29690418\nIteration 43, loss = 0.29569337\nIteration 44, loss = 0.29469477\nIteration 45, loss = 0.29445521\nIteration 46, loss = 0.29345639\nIteration 47, loss = 0.29204819\nIteration 48, loss = 0.29199822\nIteration 49, loss = 0.29047194\nIteration 50, loss = 0.28955324\nIteration 51, loss = 0.28957934\nIteration 52, loss = 0.28845467\nIteration 53, loss = 0.28689195\nIteration 54, loss = 0.28674383\nIteration 55, loss = 0.28562566\nIteration 56, loss = 0.28506208\nIteration 57, loss = 0.28437973\nIteration 58, loss = 0.28366234\nIteration 59, loss = 0.28312927\nIteration 60, loss = 0.28215792\nIteration 61, loss = 0.28130976\nIteration 62, loss = 0.28057237\nIteration 63, loss = 0.28011380\nIteration 64, loss = 0.27932665\nIteration 65, loss = 0.27814393\nIteration 66, loss = 0.27782674\nIteration 67, loss = 0.27706209\nIteration 68, loss = 0.27669708\nIteration 69, loss = 0.27531891\nIteration 70, loss = 0.27599631\nIteration 71, loss = 0.27497410\nIteration 72, loss = 0.27430789\nIteration 73, loss = 0.27307303\nIteration 74, loss = 0.27190731\nIteration 75, loss = 0.27243190\nIteration 76, loss = 0.27172850\nIteration 77, loss = 0.27079078\nIteration 78, loss = 0.27056380\nIteration 79, loss = 0.26939856\nIteration 80, loss = 0.26905301\nIteration 81, loss = 0.26769970\nIteration 82, loss = 0.26834996\nIteration 83, loss = 0.26729186\nIteration 84, loss = 0.26702043\nIteration 85, loss = 0.26645143\nIteration 86, loss = 0.26604249\nIteration 87, loss = 0.26474512\nIteration 88, loss = 0.26490862\nIteration 89, loss = 0.26442943\nIteration 90, loss = 0.26346668\nIteration 91, loss = 0.26340027\nIteration 92, loss = 0.26278267\nIteration 93, loss = 0.26221645\nIteration 94, loss = 0.26136887\nIteration 95, loss = 0.26054651\nIteration 96, loss = 0.26043769\nIteration 97, loss = 0.25985603\nIteration 98, loss = 0.26004059\nIteration 99, loss = 0.25945892\nIteration 100, loss = 0.25902988\nIteration 101, loss = 0.25766485\nIteration 102, loss = 0.25823377\nIteration 103, loss = 0.25711949\nIteration 104, loss = 0.25628068\nIteration 105, loss = 0.25558428\nIteration 106, loss = 0.25548691\nIteration 107, loss = 0.25497349\nIteration 108, loss = 0.25577735\nIteration 109, loss = 0.25455647\nIteration 110, loss = 0.25568513\nIteration 111, loss = 0.25375762\nIteration 112, loss = 0.25307828\nIteration 113, loss = 0.25247040\nIteration 114, loss = 0.25221882\nIteration 115, loss = 0.25166558\nIteration 116, loss = 0.25117444\nIteration 117, loss = 0.25131104\nIteration 118, loss = 0.25108505\nIteration 119, loss = 0.25027726\nIteration 120, loss = 0.25012849\nIteration 121, loss = 0.24902232\nIteration 122, loss = 0.24891696\nIteration 123, loss = 0.24872024\nIteration 124, loss = 0.24785821\nIteration 125, loss = 0.24773272\nIteration 126, loss = 0.24791842\nIteration 127, loss = 0.24697288\nIteration 128, loss = 0.24606256\nIteration 129, loss = 0.24554089\nIteration 130, loss = 0.24564254\nIteration 131, loss = 0.24571464\nIteration 132, loss = 0.24469891\nIteration 133, loss = 0.24400723\nIteration 134, loss = 0.24457044\nIteration 135, loss = 0.24395797\nIteration 136, loss = 0.24290912\nIteration 137, loss = 0.24384307\nIteration 138, loss = 0.24261802\nIteration 139, loss = 0.24252776\nIteration 140, loss = 0.24142129\nIteration 141, loss = 0.24184346\nIteration 142, loss = 0.24171599\nIteration 143, loss = 0.24063074\nIteration 144, loss = 0.24079283\nIteration 145, loss = 0.23993308\nIteration 146, loss = 0.24040638\nIteration 147, loss = 0.23969789\nIteration 148, loss = 0.23871958\nIteration 149, loss = 0.23938062\nIteration 150, loss = 0.23820535\nIteration 151, loss = 0.23789525\nIteration 152, loss = 0.23781464\nIteration 153, loss = 0.23822907\nIteration 154, loss = 0.23787934\nIteration 155, loss = 0.23789155\nIteration 156, loss = 0.23639756\nIteration 157, loss = 0.23720336\nIteration 158, loss = 0.23619081\nIteration 159, loss = 0.23470982\nIteration 160, loss = 0.23594156\nIteration 161, loss = 0.23568005\nIteration 162, loss = 0.23486383\nIteration 163, loss = 0.23422077\nIteration 164, loss = 0.23410588\nIteration 165, loss = 0.23370134\nIteration 166, loss = 0.23333385\nIteration 167, loss = 0.23375052\nIteration 168, loss = 0.23358698\nIteration 169, loss = 0.23298888\nIteration 170, loss = 0.23243531\nIteration 171, loss = 0.23191277\nIteration 172, loss = 0.23171301\nIteration 173, loss = 0.23119533\nIteration 174, loss = 0.23192909\nIteration 175, loss = 0.23168682\nIteration 176, loss = 0.23147632\nIteration 177, loss = 0.22985839\nIteration 178, loss = 0.23187537\nIteration 179, loss = 0.23016726\nIteration 180, loss = 0.22929272\nIteration 181, loss = 0.22885899\nIteration 182, loss = 0.22900035\nIteration 183, loss = 0.22838435\nIteration 184, loss = 0.22861821\nIteration 185, loss = 0.22847408\nIteration 186, loss = 0.22917709\nIteration 187, loss = 0.22836198\nIteration 188, loss = 0.22728702\nIteration 189, loss = 0.22728940\nIteration 190, loss = 0.22672546\nIteration 191, loss = 0.22587015\nIteration 192, loss = 0.22680781\nIteration 193, loss = 0.22618215\nIteration 194, loss = 0.22584843\nIteration 195, loss = 0.22570947\nIteration 196, loss = 0.22540489\nIteration 197, loss = 0.22500720\nIteration 198, loss = 0.22560660\nIteration 199, loss = 0.22428240\nIteration 200, loss = 0.22463168\nIteration 201, loss = 0.22397347\nIteration 202, loss = 0.22410857\nIteration 203, loss = 0.22448924\nIteration 204, loss = 0.22330147\nIteration 205, loss = 0.22383552\nIteration 206, loss = 0.22326751\nIteration 207, loss = 0.22338084\nIteration 208, loss = 0.22236622\nIteration 209, loss = 0.22311886\nIteration 210, loss = 0.22222421\nIteration 211, loss = 0.22238132\nIteration 212, loss = 0.22196411\nIteration 213, loss = 0.22134484\nIteration 214, loss = 0.22154137\nIteration 215, loss = 0.22097956\nIteration 216, loss = 0.22043254\nIteration 217, loss = 0.22150202\nIteration 218, loss = 0.22079789\nIteration 219, loss = 0.22016593\nIteration 220, loss = 0.22016621\nIteration 221, loss = 0.21939171\nIteration 222, loss = 0.21916202\nIteration 223, loss = 0.22044274\nIteration 224, loss = 0.21936057\nIteration 225, loss = 0.21883721\nIteration 226, loss = 0.21905862\nIteration 227, loss = 0.21888755\nIteration 228, loss = 0.21852648\nIteration 229, loss = 0.21888513\nIteration 230, loss = 0.21797032\nIteration 231, loss = 0.21755773\nIteration 232, loss = 0.21797507\nIteration 233, loss = 0.21733034\nIteration 234, loss = 0.21741918\nIteration 235, loss = 0.21739366\nIteration 236, loss = 0.21571754\nIteration 237, loss = 0.21674801\nIteration 238, loss = 0.21694224\nIteration 239, loss = 0.21557042\nIteration 240, loss = 0.21599857\nIteration 241, loss = 0.21583325\nIteration 242, loss = 0.21569598\nIteration 243, loss = 0.21531318\nIteration 244, loss = 0.21467893\nIteration 245, loss = 0.21508341\nIteration 246, loss = 0.21480790\nIteration 247, loss = 0.21548548\nIteration 248, loss = 0.21479347\nIteration 249, loss = 0.21483682\nIteration 250, loss = 0.21402454\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37719464\nIteration 2, loss = 0.34799239\nIteration 3, loss = 0.34218473\nIteration 4, loss = 0.33920146\nIteration 5, loss = 0.33692604\nIteration 6, loss = 0.33486459\nIteration 7, loss = 0.33216051\nIteration 8, loss = 0.33135662\nIteration 9, loss = 0.32995020\nIteration 10, loss = 0.32852547\nIteration 11, loss = 0.32753012\nIteration 12, loss = 0.32603462\nIteration 13, loss = 0.32533053\nIteration 14, loss = 0.32418066\nIteration 15, loss = 0.32341332\nIteration 16, loss = 0.32204131\nIteration 17, loss = 0.32115945\nIteration 18, loss = 0.31992656\nIteration 19, loss = 0.31873767\nIteration 20, loss = 0.31787094\nIteration 21, loss = 0.31736352\nIteration 22, loss = 0.31555180\nIteration 23, loss = 0.31515110\nIteration 24, loss = 0.31368771\nIteration 25, loss = 0.31300651\nIteration 26, loss = 0.31194569\nIteration 27, loss = 0.31090707\nIteration 28, loss = 0.31056608\nIteration 29, loss = 0.30930160\nIteration 30, loss = 0.30825776\nIteration 31, loss = 0.30708097\nIteration 32, loss = 0.30648171\nIteration 33, loss = 0.30578315\nIteration 34, loss = 0.30403429\nIteration 35, loss = 0.30322384\nIteration 36, loss = 0.30289349\nIteration 37, loss = 0.30224462\nIteration 38, loss = 0.30159807\nIteration 39, loss = 0.30097490\nIteration 40, loss = 0.29961617\nIteration 41, loss = 0.29886557\nIteration 42, loss = 0.29796387\nIteration 43, loss = 0.29716227\nIteration 44, loss = 0.29585977\nIteration 45, loss = 0.29609138\nIteration 46, loss = 0.29486960\nIteration 47, loss = 0.29350477\nIteration 48, loss = 0.29287384\nIteration 49, loss = 0.29251194\nIteration 50, loss = 0.29149066\nIteration 51, loss = 0.29076016\nIteration 52, loss = 0.28947815\nIteration 53, loss = 0.28893843\nIteration 54, loss = 0.28859159\nIteration 55, loss = 0.28792121\nIteration 56, loss = 0.28730538\nIteration 57, loss = 0.28604483\nIteration 58, loss = 0.28530613\nIteration 59, loss = 0.28448172\nIteration 60, loss = 0.28397678\nIteration 61, loss = 0.28248540\nIteration 62, loss = 0.28231448\nIteration 63, loss = 0.28208422\nIteration 64, loss = 0.28077696\nIteration 65, loss = 0.28023178\nIteration 66, loss = 0.27963067\nIteration 67, loss = 0.27931986\nIteration 68, loss = 0.27914255\nIteration 69, loss = 0.27800313\nIteration 70, loss = 0.27734039\nIteration 71, loss = 0.27749167\nIteration 72, loss = 0.27598468\nIteration 73, loss = 0.27472856\nIteration 74, loss = 0.27514333\nIteration 75, loss = 0.27447073\nIteration 76, loss = 0.27366011\nIteration 77, loss = 0.27319061\nIteration 78, loss = 0.27300190\nIteration 79, loss = 0.27137827\nIteration 80, loss = 0.27203568\nIteration 81, loss = 0.27081715\nIteration 82, loss = 0.26996139\nIteration 83, loss = 0.26978545\nIteration 84, loss = 0.26917742\nIteration 85, loss = 0.26951442\nIteration 86, loss = 0.26780069\nIteration 87, loss = 0.26712106\nIteration 88, loss = 0.26731403\nIteration 89, loss = 0.26666586\nIteration 90, loss = 0.26653837\nIteration 91, loss = 0.26562078\nIteration 92, loss = 0.26489215\nIteration 93, loss = 0.26465818\nIteration 94, loss = 0.26322551\nIteration 95, loss = 0.26263687\nIteration 96, loss = 0.26209166\nIteration 97, loss = 0.26261005\nIteration 98, loss = 0.26236404\nIteration 99, loss = 0.26167183\nIteration 100, loss = 0.26153200\nIteration 101, loss = 0.26093846\nIteration 102, loss = 0.25957544\nIteration 103, loss = 0.25993496\nIteration 104, loss = 0.25918663\nIteration 105, loss = 0.25738071\nIteration 106, loss = 0.25860573\nIteration 107, loss = 0.25794530\nIteration 108, loss = 0.25734974\nIteration 109, loss = 0.25608204\nIteration 110, loss = 0.25677547\nIteration 111, loss = 0.25562792\nIteration 112, loss = 0.25578232\nIteration 113, loss = 0.25456390\nIteration 114, loss = 0.25396825\nIteration 115, loss = 0.25462929\nIteration 116, loss = 0.25388710\nIteration 117, loss = 0.25298349\nIteration 118, loss = 0.25347964\nIteration 119, loss = 0.25245693\nIteration 120, loss = 0.25192601\nIteration 121, loss = 0.25236536\nIteration 122, loss = 0.25081562\nIteration 123, loss = 0.25161705\nIteration 124, loss = 0.25081252\nIteration 125, loss = 0.25018654\nIteration 126, loss = 0.24960760\nIteration 127, loss = 0.25050471\nIteration 128, loss = 0.24888303\nIteration 129, loss = 0.24772772\nIteration 130, loss = 0.24855552\nIteration 131, loss = 0.24791543\nIteration 132, loss = 0.24774404\nIteration 133, loss = 0.24683892\nIteration 134, loss = 0.24709416\nIteration 135, loss = 0.24663534\nIteration 136, loss = 0.24561872\nIteration 137, loss = 0.24588915\nIteration 138, loss = 0.24530891\nIteration 139, loss = 0.24507167\nIteration 140, loss = 0.24518900\nIteration 141, loss = 0.24506893\nIteration 142, loss = 0.24392050\nIteration 143, loss = 0.24403591\nIteration 144, loss = 0.24222686\nIteration 145, loss = 0.24282001\nIteration 146, loss = 0.24361215\nIteration 147, loss = 0.24246272\nIteration 148, loss = 0.24137583\nIteration 149, loss = 0.24221563\nIteration 150, loss = 0.24117748\nIteration 151, loss = 0.24190218\nIteration 152, loss = 0.24082183\nIteration 153, loss = 0.24112152\nIteration 154, loss = 0.23960134\nIteration 155, loss = 0.23994170\nIteration 156, loss = 0.23923681\nIteration 157, loss = 0.23948210\nIteration 158, loss = 0.23861632\nIteration 159, loss = 0.23968513\nIteration 160, loss = 0.23772963\nIteration 161, loss = 0.23842078\nIteration 162, loss = 0.23660362\nIteration 163, loss = 0.23786490\nIteration 164, loss = 0.23719575\nIteration 165, loss = 0.23700914\nIteration 166, loss = 0.23738751\nIteration 167, loss = 0.23621282\nIteration 168, loss = 0.23631179\nIteration 169, loss = 0.23572065\nIteration 170, loss = 0.23684638\nIteration 171, loss = 0.23451616\nIteration 172, loss = 0.23429235\nIteration 173, loss = 0.23567036\nIteration 174, loss = 0.23504318\nIteration 175, loss = 0.23499156\nIteration 176, loss = 0.23369835\nIteration 177, loss = 0.23382857\nIteration 178, loss = 0.23408099\nIteration 179, loss = 0.23355994\nIteration 180, loss = 0.23290003\nIteration 181, loss = 0.23289955\nIteration 182, loss = 0.23310295\nIteration 183, loss = 0.23205970\nIteration 184, loss = 0.23199226\nIteration 185, loss = 0.23197951\nIteration 186, loss = 0.23110514\nIteration 187, loss = 0.23127860\nIteration 188, loss = 0.23109640\nIteration 189, loss = 0.23081527\nIteration 190, loss = 0.23077810\nIteration 191, loss = 0.23029087\nIteration 192, loss = 0.23016289\nIteration 193, loss = 0.22939762\nIteration 194, loss = 0.22992183\nIteration 195, loss = 0.22971697\nIteration 196, loss = 0.22849682\nIteration 197, loss = 0.22872811\nIteration 198, loss = 0.22945675\nIteration 199, loss = 0.22902433\nIteration 200, loss = 0.22866426\nIteration 201, loss = 0.22809323\nIteration 202, loss = 0.22704406\nIteration 203, loss = 0.22732173\nIteration 204, loss = 0.22755557\nIteration 205, loss = 0.22703896\nIteration 206, loss = 0.22619668\nIteration 207, loss = 0.22703117\nIteration 208, loss = 0.22807834\nIteration 209, loss = 0.22607489\nIteration 210, loss = 0.22663658\nIteration 211, loss = 0.22542627\nIteration 212, loss = 0.22588348\nIteration 213, loss = 0.22501528\nIteration 214, loss = 0.22631709\nIteration 215, loss = 0.22559101\nIteration 216, loss = 0.22447726\nIteration 217, loss = 0.22479561\nIteration 218, loss = 0.22547196\nIteration 219, loss = 0.22357581\nIteration 220, loss = 0.22479073\nIteration 221, loss = 0.22367440\nIteration 222, loss = 0.22266500\nIteration 223, loss = 0.22254675\nIteration 224, loss = 0.22322372\nIteration 225, loss = 0.22308450\nIteration 226, loss = 0.22209969\nIteration 227, loss = 0.22281607\nIteration 228, loss = 0.22293195\nIteration 229, loss = 0.22208736\nIteration 230, loss = 0.22233992\nIteration 231, loss = 0.22192921\nIteration 232, loss = 0.22229718\nIteration 233, loss = 0.22088419\nIteration 234, loss = 0.22031212\nIteration 235, loss = 0.22000476\nIteration 236, loss = 0.22088650\nIteration 237, loss = 0.22019319\nIteration 238, loss = 0.22080135\nIteration 239, loss = 0.21968857\nIteration 240, loss = 0.22007083\nIteration 241, loss = 0.22057771\nIteration 242, loss = 0.21959336\nIteration 243, loss = 0.22000855\nIteration 244, loss = 0.21994142\nIteration 245, loss = 0.21890731\nIteration 246, loss = 0.21912569\nIteration 247, loss = 0.21845932\nIteration 248, loss = 0.21778314\nIteration 249, loss = 0.21859812\nIteration 250, loss = 0.21864498\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.37859262\nIteration 2, loss = 0.34847518\nIteration 3, loss = 0.34261773\nIteration 4, loss = 0.34018490\nIteration 5, loss = 0.33761344\nIteration 6, loss = 0.33543466\nIteration 7, loss = 0.33329723\nIteration 8, loss = 0.33193189\nIteration 9, loss = 0.32987800\nIteration 10, loss = 0.32881132\nIteration 11, loss = 0.32748336\nIteration 12, loss = 0.32613817\nIteration 13, loss = 0.32500796\nIteration 14, loss = 0.32403341\nIteration 15, loss = 0.32306450\nIteration 16, loss = 0.32191182\nIteration 17, loss = 0.32081500\nIteration 18, loss = 0.31970399\nIteration 19, loss = 0.31870161\nIteration 20, loss = 0.31710995\nIteration 21, loss = 0.31666468\nIteration 22, loss = 0.31512553\nIteration 23, loss = 0.31457006\nIteration 24, loss = 0.31349458\nIteration 25, loss = 0.31260938\nIteration 26, loss = 0.31112262\nIteration 27, loss = 0.31012319\nIteration 28, loss = 0.30956017\nIteration 29, loss = 0.30812239\nIteration 30, loss = 0.30755859\nIteration 31, loss = 0.30636151\nIteration 32, loss = 0.30543059\nIteration 33, loss = 0.30437053\nIteration 34, loss = 0.30300545\nIteration 35, loss = 0.30221256\nIteration 36, loss = 0.30148723\nIteration 37, loss = 0.30126684\nIteration 38, loss = 0.29985563\nIteration 39, loss = 0.29934051\nIteration 40, loss = 0.29770502\nIteration 41, loss = 0.29677883\nIteration 42, loss = 0.29628630\nIteration 43, loss = 0.29508835\nIteration 44, loss = 0.29445797\nIteration 45, loss = 0.29331969\nIteration 46, loss = 0.29308349\nIteration 47, loss = 0.29185374\nIteration 48, loss = 0.29099784\nIteration 49, loss = 0.29019197\nIteration 50, loss = 0.28816973\nIteration 51, loss = 0.28774714\nIteration 52, loss = 0.28718549\nIteration 53, loss = 0.28644641\nIteration 54, loss = 0.28586481\nIteration 55, loss = 0.28522510\nIteration 56, loss = 0.28413696\nIteration 57, loss = 0.28365541\nIteration 58, loss = 0.28284517\nIteration 59, loss = 0.28208071\nIteration 60, loss = 0.28160802\nIteration 61, loss = 0.28025956\nIteration 62, loss = 0.27969869\nIteration 63, loss = 0.27890923\nIteration 64, loss = 0.27820373\nIteration 65, loss = 0.27757847\nIteration 66, loss = 0.27666905\nIteration 67, loss = 0.27605003\nIteration 68, loss = 0.27592124\nIteration 69, loss = 0.27530699\nIteration 70, loss = 0.27467549\nIteration 71, loss = 0.27460598\nIteration 72, loss = 0.27344260\nIteration 73, loss = 0.27189424\nIteration 74, loss = 0.27257002\nIteration 75, loss = 0.27175526\nIteration 76, loss = 0.27112490\nIteration 77, loss = 0.27022018\nIteration 78, loss = 0.26953846\nIteration 79, loss = 0.26868016\nIteration 80, loss = 0.26833952\nIteration 81, loss = 0.26758670\nIteration 82, loss = 0.26697373\nIteration 83, loss = 0.26681883\nIteration 84, loss = 0.26622212\nIteration 85, loss = 0.26524827\nIteration 86, loss = 0.26456535\nIteration 87, loss = 0.26459967\nIteration 88, loss = 0.26518536\nIteration 89, loss = 0.26347623\nIteration 90, loss = 0.26288691\nIteration 91, loss = 0.26253740\nIteration 92, loss = 0.26123853\nIteration 93, loss = 0.26112124\nIteration 94, loss = 0.26035559\nIteration 95, loss = 0.26017517\nIteration 96, loss = 0.25925873\nIteration 97, loss = 0.25893237\nIteration 98, loss = 0.25821132\nIteration 99, loss = 0.25804686\nIteration 100, loss = 0.25825451\nIteration 101, loss = 0.25660438\nIteration 102, loss = 0.25659103\nIteration 103, loss = 0.25585665\nIteration 104, loss = 0.25466696\nIteration 105, loss = 0.25443846\nIteration 106, loss = 0.25450150\nIteration 107, loss = 0.25380583\nIteration 108, loss = 0.25342354\nIteration 109, loss = 0.25284229\nIteration 110, loss = 0.25250003\nIteration 111, loss = 0.25274532\nIteration 112, loss = 0.25143442\nIteration 113, loss = 0.25091517\nIteration 114, loss = 0.25055422\nIteration 115, loss = 0.25014321\nIteration 116, loss = 0.25034183\nIteration 117, loss = 0.24949560\nIteration 118, loss = 0.24868302\nIteration 119, loss = 0.24845003\nIteration 120, loss = 0.24864754\nIteration 121, loss = 0.24780689\nIteration 122, loss = 0.24694196\nIteration 123, loss = 0.24736357\nIteration 124, loss = 0.24668891\nIteration 125, loss = 0.24593155\nIteration 126, loss = 0.24579634\nIteration 127, loss = 0.24579077\nIteration 128, loss = 0.24434011\nIteration 129, loss = 0.24463964\nIteration 130, loss = 0.24366399\nIteration 131, loss = 0.24347632\nIteration 132, loss = 0.24245340\nIteration 133, loss = 0.24239280\nIteration 134, loss = 0.24290684\nIteration 135, loss = 0.24212998\nIteration 136, loss = 0.24158252\nIteration 137, loss = 0.24184615\nIteration 138, loss = 0.24155032\nIteration 139, loss = 0.24090504\nIteration 140, loss = 0.24023226\nIteration 141, loss = 0.24018233\nIteration 142, loss = 0.23883762\nIteration 143, loss = 0.23893247\nIteration 144, loss = 0.23876375\nIteration 145, loss = 0.23889747\nIteration 146, loss = 0.23790855\nIteration 147, loss = 0.23714294\nIteration 148, loss = 0.23728924\nIteration 149, loss = 0.23633677\nIteration 150, loss = 0.23608648\nIteration 151, loss = 0.23699454\nIteration 152, loss = 0.23594094\nIteration 153, loss = 0.23642407\nIteration 154, loss = 0.23514370\nIteration 155, loss = 0.23532839\nIteration 156, loss = 0.23445735\nIteration 157, loss = 0.23469929\nIteration 158, loss = 0.23373691\nIteration 159, loss = 0.23463843\nIteration 160, loss = 0.23349007\nIteration 161, loss = 0.23345260\nIteration 162, loss = 0.23292863\nIteration 163, loss = 0.23258631\nIteration 164, loss = 0.23264904\nIteration 165, loss = 0.23194932\nIteration 166, loss = 0.23139204\nIteration 167, loss = 0.23054693\nIteration 168, loss = 0.23018738\nIteration 169, loss = 0.23027995\nIteration 170, loss = 0.22990999\nIteration 171, loss = 0.23005563\nIteration 172, loss = 0.22890125\nIteration 173, loss = 0.22968863\nIteration 174, loss = 0.22925260\nIteration 175, loss = 0.22884697\nIteration 176, loss = 0.22868798\nIteration 177, loss = 0.22814736\nIteration 178, loss = 0.22835781\nIteration 179, loss = 0.22792285\nIteration 180, loss = 0.22717032\nIteration 181, loss = 0.22745012\nIteration 182, loss = 0.22721414\nIteration 183, loss = 0.22629400\nIteration 184, loss = 0.22714660\nIteration 185, loss = 0.22591631\nIteration 186, loss = 0.22619526\nIteration 187, loss = 0.22626858\nIteration 188, loss = 0.22517280\nIteration 189, loss = 0.22536424\nIteration 190, loss = 0.22437876\nIteration 191, loss = 0.22452863\nIteration 192, loss = 0.22421004\nIteration 193, loss = 0.22466701\nIteration 194, loss = 0.22301555\nIteration 195, loss = 0.22387523\nIteration 196, loss = 0.22285882\nIteration 197, loss = 0.22424758\nIteration 198, loss = 0.22268729\nIteration 199, loss = 0.22240959\nIteration 200, loss = 0.22189106\nIteration 201, loss = 0.22177625\nIteration 202, loss = 0.22203874\nIteration 203, loss = 0.22169974\nIteration 204, loss = 0.22220362\nIteration 205, loss = 0.22116231\nIteration 206, loss = 0.22075810\nIteration 207, loss = 0.22186090\nIteration 208, loss = 0.22058138\nIteration 209, loss = 0.21999793\nIteration 210, loss = 0.21977182\nIteration 211, loss = 0.22056685\nIteration 212, loss = 0.21992259\nIteration 213, loss = 0.21918791\nIteration 214, loss = 0.21905185\nIteration 215, loss = 0.21861862\nIteration 216, loss = 0.21908083\nIteration 217, loss = 0.21797755\nIteration 218, loss = 0.21854073\nIteration 219, loss = 0.21728883\nIteration 220, loss = 0.21746599\nIteration 221, loss = 0.21714969\nIteration 222, loss = 0.21690758\nIteration 223, loss = 0.21831460\nIteration 224, loss = 0.21599505\nIteration 225, loss = 0.21686682\nIteration 226, loss = 0.21677357\nIteration 227, loss = 0.21597093\nIteration 228, loss = 0.21564493\nIteration 229, loss = 0.21608251\nIteration 230, loss = 0.21529227\nIteration 231, loss = 0.21593026\nIteration 232, loss = 0.21571717\nIteration 233, loss = 0.21408269\nIteration 234, loss = 0.21497299\nIteration 235, loss = 0.21516452\nIteration 236, loss = 0.21424873\nIteration 237, loss = 0.21494201\nIteration 238, loss = 0.21411274\nIteration 239, loss = 0.21370654\nIteration 240, loss = 0.21434746\nIteration 241, loss = 0.21368996\nIteration 242, loss = 0.21357161\nIteration 243, loss = 0.21359243\nIteration 244, loss = 0.21373281\nIteration 245, loss = 0.21292641\nIteration 246, loss = 0.21175521\nIteration 247, loss = 0.21334547\nIteration 248, loss = 0.21181741\nIteration 249, loss = 0.21170956\nIteration 250, loss = 0.21227557\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36688632\nIteration 2, loss = 0.34978033\nIteration 3, loss = 0.34442681\nIteration 4, loss = 0.34203271\nIteration 5, loss = 0.33913238\nIteration 6, loss = 0.33692772\nIteration 7, loss = 0.33649123\nIteration 8, loss = 0.33424808\nIteration 9, loss = 0.33320500\nIteration 10, loss = 0.33167163\nIteration 11, loss = 0.32965765\nIteration 12, loss = 0.32872666\nIteration 13, loss = 0.32784750\nIteration 14, loss = 0.32671595\nIteration 15, loss = 0.32533603\nIteration 16, loss = 0.32392198\nIteration 17, loss = 0.32301387\nIteration 18, loss = 0.32149079\nIteration 19, loss = 0.32069672\nIteration 20, loss = 0.31960291\nIteration 21, loss = 0.31830469\nIteration 22, loss = 0.31652354\nIteration 23, loss = 0.31572939\nIteration 24, loss = 0.31483634\nIteration 25, loss = 0.31373995\nIteration 26, loss = 0.31217077\nIteration 27, loss = 0.31114063\nIteration 28, loss = 0.30998457\nIteration 29, loss = 0.30919013\nIteration 30, loss = 0.30823361\nIteration 31, loss = 0.30734375\nIteration 32, loss = 0.30690904\nIteration 33, loss = 0.30441165\nIteration 34, loss = 0.30432386\nIteration 35, loss = 0.30323332\nIteration 36, loss = 0.30183107\nIteration 37, loss = 0.30079282\nIteration 38, loss = 0.30048816\nIteration 39, loss = 0.29836641\nIteration 40, loss = 0.29818044\nIteration 41, loss = 0.29711111\nIteration 42, loss = 0.29632430\nIteration 43, loss = 0.29551402\nIteration 44, loss = 0.29446730\nIteration 45, loss = 0.29363962\nIteration 46, loss = 0.29237188\nIteration 47, loss = 0.29191238\nIteration 48, loss = 0.29055395\nIteration 49, loss = 0.29100700\nIteration 50, loss = 0.28997093\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36596828\nIteration 2, loss = 0.34837534\nIteration 3, loss = 0.34398762\nIteration 4, loss = 0.34129519\nIteration 5, loss = 0.33811610\nIteration 6, loss = 0.33720914\nIteration 7, loss = 0.33587957\nIteration 8, loss = 0.33357952\nIteration 9, loss = 0.33249609\nIteration 10, loss = 0.33081324\nIteration 11, loss = 0.32949410\nIteration 12, loss = 0.32770335\nIteration 13, loss = 0.32657925\nIteration 14, loss = 0.32545123\nIteration 15, loss = 0.32439497\nIteration 16, loss = 0.32290289\nIteration 17, loss = 0.32154005\nIteration 18, loss = 0.32015797\nIteration 19, loss = 0.31914362\nIteration 20, loss = 0.31804143\nIteration 21, loss = 0.31704705\nIteration 22, loss = 0.31559397\nIteration 23, loss = 0.31473710\nIteration 24, loss = 0.31325599\nIteration 25, loss = 0.31305335\nIteration 26, loss = 0.31089414\nIteration 27, loss = 0.30985279\nIteration 28, loss = 0.30877792\nIteration 29, loss = 0.30819812\nIteration 30, loss = 0.30732541\nIteration 31, loss = 0.30620934\nIteration 32, loss = 0.30557544\nIteration 33, loss = 0.30405013\nIteration 34, loss = 0.30356597\nIteration 35, loss = 0.30215641\nIteration 36, loss = 0.30133834\nIteration 37, loss = 0.30076077\nIteration 38, loss = 0.29955586\nIteration 39, loss = 0.29876888\nIteration 40, loss = 0.29812954\nIteration 41, loss = 0.29719673\nIteration 42, loss = 0.29658812\nIteration 43, loss = 0.29643791\nIteration 44, loss = 0.29411335\nIteration 45, loss = 0.29358261\nIteration 46, loss = 0.29296145\nIteration 47, loss = 0.29201625\nIteration 48, loss = 0.29167141\nIteration 49, loss = 0.29134111\nIteration 50, loss = 0.28931794\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36465758\nIteration 2, loss = 0.35001161\nIteration 3, loss = 0.34577600\nIteration 4, loss = 0.34304341\nIteration 5, loss = 0.34070852\nIteration 6, loss = 0.33879355\nIteration 7, loss = 0.33729457\nIteration 8, loss = 0.33530940\nIteration 9, loss = 0.33440786\nIteration 10, loss = 0.33296334\nIteration 11, loss = 0.33131759\nIteration 12, loss = 0.33012876\nIteration 13, loss = 0.32965727\nIteration 14, loss = 0.32853493\nIteration 15, loss = 0.32683006\nIteration 16, loss = 0.32576004\nIteration 17, loss = 0.32414478\nIteration 18, loss = 0.32272243\nIteration 19, loss = 0.32212487\nIteration 20, loss = 0.32068486\nIteration 21, loss = 0.32012097\nIteration 22, loss = 0.31835205\nIteration 23, loss = 0.31760236\nIteration 24, loss = 0.31612582\nIteration 25, loss = 0.31563406\nIteration 26, loss = 0.31411719\nIteration 27, loss = 0.31378954\nIteration 28, loss = 0.31298966\nIteration 29, loss = 0.31140654\nIteration 30, loss = 0.31059403\nIteration 31, loss = 0.30886122\nIteration 32, loss = 0.30828369\nIteration 33, loss = 0.30700760\nIteration 34, loss = 0.30618131\nIteration 35, loss = 0.30538997\nIteration 36, loss = 0.30453331\nIteration 37, loss = 0.30314055\nIteration 38, loss = 0.30276783\nIteration 39, loss = 0.30216953\nIteration 40, loss = 0.30074451\nIteration 41, loss = 0.29948893\nIteration 42, loss = 0.29915275\nIteration 43, loss = 0.29776342\nIteration 44, loss = 0.29710457\nIteration 45, loss = 0.29591561\nIteration 46, loss = 0.29570953\nIteration 47, loss = 0.29456503\nIteration 48, loss = 0.29391116\nIteration 49, loss = 0.29364510\nIteration 50, loss = 0.29172645\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36418116\nIteration 2, loss = 0.34914591\nIteration 3, loss = 0.34540406\nIteration 4, loss = 0.34197974\nIteration 5, loss = 0.33999232\nIteration 6, loss = 0.33856386\nIteration 7, loss = 0.33688128\nIteration 8, loss = 0.33427005\nIteration 9, loss = 0.33347065\nIteration 10, loss = 0.33136716\nIteration 11, loss = 0.33009106\nIteration 12, loss = 0.32817070\nIteration 13, loss = 0.32803056\nIteration 14, loss = 0.32634743\nIteration 15, loss = 0.32515196\nIteration 16, loss = 0.32400137\nIteration 17, loss = 0.32218132\nIteration 18, loss = 0.32081014\nIteration 19, loss = 0.31985063\nIteration 20, loss = 0.31884097\nIteration 21, loss = 0.31774903\nIteration 22, loss = 0.31628858\nIteration 23, loss = 0.31574350\nIteration 24, loss = 0.31393902\nIteration 25, loss = 0.31302614\nIteration 26, loss = 0.31157993\nIteration 27, loss = 0.31099031\nIteration 28, loss = 0.30907676\nIteration 29, loss = 0.30869532\nIteration 30, loss = 0.30723029\nIteration 31, loss = 0.30631336\nIteration 32, loss = 0.30473411\nIteration 33, loss = 0.30350790\nIteration 34, loss = 0.30275885\nIteration 35, loss = 0.30113220\nIteration 36, loss = 0.30018207\nIteration 37, loss = 0.30006209\nIteration 38, loss = 0.29898279\nIteration 39, loss = 0.29759163\nIteration 40, loss = 0.29702837\nIteration 41, loss = 0.29631740\nIteration 42, loss = 0.29571810\nIteration 43, loss = 0.29461796\nIteration 44, loss = 0.29399768\nIteration 45, loss = 0.29284577\nIteration 46, loss = 0.29111573\nIteration 47, loss = 0.29123116\nIteration 48, loss = 0.29049079\nIteration 49, loss = 0.29016879\nIteration 50, loss = 0.28886620\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36509124\nIteration 2, loss = 0.34976749\nIteration 3, loss = 0.34473182\nIteration 4, loss = 0.34206389\nIteration 5, loss = 0.33966240\nIteration 6, loss = 0.33751806\nIteration 7, loss = 0.33635638\nIteration 8, loss = 0.33382480\nIteration 9, loss = 0.33272774\nIteration 10, loss = 0.33131795\nIteration 11, loss = 0.32989841\nIteration 12, loss = 0.32774563\nIteration 13, loss = 0.32763560\nIteration 14, loss = 0.32605637\nIteration 15, loss = 0.32535629\nIteration 16, loss = 0.32424807\nIteration 17, loss = 0.32248717\nIteration 18, loss = 0.32146701\nIteration 19, loss = 0.32025986\nIteration 20, loss = 0.31928849\nIteration 21, loss = 0.31829118\nIteration 22, loss = 0.31702221\nIteration 23, loss = 0.31546654\nIteration 24, loss = 0.31436615\nIteration 25, loss = 0.31365774\nIteration 26, loss = 0.31215193\nIteration 27, loss = 0.31124041\nIteration 28, loss = 0.31009260\nIteration 29, loss = 0.30922900\nIteration 30, loss = 0.30800925\nIteration 31, loss = 0.30696504\nIteration 32, loss = 0.30547750\nIteration 33, loss = 0.30512818\nIteration 34, loss = 0.30393312\nIteration 35, loss = 0.30362536\nIteration 36, loss = 0.30163994\nIteration 37, loss = 0.30122068\nIteration 38, loss = 0.30021588\nIteration 39, loss = 0.29979583\nIteration 40, loss = 0.29937899\nIteration 41, loss = 0.29791519\nIteration 42, loss = 0.29703346\nIteration 43, loss = 0.29614934\nIteration 44, loss = 0.29429068\nIteration 45, loss = 0.29470351\nIteration 46, loss = 0.29359517\nIteration 47, loss = 0.29272140\nIteration 48, loss = 0.29152138\nIteration 49, loss = 0.29217937\nIteration 50, loss = 0.29048608\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36792457\nIteration 2, loss = 0.34997095\nIteration 3, loss = 0.34450913\nIteration 4, loss = 0.34228988\nIteration 5, loss = 0.34031654\nIteration 6, loss = 0.33776617\nIteration 7, loss = 0.33622506\nIteration 8, loss = 0.33486506\nIteration 9, loss = 0.33409538\nIteration 10, loss = 0.33277767\nIteration 11, loss = 0.33124848\nIteration 12, loss = 0.33034991\nIteration 13, loss = 0.32917219\nIteration 14, loss = 0.32812671\nIteration 15, loss = 0.32615971\nIteration 16, loss = 0.32561000\nIteration 17, loss = 0.32467359\nIteration 18, loss = 0.32398617\nIteration 19, loss = 0.32197021\nIteration 20, loss = 0.32186373\nIteration 21, loss = 0.32062587\nIteration 22, loss = 0.31992367\nIteration 23, loss = 0.31810817\nIteration 24, loss = 0.31686620\nIteration 25, loss = 0.31665253\nIteration 26, loss = 0.31554848\nIteration 27, loss = 0.31468598\nIteration 28, loss = 0.31346458\nIteration 29, loss = 0.31245675\nIteration 30, loss = 0.31228790\nIteration 31, loss = 0.31157420\nIteration 32, loss = 0.31035515\nIteration 33, loss = 0.30925843\nIteration 34, loss = 0.30790388\nIteration 35, loss = 0.30794155\nIteration 36, loss = 0.30657294\nIteration 37, loss = 0.30628844\nIteration 38, loss = 0.30521783\nIteration 39, loss = 0.30402997\nIteration 40, loss = 0.30455515\nIteration 41, loss = 0.30297873\nIteration 42, loss = 0.30159590\nIteration 43, loss = 0.30109891\nIteration 44, loss = 0.30076993\nIteration 45, loss = 0.29924254\nIteration 46, loss = 0.29910477\nIteration 47, loss = 0.29882239\nIteration 48, loss = 0.29795326\nIteration 49, loss = 0.29637738\nIteration 50, loss = 0.29607028\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36652607\nIteration 2, loss = 0.34846452\nIteration 3, loss = 0.34301459\nIteration 4, loss = 0.34141863\nIteration 5, loss = 0.33848373\nIteration 6, loss = 0.33654452\nIteration 7, loss = 0.33500121\nIteration 8, loss = 0.33381002\nIteration 9, loss = 0.33255814\nIteration 10, loss = 0.33085697\nIteration 11, loss = 0.33041153\nIteration 12, loss = 0.32844301\nIteration 13, loss = 0.32744497\nIteration 14, loss = 0.32685315\nIteration 15, loss = 0.32511721\nIteration 16, loss = 0.32412856\nIteration 17, loss = 0.32262046\nIteration 18, loss = 0.32131868\nIteration 19, loss = 0.32019509\nIteration 20, loss = 0.31978542\nIteration 21, loss = 0.31830652\nIteration 22, loss = 0.31683574\nIteration 23, loss = 0.31610263\nIteration 24, loss = 0.31512112\nIteration 25, loss = 0.31381429\nIteration 26, loss = 0.31241817\nIteration 27, loss = 0.31212603\nIteration 28, loss = 0.31092322\nIteration 29, loss = 0.30980028\nIteration 30, loss = 0.30920687\nIteration 31, loss = 0.30727031\nIteration 32, loss = 0.30697806\nIteration 33, loss = 0.30611585\nIteration 34, loss = 0.30482508\nIteration 35, loss = 0.30472040\nIteration 36, loss = 0.30296565\nIteration 37, loss = 0.30209241\nIteration 38, loss = 0.30134063\nIteration 39, loss = 0.30008443\nIteration 40, loss = 0.30008799\nIteration 41, loss = 0.29907246\nIteration 42, loss = 0.29721525\nIteration 43, loss = 0.29705310\nIteration 44, loss = 0.29566583\nIteration 45, loss = 0.29571740\nIteration 46, loss = 0.29477592\nIteration 47, loss = 0.29376797\nIteration 48, loss = 0.29249656\nIteration 49, loss = 0.29176840\nIteration 50, loss = 0.29203395\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36742943\nIteration 2, loss = 0.34972584\nIteration 3, loss = 0.34510091\nIteration 4, loss = 0.34291212\nIteration 5, loss = 0.34084110\nIteration 6, loss = 0.33878754\nIteration 7, loss = 0.33708381\nIteration 8, loss = 0.33523569\nIteration 9, loss = 0.33471932\nIteration 10, loss = 0.33254337\nIteration 11, loss = 0.33183199\nIteration 12, loss = 0.32953586\nIteration 13, loss = 0.32886849\nIteration 14, loss = 0.32835142\nIteration 15, loss = 0.32673057\nIteration 16, loss = 0.32546580\nIteration 17, loss = 0.32515101\nIteration 18, loss = 0.32339892\nIteration 19, loss = 0.32226303\nIteration 20, loss = 0.32085751\nIteration 21, loss = 0.31996526\nIteration 22, loss = 0.31936571\nIteration 23, loss = 0.31828906\nIteration 24, loss = 0.31682669\nIteration 25, loss = 0.31561003\nIteration 26, loss = 0.31439780\nIteration 27, loss = 0.31314864\nIteration 28, loss = 0.31286668\nIteration 29, loss = 0.31199359\nIteration 30, loss = 0.31024879\nIteration 31, loss = 0.30954349\nIteration 32, loss = 0.30871318\nIteration 33, loss = 0.30824048\nIteration 34, loss = 0.30653087\nIteration 35, loss = 0.30599642\nIteration 36, loss = 0.30482697\nIteration 37, loss = 0.30455273\nIteration 38, loss = 0.30302911\nIteration 39, loss = 0.30270739\nIteration 40, loss = 0.30206564\nIteration 41, loss = 0.30116393\nIteration 42, loss = 0.30069174\nIteration 43, loss = 0.29857896\nIteration 44, loss = 0.29771731\nIteration 45, loss = 0.29758419\nIteration 46, loss = 0.29691168\nIteration 47, loss = 0.29671724\nIteration 48, loss = 0.29545057\nIteration 49, loss = 0.29406024\nIteration 50, loss = 0.29376932\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36635142\nIteration 2, loss = 0.34926659\nIteration 3, loss = 0.34467480\nIteration 4, loss = 0.34227953\nIteration 5, loss = 0.34023021\nIteration 6, loss = 0.33832813\nIteration 7, loss = 0.33574304\nIteration 8, loss = 0.33490617\nIteration 9, loss = 0.33373457\nIteration 10, loss = 0.33180359\nIteration 11, loss = 0.33075755\nIteration 12, loss = 0.32927590\nIteration 13, loss = 0.32832315\nIteration 14, loss = 0.32717471\nIteration 15, loss = 0.32619154\nIteration 16, loss = 0.32507650\nIteration 17, loss = 0.32377058\nIteration 18, loss = 0.32267874\nIteration 19, loss = 0.32135983\nIteration 20, loss = 0.32047898\nIteration 21, loss = 0.31983346\nIteration 22, loss = 0.31808453\nIteration 23, loss = 0.31716723\nIteration 24, loss = 0.31569839\nIteration 25, loss = 0.31505130\nIteration 26, loss = 0.31388639\nIteration 27, loss = 0.31273147\nIteration 28, loss = 0.31243337\nIteration 29, loss = 0.31130364\nIteration 30, loss = 0.30977219\nIteration 31, loss = 0.30931029\nIteration 32, loss = 0.30873190\nIteration 33, loss = 0.30767040\nIteration 34, loss = 0.30631147\nIteration 35, loss = 0.30575462\nIteration 36, loss = 0.30493605\nIteration 37, loss = 0.30470924\nIteration 38, loss = 0.30311661\nIteration 39, loss = 0.30265767\nIteration 40, loss = 0.30153603\nIteration 41, loss = 0.30089878\nIteration 42, loss = 0.29954503\nIteration 43, loss = 0.29875962\nIteration 44, loss = 0.29826034\nIteration 45, loss = 0.29800988\nIteration 46, loss = 0.29686489\nIteration 47, loss = 0.29579158\nIteration 48, loss = 0.29504536\nIteration 49, loss = 0.29532746\nIteration 50, loss = 0.29356184\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36753868\nIteration 2, loss = 0.34863116\nIteration 3, loss = 0.34405475\nIteration 4, loss = 0.34299264\nIteration 5, loss = 0.34042236\nIteration 6, loss = 0.33802674\nIteration 7, loss = 0.33615348\nIteration 8, loss = 0.33496146\nIteration 9, loss = 0.33290869\nIteration 10, loss = 0.33154746\nIteration 11, loss = 0.33078283\nIteration 12, loss = 0.32942776\nIteration 13, loss = 0.32797897\nIteration 14, loss = 0.32780386\nIteration 15, loss = 0.32617732\nIteration 16, loss = 0.32498233\nIteration 17, loss = 0.32389662\nIteration 18, loss = 0.32251902\nIteration 19, loss = 0.32156760\nIteration 20, loss = 0.32063556\nIteration 21, loss = 0.31889258\nIteration 22, loss = 0.31784667\nIteration 23, loss = 0.31716127\nIteration 24, loss = 0.31585101\nIteration 25, loss = 0.31476589\nIteration 26, loss = 0.31388418\nIteration 27, loss = 0.31238520\nIteration 28, loss = 0.31166582\nIteration 29, loss = 0.31115316\nIteration 30, loss = 0.30994110\nIteration 31, loss = 0.30934221\nIteration 32, loss = 0.30743607\nIteration 33, loss = 0.30710638\nIteration 34, loss = 0.30567035\nIteration 35, loss = 0.30438733\nIteration 36, loss = 0.30485196\nIteration 37, loss = 0.30440546\nIteration 38, loss = 0.30304447\nIteration 39, loss = 0.30181880\nIteration 40, loss = 0.30027395\nIteration 41, loss = 0.30017396\nIteration 42, loss = 0.29950302\nIteration 43, loss = 0.29791160\nIteration 44, loss = 0.29782673\nIteration 45, loss = 0.29662058\nIteration 46, loss = 0.29652422\nIteration 47, loss = 0.29584443\nIteration 48, loss = 0.29371004\nIteration 49, loss = 0.29428216\nIteration 50, loss = 0.29306977\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36688632\nIteration 2, loss = 0.34978033\nIteration 3, loss = 0.34442681\nIteration 4, loss = 0.34203271\nIteration 5, loss = 0.33913238\nIteration 6, loss = 0.33692772\nIteration 7, loss = 0.33649123\nIteration 8, loss = 0.33424808\nIteration 9, loss = 0.33320500\nIteration 10, loss = 0.33167163\nIteration 11, loss = 0.32965765\nIteration 12, loss = 0.32872666\nIteration 13, loss = 0.32784750\nIteration 14, loss = 0.32671595\nIteration 15, loss = 0.32533603\nIteration 16, loss = 0.32392198\nIteration 17, loss = 0.32301387\nIteration 18, loss = 0.32149079\nIteration 19, loss = 0.32069672\nIteration 20, loss = 0.31960291\nIteration 21, loss = 0.31830469\nIteration 22, loss = 0.31652354\nIteration 23, loss = 0.31572939\nIteration 24, loss = 0.31483634\nIteration 25, loss = 0.31373995\nIteration 26, loss = 0.31217077\nIteration 27, loss = 0.31114063\nIteration 28, loss = 0.30998457\nIteration 29, loss = 0.30919013\nIteration 30, loss = 0.30823361\nIteration 31, loss = 0.30734375\nIteration 32, loss = 0.30690904\nIteration 33, loss = 0.30441165\nIteration 34, loss = 0.30432386\nIteration 35, loss = 0.30323332\nIteration 36, loss = 0.30183107\nIteration 37, loss = 0.30079282\nIteration 38, loss = 0.30048816\nIteration 39, loss = 0.29836641\nIteration 40, loss = 0.29818044\nIteration 41, loss = 0.29711111\nIteration 42, loss = 0.29632430\nIteration 43, loss = 0.29551402\nIteration 44, loss = 0.29446730\nIteration 45, loss = 0.29363962\nIteration 46, loss = 0.29237188\nIteration 47, loss = 0.29191238\nIteration 48, loss = 0.29055395\nIteration 49, loss = 0.29100700\nIteration 50, loss = 0.28997093\nIteration 51, loss = 0.28864444\nIteration 52, loss = 0.28883193\nIteration 53, loss = 0.28720971\nIteration 54, loss = 0.28628241\nIteration 55, loss = 0.28620153\nIteration 56, loss = 0.28572115\nIteration 57, loss = 0.28392084\nIteration 58, loss = 0.28387789\nIteration 59, loss = 0.28380744\nIteration 60, loss = 0.28307339\nIteration 61, loss = 0.28220021\nIteration 62, loss = 0.28118169\nIteration 63, loss = 0.28117702\nIteration 64, loss = 0.28008919\nIteration 65, loss = 0.28021431\nIteration 66, loss = 0.27839046\nIteration 67, loss = 0.27855597\nIteration 68, loss = 0.27805860\nIteration 69, loss = 0.27789611\nIteration 70, loss = 0.27646052\nIteration 71, loss = 0.27605981\nIteration 72, loss = 0.27585609\nIteration 73, loss = 0.27519135\nIteration 74, loss = 0.27417267\nIteration 75, loss = 0.27387602\nIteration 76, loss = 0.27420019\nIteration 77, loss = 0.27333149\nIteration 78, loss = 0.27304176\nIteration 79, loss = 0.27166162\nIteration 80, loss = 0.27196935\nIteration 81, loss = 0.27062381\nIteration 82, loss = 0.27087406\nIteration 83, loss = 0.27127081\nIteration 84, loss = 0.27008470\nIteration 85, loss = 0.26985388\nIteration 86, loss = 0.26912700\nIteration 87, loss = 0.26890812\nIteration 88, loss = 0.26900288\nIteration 89, loss = 0.26812493\nIteration 90, loss = 0.26770368\nIteration 91, loss = 0.26751496\nIteration 92, loss = 0.26699895\nIteration 93, loss = 0.26553971\nIteration 94, loss = 0.26692680\nIteration 95, loss = 0.26614112\nIteration 96, loss = 0.26518841\nIteration 97, loss = 0.26472929\nIteration 98, loss = 0.26546286\nIteration 99, loss = 0.26421891\nIteration 100, loss = 0.26360763\nIteration 101, loss = 0.26424153\nIteration 102, loss = 0.26312393\nIteration 103, loss = 0.26259004\nIteration 104, loss = 0.26314251\nIteration 105, loss = 0.26404457\nIteration 106, loss = 0.26181637\nIteration 107, loss = 0.26141160\nIteration 108, loss = 0.26226091\nIteration 109, loss = 0.26150228\nIteration 110, loss = 0.26029945\nIteration 111, loss = 0.26060916\nIteration 112, loss = 0.26056543\nIteration 113, loss = 0.26102069\nIteration 114, loss = 0.25931605\nIteration 115, loss = 0.25834603\nIteration 116, loss = 0.26041012\nIteration 117, loss = 0.25737418\nIteration 118, loss = 0.25768621\nIteration 119, loss = 0.25907279\nIteration 120, loss = 0.25668279\nIteration 121, loss = 0.25791973\nIteration 122, loss = 0.25757166\nIteration 123, loss = 0.25903270\nIteration 124, loss = 0.25646658\nIteration 125, loss = 0.25827746\nIteration 126, loss = 0.25659467\nIteration 127, loss = 0.25523965\nIteration 128, loss = 0.25473988\nIteration 129, loss = 0.25639970\nIteration 130, loss = 0.25469255\nIteration 131, loss = 0.25422961\nIteration 132, loss = 0.25464258\nIteration 133, loss = 0.25403593\nIteration 134, loss = 0.25351031\nIteration 135, loss = 0.25397816\nIteration 136, loss = 0.25385368\nIteration 137, loss = 0.25273971\nIteration 138, loss = 0.25264884\nIteration 139, loss = 0.25339497\nIteration 140, loss = 0.25461009\nIteration 141, loss = 0.25272818\nIteration 142, loss = 0.25284644\nIteration 143, loss = 0.25169254\nIteration 144, loss = 0.25289735\nIteration 145, loss = 0.25117658\nIteration 146, loss = 0.25203472\nIteration 147, loss = 0.25111198\nIteration 148, loss = 0.25227851\nIteration 149, loss = 0.24979925\nIteration 150, loss = 0.25012092\nIteration 151, loss = 0.24967101\nIteration 152, loss = 0.25124030\nIteration 153, loss = 0.25021879\nIteration 154, loss = 0.25071699\nIteration 155, loss = 0.25026738\nIteration 156, loss = 0.24838538\nIteration 157, loss = 0.24974304\nIteration 158, loss = 0.24824752\nIteration 159, loss = 0.24932621\nIteration 160, loss = 0.24965110\nIteration 161, loss = 0.24922582\nIteration 162, loss = 0.24932510\nIteration 163, loss = 0.24800102\nIteration 164, loss = 0.24763303\nIteration 165, loss = 0.24689722\nIteration 166, loss = 0.24713971\nIteration 167, loss = 0.24681247\nIteration 168, loss = 0.24844001\nIteration 169, loss = 0.24553177\nIteration 170, loss = 0.24647360\nIteration 171, loss = 0.24716405\nIteration 172, loss = 0.24511573\nIteration 173, loss = 0.24584962\nIteration 174, loss = 0.24699875\nIteration 175, loss = 0.24484054\nIteration 176, loss = 0.24734045\nIteration 177, loss = 0.24624160\nIteration 178, loss = 0.24447680\nIteration 179, loss = 0.24491586\nIteration 180, loss = 0.24572707\nIteration 181, loss = 0.24644048\nIteration 182, loss = 0.24588313\nIteration 183, loss = 0.24504814\nIteration 184, loss = 0.24349316\nIteration 185, loss = 0.24403250\nIteration 186, loss = 0.24353996\nIteration 187, loss = 0.24335818\nIteration 188, loss = 0.24334480\nIteration 189, loss = 0.24350133\nIteration 190, loss = 0.24300810\nIteration 191, loss = 0.24258399\nIteration 192, loss = 0.24480141\nIteration 193, loss = 0.24217818\nIteration 194, loss = 0.24263125\nIteration 195, loss = 0.24351724\nIteration 196, loss = 0.24302852\nIteration 197, loss = 0.24459214\nIteration 198, loss = 0.24274873\nIteration 199, loss = 0.24256905\nIteration 200, loss = 0.24081854\nIteration 201, loss = 0.24134526\nIteration 202, loss = 0.24144286\nIteration 203, loss = 0.24180247\nIteration 204, loss = 0.23980558\nIteration 205, loss = 0.24139435\nIteration 206, loss = 0.24190048\nIteration 207, loss = 0.24099797\nIteration 208, loss = 0.23949389\nIteration 209, loss = 0.24126178\nIteration 210, loss = 0.23998907\nIteration 211, loss = 0.23905336\nIteration 212, loss = 0.24340349\nIteration 213, loss = 0.23902806\nIteration 214, loss = 0.23693546\nIteration 215, loss = 0.24021820\nIteration 216, loss = 0.24127037\nIteration 217, loss = 0.23909090\nIteration 218, loss = 0.24014503\nIteration 219, loss = 0.23832297\nIteration 220, loss = 0.23862032\nIteration 221, loss = 0.23725378\nIteration 222, loss = 0.24112690\nIteration 223, loss = 0.23850148\nIteration 224, loss = 0.23708426\nIteration 225, loss = 0.23800451\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\nIteration 1, loss = 0.36596828\nIteration 2, loss = 0.34837534\nIteration 3, loss = 0.34398762\nIteration 4, loss = 0.34129519\nIteration 5, loss = 0.33811610\nIteration 6, loss = 0.33720914\nIteration 7, loss = 0.33587957\nIteration 8, loss = 0.33357952\nIteration 9, loss = 0.33249609\nIteration 10, loss = 0.33081324\nIteration 11, loss = 0.32949410\nIteration 12, loss = 0.32770335\nIteration 13, loss = 0.32657925\nIteration 14, loss = 0.32545123\nIteration 15, loss = 0.32439497\nIteration 16, loss = 0.32290289\nIteration 17, loss = 0.32154005\nIteration 18, loss = 0.32015797\nIteration 19, loss = 0.31914362\nIteration 20, loss = 0.31804143\nIteration 21, loss = 0.31704705\nIteration 22, loss = 0.31559397\nIteration 23, loss = 0.31473710\nIteration 24, loss = 0.31325599\nIteration 25, loss = 0.31305335\nIteration 26, loss = 0.31089414\nIteration 27, loss = 0.30985279\nIteration 28, loss = 0.30877792\nIteration 29, loss = 0.30819812\nIteration 30, loss = 0.30732541\nIteration 31, loss = 0.30620934\nIteration 32, loss = 0.30557544\nIteration 33, loss = 0.30405013\nIteration 34, loss = 0.30356597\nIteration 35, loss = 0.30215641\nIteration 36, loss = 0.30133834\nIteration 37, loss = 0.30076077\nIteration 38, loss = 0.29955586\nIteration 39, loss = 0.29876888\nIteration 40, loss = 0.29812954\nIteration 41, loss = 0.29719673\nIteration 42, loss = 0.29658812\nIteration 43, loss = 0.29643791\nIteration 44, loss = 0.29411335\nIteration 45, loss = 0.29358261\nIteration 46, loss = 0.29296145\nIteration 47, loss = 0.29201625\nIteration 48, loss = 0.29167141\nIteration 49, loss = 0.29134111\nIteration 50, loss = 0.28931794\nIteration 51, loss = 0.29025989\nIteration 52, loss = 0.28900902\nIteration 53, loss = 0.28824330\nIteration 54, loss = 0.28795882\nIteration 55, loss = 0.28702111\nIteration 56, loss = 0.28679245\nIteration 57, loss = 0.28583513\nIteration 58, loss = 0.28478534\nIteration 59, loss = 0.28454538\nIteration 60, loss = 0.28447624\nIteration 61, loss = 0.28389603\nIteration 62, loss = 0.28193157\nIteration 63, loss = 0.28222505\nIteration 64, loss = 0.28213447\nIteration 65, loss = 0.28081216\nIteration 66, loss = 0.28044198\nIteration 67, loss = 0.28028198\nIteration 68, loss = 0.27913261\nIteration 69, loss = 0.27955138\nIteration 70, loss = 0.27886414\nIteration 71, loss = 0.27784876\nIteration 72, loss = 0.27787175\nIteration 73, loss = 0.27660272\nIteration 74, loss = 0.27644117\nIteration 75, loss = 0.27597394\nIteration 76, loss = 0.27657432\nIteration 77, loss = 0.27577669\nIteration 78, loss = 0.27420417\nIteration 79, loss = 0.27437982\nIteration 80, loss = 0.27549090\nIteration 81, loss = 0.27611511\nIteration 82, loss = 0.27213207\nIteration 83, loss = 0.27309383\nIteration 84, loss = 0.27182948\nIteration 85, loss = 0.27203976\nIteration 86, loss = 0.27192055\nIteration 87, loss = 0.27049427\nIteration 88, loss = 0.27034866\nIteration 89, loss = 0.27070855\nIteration 90, loss = 0.27170791\nIteration 91, loss = 0.27034597\nIteration 92, loss = 0.26995323\nIteration 93, loss = 0.26859001\nIteration 94, loss = 0.26968392\nIteration 95, loss = 0.26812413\nIteration 96, loss = 0.26738481\nIteration 97, loss = 0.26666828\nIteration 98, loss = 0.26750244\nIteration 99, loss = 0.26855972\nIteration 100, loss = 0.26718977\nIteration 101, loss = 0.26645917\nIteration 102, loss = 0.26720734\nIteration 103, loss = 0.26728812\nIteration 104, loss = 0.26541437\nIteration 105, loss = 0.26591452\nIteration 106, loss = 0.26569393\nIteration 107, loss = 0.26465002\nIteration 108, loss = 0.26415065\nIteration 109, loss = 0.26489131\nIteration 110, loss = 0.26493762\nIteration 111, loss = 0.26298438\nIteration 112, loss = 0.26369139\nIteration 113, loss = 0.26383540\nIteration 114, loss = 0.26295481\nIteration 115, loss = 0.26293970\nIteration 116, loss = 0.26055740\nIteration 117, loss = 0.26123635\nIteration 118, loss = 0.26022230\nIteration 119, loss = 0.26214478\nIteration 120, loss = 0.26134608\nIteration 121, loss = 0.26212216\nIteration 122, loss = 0.25992364\nIteration 123, loss = 0.26005449\nIteration 124, loss = 0.25987724\nIteration 125, loss = 0.26084726\nIteration 126, loss = 0.25856434\nIteration 127, loss = 0.25871090\nIteration 128, loss = 0.26024645\nIteration 129, loss = 0.25848820\nIteration 130, loss = 0.25739566\nIteration 131, loss = 0.25918816\nIteration 132, loss = 0.25965823\nIteration 133, loss = 0.25768367\nIteration 134, loss = 0.25796577\nIteration 135, loss = 0.25874297\nIteration 136, loss = 0.25652396\nIteration 137, loss = 0.25664354\nIteration 138, loss = 0.25571765\nIteration 139, loss = 0.25860713\nIteration 140, loss = 0.25642879\nIteration 141, loss = 0.25718037\nIteration 142, loss = 0.25425919\nIteration 143, loss = 0.25454501\nIteration 144, loss = 0.25504418\nIteration 145, loss = 0.25486135\nIteration 146, loss = 0.25412527\nIteration 147, loss = 0.25483909\nIteration 148, loss = 0.25457127\nIteration 149, loss = 0.25375244\nIteration 150, loss = 0.25465993\nIteration 151, loss = 0.25402526\nIteration 152, loss = 0.25421767\nIteration 153, loss = 0.25383631\nIteration 154, loss = 0.25356253\nIteration 155, loss = 0.25154762\nIteration 156, loss = 0.25320983\nIteration 157, loss = 0.25429425\nIteration 158, loss = 0.25266599\nIteration 159, loss = 0.25179176\nIteration 160, loss = 0.25254511\nIteration 161, loss = 0.25159734\nIteration 162, loss = 0.25233461\nIteration 163, loss = 0.25145553\nIteration 164, loss = 0.25342333\nIteration 165, loss = 0.25193171\nIteration 166, loss = 0.25151270\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\nIteration 1, loss = 0.36465758\nIteration 2, loss = 0.35001161\nIteration 3, loss = 0.34577600\nIteration 4, loss = 0.34304341\nIteration 5, loss = 0.34070852\nIteration 6, loss = 0.33879355\nIteration 7, loss = 0.33729457\nIteration 8, loss = 0.33530940\nIteration 9, loss = 0.33440786\nIteration 10, loss = 0.33296334\nIteration 11, loss = 0.33131759\nIteration 12, loss = 0.33012876\nIteration 13, loss = 0.32965727\nIteration 14, loss = 0.32853493\nIteration 15, loss = 0.32683006\nIteration 16, loss = 0.32576004\nIteration 17, loss = 0.32414478\nIteration 18, loss = 0.32272243\nIteration 19, loss = 0.32212487\nIteration 20, loss = 0.32068486\nIteration 21, loss = 0.32012097\nIteration 22, loss = 0.31835205\nIteration 23, loss = 0.31760236\nIteration 24, loss = 0.31612582\nIteration 25, loss = 0.31563406\nIteration 26, loss = 0.31411719\nIteration 27, loss = 0.31378954\nIteration 28, loss = 0.31298966\nIteration 29, loss = 0.31140654\nIteration 30, loss = 0.31059403\nIteration 31, loss = 0.30886122\nIteration 32, loss = 0.30828369\nIteration 33, loss = 0.30700760\nIteration 34, loss = 0.30618131\nIteration 35, loss = 0.30538997\nIteration 36, loss = 0.30453331\nIteration 37, loss = 0.30314055\nIteration 38, loss = 0.30276783\nIteration 39, loss = 0.30216953\nIteration 40, loss = 0.30074451\nIteration 41, loss = 0.29948893\nIteration 42, loss = 0.29915275\nIteration 43, loss = 0.29776342\nIteration 44, loss = 0.29710457\nIteration 45, loss = 0.29591561\nIteration 46, loss = 0.29570953\nIteration 47, loss = 0.29456503\nIteration 48, loss = 0.29391116\nIteration 49, loss = 0.29364510\nIteration 50, loss = 0.29172645\nIteration 51, loss = 0.29179685\nIteration 52, loss = 0.28994746\nIteration 53, loss = 0.29003073\nIteration 54, loss = 0.28983027\nIteration 55, loss = 0.28902067\nIteration 56, loss = 0.28830580\nIteration 57, loss = 0.28781322\nIteration 58, loss = 0.28674251\nIteration 59, loss = 0.28681339\nIteration 60, loss = 0.28581954\nIteration 61, loss = 0.28507676\nIteration 62, loss = 0.28549878\nIteration 63, loss = 0.28363038\nIteration 64, loss = 0.28322164\nIteration 65, loss = 0.28298516\nIteration 66, loss = 0.28145152\nIteration 67, loss = 0.28221471\nIteration 68, loss = 0.28067900\nIteration 69, loss = 0.27988066\nIteration 70, loss = 0.27993568\nIteration 71, loss = 0.27956108\nIteration 72, loss = 0.27873667\nIteration 73, loss = 0.27887474\nIteration 74, loss = 0.27764391\nIteration 75, loss = 0.27772544\nIteration 76, loss = 0.27777923\nIteration 77, loss = 0.27643906\nIteration 78, loss = 0.27639372\nIteration 79, loss = 0.27552313\nIteration 80, loss = 0.27426514\nIteration 81, loss = 0.27472327\nIteration 82, loss = 0.27481835\nIteration 83, loss = 0.27431481\nIteration 84, loss = 0.27413088\nIteration 85, loss = 0.27247381\nIteration 86, loss = 0.27165917\nIteration 87, loss = 0.27323462\nIteration 88, loss = 0.27104647\nIteration 89, loss = 0.27074555\nIteration 90, loss = 0.27012426\nIteration 91, loss = 0.26980145\nIteration 92, loss = 0.26930593\nIteration 93, loss = 0.27012367\nIteration 94, loss = 0.26931853\nIteration 95, loss = 0.26818248\nIteration 96, loss = 0.26786157\nIteration 97, loss = 0.26777680\nIteration 98, loss = 0.26704479\nIteration 99, loss = 0.26836170\nIteration 100, loss = 0.26659221\nIteration 101, loss = 0.26667124\nIteration 102, loss = 0.26602632\nIteration 103, loss = 0.26587713\nIteration 104, loss = 0.26487310\nIteration 105, loss = 0.26504425\nIteration 106, loss = 0.26400365\nIteration 107, loss = 0.26240156\nIteration 108, loss = 0.26420169\nIteration 109, loss = 0.26310898\nIteration 110, loss = 0.26336802\nIteration 111, loss = 0.26352094\nIteration 112, loss = 0.26274145\nIteration 113, loss = 0.26288174\nIteration 114, loss = 0.26236937\nIteration 115, loss = 0.26169407\nIteration 116, loss = 0.26143279\nIteration 117, loss = 0.26033334\nIteration 118, loss = 0.26050125\nIteration 119, loss = 0.25943068\nIteration 120, loss = 0.25952628\nIteration 121, loss = 0.26023593\nIteration 122, loss = 0.26078766\nIteration 123, loss = 0.25892705\nIteration 124, loss = 0.26077480\nIteration 125, loss = 0.25777409\nIteration 126, loss = 0.25856048\nIteration 127, loss = 0.25965745\nIteration 128, loss = 0.25858527\nIteration 129, loss = 0.25810852\nIteration 130, loss = 0.25708675\nIteration 131, loss = 0.25643769\nIteration 132, loss = 0.25734618\nIteration 133, loss = 0.25791484\nIteration 134, loss = 0.25632742\nIteration 135, loss = 0.25678405\nIteration 136, loss = 0.25593420\nIteration 137, loss = 0.25605332\nIteration 138, loss = 0.25551843\nIteration 139, loss = 0.25570499\nIteration 140, loss = 0.25435456\nIteration 141, loss = 0.25488176\nIteration 142, loss = 0.25581568\nIteration 143, loss = 0.25417853\nIteration 144, loss = 0.25441339\nIteration 145, loss = 0.25577666\nIteration 146, loss = 0.25467501\nIteration 147, loss = 0.25323380\nIteration 148, loss = 0.25401239\nIteration 149, loss = 0.25305445\nIteration 150, loss = 0.25214412\nIteration 151, loss = 0.25281859\nIteration 152, loss = 0.25264839\nIteration 153, loss = 0.25292845\nIteration 154, loss = 0.25260438\nIteration 155, loss = 0.25182335\nIteration 156, loss = 0.25079938\nIteration 157, loss = 0.25207131\nIteration 158, loss = 0.25089969\nIteration 159, loss = 0.25089837\nIteration 160, loss = 0.25017131\nIteration 161, loss = 0.25126533\nIteration 162, loss = 0.25291342\nIteration 163, loss = 0.25113980\nIteration 164, loss = 0.24917181\nIteration 165, loss = 0.24976528\nIteration 166, loss = 0.24975632\nIteration 167, loss = 0.24836216\nIteration 168, loss = 0.25009685\nIteration 169, loss = 0.24939176\nIteration 170, loss = 0.24994661\nIteration 171, loss = 0.24977121\nIteration 172, loss = 0.24830887\nIteration 173, loss = 0.24939887\nIteration 174, loss = 0.24857304\nIteration 175, loss = 0.24849981\nIteration 176, loss = 0.24706277\nIteration 177, loss = 0.24802551\nIteration 178, loss = 0.24726679\nIteration 179, loss = 0.24797231\nIteration 180, loss = 0.24831501\nIteration 181, loss = 0.24648099\nIteration 182, loss = 0.24812712\nIteration 183, loss = 0.24845114\nIteration 184, loss = 0.24725279\nIteration 185, loss = 0.24852822\nIteration 186, loss = 0.24737782\nIteration 187, loss = 0.24694114\nIteration 188, loss = 0.24414690\nIteration 189, loss = 0.24570543\nIteration 190, loss = 0.24719121\nIteration 191, loss = 0.24735235\nIteration 192, loss = 0.24420654\nIteration 193, loss = 0.24403849\nIteration 194, loss = 0.24676390\nIteration 195, loss = 0.24440622\nIteration 196, loss = 0.24499603\nIteration 197, loss = 0.24587485\nIteration 198, loss = 0.24563289\nIteration 199, loss = 0.24369626\nIteration 200, loss = 0.24394471\nIteration 201, loss = 0.24379085\nIteration 202, loss = 0.24512500\nIteration 203, loss = 0.24533667\nIteration 204, loss = 0.24383950\nIteration 205, loss = 0.24461653\nIteration 206, loss = 0.24311757\nIteration 207, loss = 0.24499651\nIteration 208, loss = 0.24405535\nIteration 209, loss = 0.24320719\nIteration 210, loss = 0.24270322\nIteration 211, loss = 0.24372826\nIteration 212, loss = 0.24300560\nIteration 213, loss = 0.24245078\nIteration 214, loss = 0.24082033\nIteration 215, loss = 0.24179890\nIteration 216, loss = 0.24213690\nIteration 217, loss = 0.24461586\nIteration 218, loss = 0.24195574\nIteration 219, loss = 0.24237955\nIteration 220, loss = 0.24172418\nIteration 221, loss = 0.24040668\nIteration 222, loss = 0.24202290\nIteration 223, loss = 0.24125277\nIteration 224, loss = 0.24286216\nIteration 225, loss = 0.24206064\nIteration 226, loss = 0.24153332\nIteration 227, loss = 0.24099153\nIteration 228, loss = 0.24016848\nIteration 229, loss = 0.24069334\nIteration 230, loss = 0.24162123\nIteration 231, loss = 0.24008567\nIteration 232, loss = 0.23859849\nIteration 233, loss = 0.24035653\nIteration 234, loss = 0.24208520\nIteration 235, loss = 0.24047793\nIteration 236, loss = 0.24009175\nIteration 237, loss = 0.24137211\nIteration 238, loss = 0.24074067\nIteration 239, loss = 0.23997511\nIteration 240, loss = 0.23932745\nIteration 241, loss = 0.23917182\nIteration 242, loss = 0.24078816\nIteration 243, loss = 0.23950656\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\nIteration 1, loss = 0.36418116\nIteration 2, loss = 0.34914591\nIteration 3, loss = 0.34540406\nIteration 4, loss = 0.34197974\nIteration 5, loss = 0.33999232\nIteration 6, loss = 0.33856386\nIteration 7, loss = 0.33688128\nIteration 8, loss = 0.33427005\nIteration 9, loss = 0.33347065\nIteration 10, loss = 0.33136716\nIteration 11, loss = 0.33009106\nIteration 12, loss = 0.32817070\nIteration 13, loss = 0.32803056\nIteration 14, loss = 0.32634743\nIteration 15, loss = 0.32515196\nIteration 16, loss = 0.32400137\nIteration 17, loss = 0.32218132\nIteration 18, loss = 0.32081014\nIteration 19, loss = 0.31985063\nIteration 20, loss = 0.31884097\nIteration 21, loss = 0.31774903\nIteration 22, loss = 0.31628858\nIteration 23, loss = 0.31574350\nIteration 24, loss = 0.31393902\nIteration 25, loss = 0.31302614\nIteration 26, loss = 0.31157993\nIteration 27, loss = 0.31099031\nIteration 28, loss = 0.30907676\nIteration 29, loss = 0.30869532\nIteration 30, loss = 0.30723029\nIteration 31, loss = 0.30631336\nIteration 32, loss = 0.30473411\nIteration 33, loss = 0.30350790\nIteration 34, loss = 0.30275885\nIteration 35, loss = 0.30113220\nIteration 36, loss = 0.30018207\nIteration 37, loss = 0.30006209\nIteration 38, loss = 0.29898279\nIteration 39, loss = 0.29759163\nIteration 40, loss = 0.29702837\nIteration 41, loss = 0.29631740\nIteration 42, loss = 0.29571810\nIteration 43, loss = 0.29461796\nIteration 44, loss = 0.29399768\nIteration 45, loss = 0.29284577\nIteration 46, loss = 0.29111573\nIteration 47, loss = 0.29123116\nIteration 48, loss = 0.29049079\nIteration 49, loss = 0.29016879\nIteration 50, loss = 0.28886620\nIteration 51, loss = 0.28825505\nIteration 52, loss = 0.28767677\nIteration 53, loss = 0.28696467\nIteration 54, loss = 0.28519573\nIteration 55, loss = 0.28583932\nIteration 56, loss = 0.28476858\nIteration 57, loss = 0.28419645\nIteration 58, loss = 0.28363872\nIteration 59, loss = 0.28269965\nIteration 60, loss = 0.28194914\nIteration 61, loss = 0.28178184\nIteration 62, loss = 0.28034792\nIteration 63, loss = 0.27993504\nIteration 64, loss = 0.27895244\nIteration 65, loss = 0.27987038\nIteration 66, loss = 0.27784553\nIteration 67, loss = 0.27756468\nIteration 68, loss = 0.27651233\nIteration 69, loss = 0.27688788\nIteration 70, loss = 0.27635067\nIteration 71, loss = 0.27628983\nIteration 72, loss = 0.27594701\nIteration 73, loss = 0.27337677\nIteration 74, loss = 0.27386942\nIteration 75, loss = 0.27390746\nIteration 76, loss = 0.27303321\nIteration 77, loss = 0.27209437\nIteration 78, loss = 0.27256622\nIteration 79, loss = 0.27084228\nIteration 80, loss = 0.27123515\nIteration 81, loss = 0.27202094\nIteration 82, loss = 0.27219780\nIteration 83, loss = 0.27063088\nIteration 84, loss = 0.26847079\nIteration 85, loss = 0.26888245\nIteration 86, loss = 0.26872203\nIteration 87, loss = 0.26909669\nIteration 88, loss = 0.26871839\nIteration 89, loss = 0.26821713\nIteration 90, loss = 0.26848445\nIteration 91, loss = 0.26636179\nIteration 92, loss = 0.26657206\nIteration 93, loss = 0.26712298\nIteration 94, loss = 0.26629102\nIteration 95, loss = 0.26496261\nIteration 96, loss = 0.26490996\nIteration 97, loss = 0.26485457\nIteration 98, loss = 0.26443459\nIteration 99, loss = 0.26361947\nIteration 100, loss = 0.26343214\nIteration 101, loss = 0.26325920\nIteration 102, loss = 0.26167170\nIteration 103, loss = 0.26122582\nIteration 104, loss = 0.26223123\nIteration 105, loss = 0.26300970\nIteration 106, loss = 0.26174399\nIteration 107, loss = 0.26149531\nIteration 108, loss = 0.26184369\nIteration 109, loss = 0.25982776\nIteration 110, loss = 0.26051090\nIteration 111, loss = 0.25973957\nIteration 112, loss = 0.25983163\nIteration 113, loss = 0.26050953\nIteration 114, loss = 0.25878177\nIteration 115, loss = 0.25873797\nIteration 116, loss = 0.25854317\nIteration 117, loss = 0.25889734\nIteration 118, loss = 0.25796002\nIteration 119, loss = 0.25735317\nIteration 120, loss = 0.25626294\nIteration 121, loss = 0.25662431\nIteration 122, loss = 0.25834035\nIteration 123, loss = 0.25671440\nIteration 124, loss = 0.25720063\nIteration 125, loss = 0.25628630\nIteration 126, loss = 0.25626307\nIteration 127, loss = 0.25520297\nIteration 128, loss = 0.25474122\nIteration 129, loss = 0.25579733\nIteration 130, loss = 0.25477309\nIteration 131, loss = 0.25589059\nIteration 132, loss = 0.25422452\nIteration 133, loss = 0.25383023\nIteration 134, loss = 0.25363076\nIteration 135, loss = 0.25543013\nIteration 136, loss = 0.25379068\nIteration 137, loss = 0.25241290\nIteration 138, loss = 0.25206662\nIteration 139, loss = 0.25407309\nIteration 140, loss = 0.25236042\nIteration 141, loss = 0.25297151\nIteration 142, loss = 0.25275918\nIteration 143, loss = 0.25130733\nIteration 144, loss = 0.25172619\nIteration 145, loss = 0.25096676\nIteration 146, loss = 0.25194009\nIteration 147, loss = 0.25007326\nIteration 148, loss = 0.25167865\nIteration 149, loss = 0.24934735\nIteration 150, loss = 0.25020937\nIteration 151, loss = 0.25149550\nIteration 152, loss = 0.25003459\nIteration 153, loss = 0.25048408\nIteration 154, loss = 0.24926517\nIteration 155, loss = 0.24937614\nIteration 156, loss = 0.24897534\nIteration 157, loss = 0.24839832\nIteration 158, loss = 0.24985469\nIteration 159, loss = 0.24880844\nIteration 160, loss = 0.24669091\nIteration 161, loss = 0.24869110\nIteration 162, loss = 0.24848806\nIteration 163, loss = 0.24774204\nIteration 164, loss = 0.24873982\nIteration 165, loss = 0.24854677\nIteration 166, loss = 0.24777255\nIteration 167, loss = 0.24754964\nIteration 168, loss = 0.24506157\nIteration 169, loss = 0.24702980\nIteration 170, loss = 0.24690781\nIteration 171, loss = 0.24678734\nIteration 172, loss = 0.24769071\nIteration 173, loss = 0.24621367\nIteration 174, loss = 0.24553262\nIteration 175, loss = 0.24495022\nIteration 176, loss = 0.24703584\nIteration 177, loss = 0.24663823\nIteration 178, loss = 0.24532313\nIteration 179, loss = 0.24413574\nIteration 180, loss = 0.24408408\nIteration 181, loss = 0.24558683\nIteration 182, loss = 0.24504586\nIteration 183, loss = 0.24721507\nIteration 184, loss = 0.24474680\nIteration 185, loss = 0.24462729\nIteration 186, loss = 0.24539102\nIteration 187, loss = 0.24388687\nIteration 188, loss = 0.24366529\nIteration 189, loss = 0.24401358\nIteration 190, loss = 0.24362324\nIteration 191, loss = 0.24409976\nIteration 192, loss = 0.23999342\nIteration 193, loss = 0.24337801\nIteration 194, loss = 0.24141913\nIteration 195, loss = 0.24269381\nIteration 196, loss = 0.24390966\nIteration 197, loss = 0.24232174\nIteration 198, loss = 0.24194248\nIteration 199, loss = 0.24308514\nIteration 200, loss = 0.24061189\nIteration 201, loss = 0.24316920\nIteration 202, loss = 0.24436817\nIteration 203, loss = 0.24215435\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\nIteration 1, loss = 0.36509124\nIteration 2, loss = 0.34976749\nIteration 3, loss = 0.34473182\nIteration 4, loss = 0.34206389\nIteration 5, loss = 0.33966240\nIteration 6, loss = 0.33751806\nIteration 7, loss = 0.33635638\nIteration 8, loss = 0.33382480\nIteration 9, loss = 0.33272774\nIteration 10, loss = 0.33131795\nIteration 11, loss = 0.32989841\nIteration 12, loss = 0.32774563\nIteration 13, loss = 0.32763560\nIteration 14, loss = 0.32605637\nIteration 15, loss = 0.32535629\nIteration 16, loss = 0.32424807\nIteration 17, loss = 0.32248717\nIteration 18, loss = 0.32146701\nIteration 19, loss = 0.32025986\nIteration 20, loss = 0.31928849\nIteration 21, loss = 0.31829118\nIteration 22, loss = 0.31702221\nIteration 23, loss = 0.31546654\nIteration 24, loss = 0.31436615\nIteration 25, loss = 0.31365774\nIteration 26, loss = 0.31215193\nIteration 27, loss = 0.31124041\nIteration 28, loss = 0.31009260\nIteration 29, loss = 0.30922900\nIteration 30, loss = 0.30800925\nIteration 31, loss = 0.30696504\nIteration 32, loss = 0.30547750\nIteration 33, loss = 0.30512818\nIteration 34, loss = 0.30393312\nIteration 35, loss = 0.30362536\nIteration 36, loss = 0.30163994\nIteration 37, loss = 0.30122068\nIteration 38, loss = 0.30021588\nIteration 39, loss = 0.29979583\nIteration 40, loss = 0.29937899\nIteration 41, loss = 0.29791519\nIteration 42, loss = 0.29703346\nIteration 43, loss = 0.29614934\nIteration 44, loss = 0.29429068\nIteration 45, loss = 0.29470351\nIteration 46, loss = 0.29359517\nIteration 47, loss = 0.29272140\nIteration 48, loss = 0.29152138\nIteration 49, loss = 0.29217937\nIteration 50, loss = 0.29048608\nIteration 51, loss = 0.29026258\nIteration 52, loss = 0.29003872\nIteration 53, loss = 0.28842410\nIteration 54, loss = 0.28813058\nIteration 55, loss = 0.28688330\nIteration 56, loss = 0.28543107\nIteration 57, loss = 0.28626012\nIteration 58, loss = 0.28378116\nIteration 59, loss = 0.28340106\nIteration 60, loss = 0.28313439\nIteration 61, loss = 0.28406834\nIteration 62, loss = 0.28260207\nIteration 63, loss = 0.28103892\nIteration 64, loss = 0.28028523\nIteration 65, loss = 0.28079776\nIteration 66, loss = 0.28072710\nIteration 67, loss = 0.27925276\nIteration 68, loss = 0.27851108\nIteration 69, loss = 0.27813541\nIteration 70, loss = 0.27786360\nIteration 71, loss = 0.27761125\nIteration 72, loss = 0.27772993\nIteration 73, loss = 0.27691520\nIteration 74, loss = 0.27643226\nIteration 75, loss = 0.27539638\nIteration 76, loss = 0.27468659\nIteration 77, loss = 0.27428169\nIteration 78, loss = 0.27491475\nIteration 79, loss = 0.27274375\nIteration 80, loss = 0.27329981\nIteration 81, loss = 0.27221490\nIteration 82, loss = 0.27284484\nIteration 83, loss = 0.27161367\nIteration 84, loss = 0.27140433\nIteration 85, loss = 0.26948015\nIteration 86, loss = 0.27017069\nIteration 87, loss = 0.27043632\nIteration 88, loss = 0.27122639\nIteration 89, loss = 0.26917551\nIteration 90, loss = 0.26880430\nIteration 91, loss = 0.26801571\nIteration 92, loss = 0.26755181\nIteration 93, loss = 0.26706247\nIteration 94, loss = 0.26579197\nIteration 95, loss = 0.26643600\nIteration 96, loss = 0.26665830\nIteration 97, loss = 0.26691407\nIteration 98, loss = 0.26767836\nIteration 99, loss = 0.26515072\nIteration 100, loss = 0.26639507\nIteration 101, loss = 0.26550942\nIteration 102, loss = 0.26520301\nIteration 103, loss = 0.26401366\nIteration 104, loss = 0.26368604\nIteration 105, loss = 0.26370274\nIteration 106, loss = 0.26421150\nIteration 107, loss = 0.26310753\nIteration 108, loss = 0.26467089\nIteration 109, loss = 0.26190869\nIteration 110, loss = 0.26126445\nIteration 111, loss = 0.26316132\nIteration 112, loss = 0.26043201\nIteration 113, loss = 0.26249862\nIteration 114, loss = 0.26122907\nIteration 115, loss = 0.26116353\nIteration 116, loss = 0.26134688\nIteration 117, loss = 0.26018175\nIteration 118, loss = 0.26156943\nIteration 119, loss = 0.25875310\nIteration 120, loss = 0.25981687\nIteration 121, loss = 0.25891565\nIteration 122, loss = 0.25827247\nIteration 123, loss = 0.25992638\nIteration 124, loss = 0.25882292\nIteration 125, loss = 0.25928474\nIteration 126, loss = 0.25911605\nIteration 127, loss = 0.25728387\nIteration 128, loss = 0.25740616\nIteration 129, loss = 0.25704782\nIteration 130, loss = 0.25745407\nIteration 131, loss = 0.25788683\nIteration 132, loss = 0.25763524\nIteration 133, loss = 0.25650284\nIteration 134, loss = 0.25575472\nIteration 135, loss = 0.25564356\nIteration 136, loss = 0.25854147\nIteration 137, loss = 0.25572286\nIteration 138, loss = 0.25593246\nIteration 139, loss = 0.25526642\nIteration 140, loss = 0.25507262\nIteration 141, loss = 0.25307134\nIteration 142, loss = 0.25375068\nIteration 143, loss = 0.25399081\nIteration 144, loss = 0.25404972\nIteration 145, loss = 0.25546924\nIteration 146, loss = 0.25407793\nIteration 147, loss = 0.25453388\nIteration 148, loss = 0.25196979\nIteration 149, loss = 0.25387350\nIteration 150, loss = 0.25351900\nIteration 151, loss = 0.25261734\nIteration 152, loss = 0.25358932\nIteration 153, loss = 0.25252795\nIteration 154, loss = 0.25060266\nIteration 155, loss = 0.25374925\nIteration 156, loss = 0.25210164\nIteration 157, loss = 0.25089319\nIteration 158, loss = 0.25231268\nIteration 159, loss = 0.25211516\nIteration 160, loss = 0.25224990\nIteration 161, loss = 0.25154909\nIteration 162, loss = 0.24929157\nIteration 163, loss = 0.25139992\nIteration 164, loss = 0.24890733\nIteration 165, loss = 0.25001228\nIteration 166, loss = 0.24996243\nIteration 167, loss = 0.24962680\nIteration 168, loss = 0.24877080\nIteration 169, loss = 0.24805880\nIteration 170, loss = 0.25043667\nIteration 171, loss = 0.24983370\nIteration 172, loss = 0.24837691\nIteration 173, loss = 0.24865473\nIteration 174, loss = 0.24842878\nIteration 175, loss = 0.24852819\nIteration 176, loss = 0.24948591\nIteration 177, loss = 0.24887146\nIteration 178, loss = 0.24787638\nIteration 179, loss = 0.24822293\nIteration 180, loss = 0.24743882\nIteration 181, loss = 0.24801204\nIteration 182, loss = 0.24693882\nIteration 183, loss = 0.24666580\nIteration 184, loss = 0.24831557\nIteration 185, loss = 0.24913350\nIteration 186, loss = 0.24432865\nIteration 187, loss = 0.24603030\nIteration 188, loss = 0.24623461\nIteration 189, loss = 0.24663569\nIteration 190, loss = 0.24542378\nIteration 191, loss = 0.24450376\nIteration 192, loss = 0.24515987\nIteration 193, loss = 0.24439642\nIteration 194, loss = 0.24662194\nIteration 195, loss = 0.24775683\nIteration 196, loss = 0.24393704\nIteration 197, loss = 0.24506643\nIteration 198, loss = 0.24663294\nIteration 199, loss = 0.24520348\nIteration 200, loss = 0.24588011\nIteration 201, loss = 0.24343786\nIteration 202, loss = 0.24481477\nIteration 203, loss = 0.24391549\nIteration 204, loss = 0.24394465\nIteration 205, loss = 0.24592497\nIteration 206, loss = 0.24351337\nIteration 207, loss = 0.24364582\nIteration 208, loss = 0.24323136\nIteration 209, loss = 0.24446837\nIteration 210, loss = 0.24423056\nIteration 211, loss = 0.24432151\nIteration 212, loss = 0.24272252\nIteration 213, loss = 0.24230736\nIteration 214, loss = 0.24423496\nIteration 215, loss = 0.24440248\nIteration 216, loss = 0.24247976\nIteration 217, loss = 0.24273470\nIteration 218, loss = 0.24237021\nIteration 219, loss = 0.24427779\nIteration 220, loss = 0.24078957\nIteration 221, loss = 0.24241058\nIteration 222, loss = 0.24117721\nIteration 223, loss = 0.24093522\nIteration 224, loss = 0.24287471\nIteration 225, loss = 0.24251620\nIteration 226, loss = 0.24107781\nIteration 227, loss = 0.24351894\nIteration 228, loss = 0.24136127\nIteration 229, loss = 0.24008879\nIteration 230, loss = 0.24008938\nIteration 231, loss = 0.24213136\nIteration 232, loss = 0.24122008\nIteration 233, loss = 0.24168779\nIteration 234, loss = 0.24426163\nIteration 235, loss = 0.24088354\nIteration 236, loss = 0.24028169\nIteration 237, loss = 0.24010302\nIteration 238, loss = 0.23953304\nIteration 239, loss = 0.24187442\nIteration 240, loss = 0.24042083\nIteration 241, loss = 0.23994630\nIteration 242, loss = 0.24096858\nIteration 243, loss = 0.23965017\nIteration 244, loss = 0.23814884\nIteration 245, loss = 0.24124187\nIteration 246, loss = 0.23869378\nIteration 247, loss = 0.24011965\nIteration 248, loss = 0.24007734\nIteration 249, loss = 0.24247816\nIteration 250, loss = 0.24075756\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36792457\nIteration 2, loss = 0.34997095\nIteration 3, loss = 0.34450913\nIteration 4, loss = 0.34228988\nIteration 5, loss = 0.34031654\nIteration 6, loss = 0.33776617\nIteration 7, loss = 0.33622506\nIteration 8, loss = 0.33486506\nIteration 9, loss = 0.33409538\nIteration 10, loss = 0.33277767\nIteration 11, loss = 0.33124848\nIteration 12, loss = 0.33034991\nIteration 13, loss = 0.32917219\nIteration 14, loss = 0.32812671\nIteration 15, loss = 0.32615971\nIteration 16, loss = 0.32561000\nIteration 17, loss = 0.32467359\nIteration 18, loss = 0.32398617\nIteration 19, loss = 0.32197021\nIteration 20, loss = 0.32186373\nIteration 21, loss = 0.32062587\nIteration 22, loss = 0.31992367\nIteration 23, loss = 0.31810817\nIteration 24, loss = 0.31686620\nIteration 25, loss = 0.31665253\nIteration 26, loss = 0.31554848\nIteration 27, loss = 0.31468598\nIteration 28, loss = 0.31346458\nIteration 29, loss = 0.31245675\nIteration 30, loss = 0.31228790\nIteration 31, loss = 0.31157420\nIteration 32, loss = 0.31035515\nIteration 33, loss = 0.30925843\nIteration 34, loss = 0.30790388\nIteration 35, loss = 0.30794155\nIteration 36, loss = 0.30657294\nIteration 37, loss = 0.30628844\nIteration 38, loss = 0.30521783\nIteration 39, loss = 0.30402997\nIteration 40, loss = 0.30455515\nIteration 41, loss = 0.30297873\nIteration 42, loss = 0.30159590\nIteration 43, loss = 0.30109891\nIteration 44, loss = 0.30076993\nIteration 45, loss = 0.29924254\nIteration 46, loss = 0.29910477\nIteration 47, loss = 0.29882239\nIteration 48, loss = 0.29795326\nIteration 49, loss = 0.29637738\nIteration 50, loss = 0.29607028\nIteration 51, loss = 0.29571602\nIteration 52, loss = 0.29618074\nIteration 53, loss = 0.29411978\nIteration 54, loss = 0.29333712\nIteration 55, loss = 0.29354547\nIteration 56, loss = 0.29271783\nIteration 57, loss = 0.29303951\nIteration 58, loss = 0.29117465\nIteration 59, loss = 0.29159352\nIteration 60, loss = 0.29087479\nIteration 61, loss = 0.29042476\nIteration 62, loss = 0.28928827\nIteration 63, loss = 0.28873636\nIteration 64, loss = 0.28856120\nIteration 65, loss = 0.28856813\nIteration 66, loss = 0.28761006\nIteration 67, loss = 0.28734384\nIteration 68, loss = 0.28679640\nIteration 69, loss = 0.28595229\nIteration 70, loss = 0.28528568\nIteration 71, loss = 0.28583858\nIteration 72, loss = 0.28431948\nIteration 73, loss = 0.28456731\nIteration 74, loss = 0.28412038\nIteration 75, loss = 0.28413667\nIteration 76, loss = 0.28336531\nIteration 77, loss = 0.28379435\nIteration 78, loss = 0.28234613\nIteration 79, loss = 0.28246945\nIteration 80, loss = 0.28232400\nIteration 81, loss = 0.28053713\nIteration 82, loss = 0.28213199\nIteration 83, loss = 0.28107221\nIteration 84, loss = 0.28023663\nIteration 85, loss = 0.28001109\nIteration 86, loss = 0.27851980\nIteration 87, loss = 0.27947164\nIteration 88, loss = 0.27948543\nIteration 89, loss = 0.27966200\nIteration 90, loss = 0.27893850\nIteration 91, loss = 0.27857536\nIteration 92, loss = 0.27786785\nIteration 93, loss = 0.27808933\nIteration 94, loss = 0.27708134\nIteration 95, loss = 0.27712807\nIteration 96, loss = 0.27682761\nIteration 97, loss = 0.27694110\nIteration 98, loss = 0.27667957\nIteration 99, loss = 0.27579354\nIteration 100, loss = 0.27533833\nIteration 101, loss = 0.27472011\nIteration 102, loss = 0.27676442\nIteration 103, loss = 0.27476855\nIteration 104, loss = 0.27267524\nIteration 105, loss = 0.27305823\nIteration 106, loss = 0.27361245\nIteration 107, loss = 0.27396957\nIteration 108, loss = 0.27400203\nIteration 109, loss = 0.27472223\nIteration 110, loss = 0.27249607\nIteration 111, loss = 0.27239568\nIteration 112, loss = 0.27200051\nIteration 113, loss = 0.27108783\nIteration 114, loss = 0.27117386\nIteration 115, loss = 0.27096855\nIteration 116, loss = 0.27047294\nIteration 117, loss = 0.27146435\nIteration 118, loss = 0.27056815\nIteration 119, loss = 0.27065606\nIteration 120, loss = 0.26968485\nIteration 121, loss = 0.27036002\nIteration 122, loss = 0.26919213\nIteration 123, loss = 0.26945060\nIteration 124, loss = 0.27049506\nIteration 125, loss = 0.26922044\nIteration 126, loss = 0.26928125\nIteration 127, loss = 0.26808812\nIteration 128, loss = 0.26644101\nIteration 129, loss = 0.26835465\nIteration 130, loss = 0.26711034\nIteration 131, loss = 0.26812063\nIteration 132, loss = 0.26782614\nIteration 133, loss = 0.26764514\nIteration 134, loss = 0.26559525\nIteration 135, loss = 0.26763603\nIteration 136, loss = 0.26710294\nIteration 137, loss = 0.26626118\nIteration 138, loss = 0.26543304\nIteration 139, loss = 0.26561631\nIteration 140, loss = 0.26656466\nIteration 141, loss = 0.26489361\nIteration 142, loss = 0.26531047\nIteration 143, loss = 0.26449571\nIteration 144, loss = 0.26472551\nIteration 145, loss = 0.26560815\nIteration 146, loss = 0.26508645\nIteration 147, loss = 0.26546969\nIteration 148, loss = 0.26432473\nIteration 149, loss = 0.26263213\nIteration 150, loss = 0.26562818\nIteration 151, loss = 0.26373000\nIteration 152, loss = 0.26419879\nIteration 153, loss = 0.26450133\nIteration 154, loss = 0.26204223\nIteration 155, loss = 0.26370177\nIteration 156, loss = 0.26172289\nIteration 157, loss = 0.26260554\nIteration 158, loss = 0.26191209\nIteration 159, loss = 0.26129504\nIteration 160, loss = 0.26134047\nIteration 161, loss = 0.26156829\nIteration 162, loss = 0.26204248\nIteration 163, loss = 0.26324028\nIteration 164, loss = 0.26220246\nIteration 165, loss = 0.26035445\nIteration 166, loss = 0.26163848\nIteration 167, loss = 0.26148598\nIteration 168, loss = 0.26065543\nIteration 169, loss = 0.26013562\nIteration 170, loss = 0.26291142\nIteration 171, loss = 0.25886444\nIteration 172, loss = 0.26098395\nIteration 173, loss = 0.25969968\nIteration 174, loss = 0.26037551\nIteration 175, loss = 0.26013974\nIteration 176, loss = 0.26015027\nIteration 177, loss = 0.26041663\nIteration 178, loss = 0.26057896\nIteration 179, loss = 0.25930167\nIteration 180, loss = 0.25860569\nIteration 181, loss = 0.25878738\nIteration 182, loss = 0.25924516\nIteration 183, loss = 0.25658518\nIteration 184, loss = 0.25864035\nIteration 185, loss = 0.26013452\nIteration 186, loss = 0.25784354\nIteration 187, loss = 0.25861017\nIteration 188, loss = 0.25713412\nIteration 189, loss = 0.25743915\nIteration 190, loss = 0.25996284\nIteration 191, loss = 0.25646767\nIteration 192, loss = 0.25696223\nIteration 193, loss = 0.25932873\nIteration 194, loss = 0.25764174\nIteration 195, loss = 0.25664771\nIteration 196, loss = 0.25636448\nIteration 197, loss = 0.25577622\nIteration 198, loss = 0.25524524\nIteration 199, loss = 0.25715563\nIteration 200, loss = 0.25730709\nIteration 201, loss = 0.25634166\nIteration 202, loss = 0.25636754\nIteration 203, loss = 0.25680897\nIteration 204, loss = 0.25724025\nIteration 205, loss = 0.25597183\nIteration 206, loss = 0.25517091\nIteration 207, loss = 0.25353738\nIteration 208, loss = 0.25582668\nIteration 209, loss = 0.25852070\nIteration 210, loss = 0.25440252\nIteration 211, loss = 0.25460107\nIteration 212, loss = 0.25421049\nIteration 213, loss = 0.25588971\nIteration 214, loss = 0.25486007\nIteration 215, loss = 0.25699244\nIteration 216, loss = 0.25435207\nIteration 217, loss = 0.25449681\nIteration 218, loss = 0.25328136\nIteration 219, loss = 0.25590058\nIteration 220, loss = 0.25422915\nIteration 221, loss = 0.25624474\nIteration 222, loss = 0.25587950\nIteration 223, loss = 0.25674703\nIteration 224, loss = 0.25384906\nIteration 225, loss = 0.25365974\nIteration 226, loss = 0.25391674\nIteration 227, loss = 0.25372928\nIteration 228, loss = 0.25229196\nIteration 229, loss = 0.25401230\nIteration 230, loss = 0.25416613\nIteration 231, loss = 0.25276386\nIteration 232, loss = 0.25365047\nIteration 233, loss = 0.25245004\nIteration 234, loss = 0.25478740\nIteration 235, loss = 0.25039288\nIteration 236, loss = 0.25320174\nIteration 237, loss = 0.25427038\nIteration 238, loss = 0.25457882\nIteration 239, loss = 0.25159629\nIteration 240, loss = 0.25243994\nIteration 241, loss = 0.25174150\nIteration 242, loss = 0.25288508\nIteration 243, loss = 0.25373062\nIteration 244, loss = 0.25180832\nIteration 245, loss = 0.25187021\nIteration 246, loss = 0.24998507\nIteration 247, loss = 0.25315925\nIteration 248, loss = 0.25382277\nIteration 249, loss = 0.25252998\nIteration 250, loss = 0.25206541\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36652607\nIteration 2, loss = 0.34846452\nIteration 3, loss = 0.34301459\nIteration 4, loss = 0.34141863\nIteration 5, loss = 0.33848373\nIteration 6, loss = 0.33654452\nIteration 7, loss = 0.33500121\nIteration 8, loss = 0.33381002\nIteration 9, loss = 0.33255814\nIteration 10, loss = 0.33085697\nIteration 11, loss = 0.33041153\nIteration 12, loss = 0.32844301\nIteration 13, loss = 0.32744497\nIteration 14, loss = 0.32685315\nIteration 15, loss = 0.32511721\nIteration 16, loss = 0.32412856\nIteration 17, loss = 0.32262046\nIteration 18, loss = 0.32131868\nIteration 19, loss = 0.32019509\nIteration 20, loss = 0.31978542\nIteration 21, loss = 0.31830652\nIteration 22, loss = 0.31683574\nIteration 23, loss = 0.31610263\nIteration 24, loss = 0.31512112\nIteration 25, loss = 0.31381429\nIteration 26, loss = 0.31241817\nIteration 27, loss = 0.31212603\nIteration 28, loss = 0.31092322\nIteration 29, loss = 0.30980028\nIteration 30, loss = 0.30920687\nIteration 31, loss = 0.30727031\nIteration 32, loss = 0.30697806\nIteration 33, loss = 0.30611585\nIteration 34, loss = 0.30482508\nIteration 35, loss = 0.30472040\nIteration 36, loss = 0.30296565\nIteration 37, loss = 0.30209241\nIteration 38, loss = 0.30134063\nIteration 39, loss = 0.30008443\nIteration 40, loss = 0.30008799\nIteration 41, loss = 0.29907246\nIteration 42, loss = 0.29721525\nIteration 43, loss = 0.29705310\nIteration 44, loss = 0.29566583\nIteration 45, loss = 0.29571740\nIteration 46, loss = 0.29477592\nIteration 47, loss = 0.29376797\nIteration 48, loss = 0.29249656\nIteration 49, loss = 0.29176840\nIteration 50, loss = 0.29203395\nIteration 51, loss = 0.29026981\nIteration 52, loss = 0.28919837\nIteration 53, loss = 0.28819719\nIteration 54, loss = 0.28883858\nIteration 55, loss = 0.28758760\nIteration 56, loss = 0.28740726\nIteration 57, loss = 0.28617095\nIteration 58, loss = 0.28617017\nIteration 59, loss = 0.28565770\nIteration 60, loss = 0.28478078\nIteration 61, loss = 0.28437997\nIteration 62, loss = 0.28373896\nIteration 63, loss = 0.28315293\nIteration 64, loss = 0.28232981\nIteration 65, loss = 0.28099223\nIteration 66, loss = 0.28081516\nIteration 67, loss = 0.28019407\nIteration 68, loss = 0.27909910\nIteration 69, loss = 0.27913463\nIteration 70, loss = 0.27841207\nIteration 71, loss = 0.27760077\nIteration 72, loss = 0.27750146\nIteration 73, loss = 0.27856172\nIteration 74, loss = 0.27631474\nIteration 75, loss = 0.27587315\nIteration 76, loss = 0.27668515\nIteration 77, loss = 0.27485885\nIteration 78, loss = 0.27415564\nIteration 79, loss = 0.27355487\nIteration 80, loss = 0.27412397\nIteration 81, loss = 0.27316295\nIteration 82, loss = 0.27305283\nIteration 83, loss = 0.27190184\nIteration 84, loss = 0.27189607\nIteration 85, loss = 0.27109144\nIteration 86, loss = 0.27058550\nIteration 87, loss = 0.27071294\nIteration 88, loss = 0.27071295\nIteration 89, loss = 0.27074777\nIteration 90, loss = 0.26966383\nIteration 91, loss = 0.26871037\nIteration 92, loss = 0.26825822\nIteration 93, loss = 0.26886771\nIteration 94, loss = 0.26862582\nIteration 95, loss = 0.26685223\nIteration 96, loss = 0.26757959\nIteration 97, loss = 0.26620450\nIteration 98, loss = 0.26664606\nIteration 99, loss = 0.26607350\nIteration 100, loss = 0.26528982\nIteration 101, loss = 0.26582563\nIteration 102, loss = 0.26483965\nIteration 103, loss = 0.26422865\nIteration 104, loss = 0.26514743\nIteration 105, loss = 0.26395881\nIteration 106, loss = 0.26316358\nIteration 107, loss = 0.26348776\nIteration 108, loss = 0.26382400\nIteration 109, loss = 0.26241795\nIteration 110, loss = 0.26161522\nIteration 111, loss = 0.26059877\nIteration 112, loss = 0.26180813\nIteration 113, loss = 0.26106300\nIteration 114, loss = 0.26248253\nIteration 115, loss = 0.26053365\nIteration 116, loss = 0.26025987\nIteration 117, loss = 0.25924859\nIteration 118, loss = 0.25855890\nIteration 119, loss = 0.25991658\nIteration 120, loss = 0.25889055\nIteration 121, loss = 0.25947107\nIteration 122, loss = 0.25825517\nIteration 123, loss = 0.25820110\nIteration 124, loss = 0.25887693\nIteration 125, loss = 0.25745088\nIteration 126, loss = 0.25772250\nIteration 127, loss = 0.25762131\nIteration 128, loss = 0.25725968\nIteration 129, loss = 0.25566195\nIteration 130, loss = 0.25644794\nIteration 131, loss = 0.25578537\nIteration 132, loss = 0.25644845\nIteration 133, loss = 0.25667159\nIteration 134, loss = 0.25424261\nIteration 135, loss = 0.25585568\nIteration 136, loss = 0.25510661\nIteration 137, loss = 0.25561707\nIteration 138, loss = 0.25351957\nIteration 139, loss = 0.25537721\nIteration 140, loss = 0.25330927\nIteration 141, loss = 0.25443510\nIteration 142, loss = 0.25221844\nIteration 143, loss = 0.25289089\nIteration 144, loss = 0.25330790\nIteration 145, loss = 0.25292181\nIteration 146, loss = 0.25140752\nIteration 147, loss = 0.25310899\nIteration 148, loss = 0.25245462\nIteration 149, loss = 0.25159506\nIteration 150, loss = 0.25135184\nIteration 151, loss = 0.25192065\nIteration 152, loss = 0.25231676\nIteration 153, loss = 0.25251128\nIteration 154, loss = 0.25129908\nIteration 155, loss = 0.25110932\nIteration 156, loss = 0.25045833\nIteration 157, loss = 0.25201315\nIteration 158, loss = 0.24921504\nIteration 159, loss = 0.24945903\nIteration 160, loss = 0.24853378\nIteration 161, loss = 0.25071494\nIteration 162, loss = 0.25072245\nIteration 163, loss = 0.24808038\nIteration 164, loss = 0.24972876\nIteration 165, loss = 0.25047839\nIteration 166, loss = 0.24827507\nIteration 167, loss = 0.25021865\nIteration 168, loss = 0.24839239\nIteration 169, loss = 0.24987426\nIteration 170, loss = 0.24800838\nIteration 171, loss = 0.24734054\nIteration 172, loss = 0.24960902\nIteration 173, loss = 0.24852727\nIteration 174, loss = 0.24785417\nIteration 175, loss = 0.24752258\nIteration 176, loss = 0.24821254\nIteration 177, loss = 0.24696947\nIteration 178, loss = 0.24814979\nIteration 179, loss = 0.24690210\nIteration 180, loss = 0.24588772\nIteration 181, loss = 0.24909614\nIteration 182, loss = 0.24692486\nIteration 183, loss = 0.24641525\nIteration 184, loss = 0.24738708\nIteration 185, loss = 0.24639244\nIteration 186, loss = 0.24552240\nIteration 187, loss = 0.24534912\nIteration 188, loss = 0.24728770\nIteration 189, loss = 0.24525442\nIteration 190, loss = 0.24456099\nIteration 191, loss = 0.24653516\nIteration 192, loss = 0.24519255\nIteration 193, loss = 0.24619938\nIteration 194, loss = 0.24352179\nIteration 195, loss = 0.24430280\nIteration 196, loss = 0.24341432\nIteration 197, loss = 0.24566938\nIteration 198, loss = 0.24390929\nIteration 199, loss = 0.24397748\nIteration 200, loss = 0.24475827\nIteration 201, loss = 0.24277716\nIteration 202, loss = 0.24458753\nIteration 203, loss = 0.24378877\nIteration 204, loss = 0.24390020\nIteration 205, loss = 0.24235833\nIteration 206, loss = 0.24328652\nIteration 207, loss = 0.24262832\nIteration 208, loss = 0.24445738\nIteration 209, loss = 0.24255956\nIteration 210, loss = 0.24325060\nIteration 211, loss = 0.24300484\nIteration 212, loss = 0.24391196\nIteration 213, loss = 0.24118676\nIteration 214, loss = 0.24218933\nIteration 215, loss = 0.24166354\nIteration 216, loss = 0.24159746\nIteration 217, loss = 0.24132336\nIteration 218, loss = 0.24202320\nIteration 219, loss = 0.24201050\nIteration 220, loss = 0.24229672\nIteration 221, loss = 0.24130786\nIteration 222, loss = 0.24166353\nIteration 223, loss = 0.24231172\nIteration 224, loss = 0.24189935\nTraining loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\nIteration 1, loss = 0.36742943\nIteration 2, loss = 0.34972584\nIteration 3, loss = 0.34510091\nIteration 4, loss = 0.34291212\nIteration 5, loss = 0.34084110\nIteration 6, loss = 0.33878754\nIteration 7, loss = 0.33708381\nIteration 8, loss = 0.33523569\nIteration 9, loss = 0.33471932\nIteration 10, loss = 0.33254337\nIteration 11, loss = 0.33183199\nIteration 12, loss = 0.32953586\nIteration 13, loss = 0.32886849\nIteration 14, loss = 0.32835142\nIteration 15, loss = 0.32673057\nIteration 16, loss = 0.32546580\nIteration 17, loss = 0.32515101\nIteration 18, loss = 0.32339892\nIteration 19, loss = 0.32226303\nIteration 20, loss = 0.32085751\nIteration 21, loss = 0.31996526\nIteration 22, loss = 0.31936571\nIteration 23, loss = 0.31828906\nIteration 24, loss = 0.31682669\nIteration 25, loss = 0.31561003\nIteration 26, loss = 0.31439780\nIteration 27, loss = 0.31314864\nIteration 28, loss = 0.31286668\nIteration 29, loss = 0.31199359\nIteration 30, loss = 0.31024879\nIteration 31, loss = 0.30954349\nIteration 32, loss = 0.30871318\nIteration 33, loss = 0.30824048\nIteration 34, loss = 0.30653087\nIteration 35, loss = 0.30599642\nIteration 36, loss = 0.30482697\nIteration 37, loss = 0.30455273\nIteration 38, loss = 0.30302911\nIteration 39, loss = 0.30270739\nIteration 40, loss = 0.30206564\nIteration 41, loss = 0.30116393\nIteration 42, loss = 0.30069174\nIteration 43, loss = 0.29857896\nIteration 44, loss = 0.29771731\nIteration 45, loss = 0.29758419\nIteration 46, loss = 0.29691168\nIteration 47, loss = 0.29671724\nIteration 48, loss = 0.29545057\nIteration 49, loss = 0.29406024\nIteration 50, loss = 0.29376932\nIteration 51, loss = 0.29325343\nIteration 52, loss = 0.29369804\nIteration 53, loss = 0.29135148\nIteration 54, loss = 0.28990410\nIteration 55, loss = 0.29084103\nIteration 56, loss = 0.29026884\nIteration 57, loss = 0.28977128\nIteration 58, loss = 0.28851546\nIteration 59, loss = 0.28794810\nIteration 60, loss = 0.28660198\nIteration 61, loss = 0.28686137\nIteration 62, loss = 0.28620169\nIteration 63, loss = 0.28527701\nIteration 64, loss = 0.28584831\nIteration 65, loss = 0.28394100\nIteration 66, loss = 0.28413434\nIteration 67, loss = 0.28328670\nIteration 68, loss = 0.28224383\nIteration 69, loss = 0.28183233\nIteration 70, loss = 0.28174857\nIteration 71, loss = 0.28193950\nIteration 72, loss = 0.28074321\nIteration 73, loss = 0.27973080\nIteration 74, loss = 0.28021015\nIteration 75, loss = 0.27961164\nIteration 76, loss = 0.27898311\nIteration 77, loss = 0.27772148\nIteration 78, loss = 0.27743955\nIteration 79, loss = 0.27648045\nIteration 80, loss = 0.27748685\nIteration 81, loss = 0.27663692\nIteration 82, loss = 0.27631311\nIteration 83, loss = 0.27491933\nIteration 84, loss = 0.27476527\nIteration 85, loss = 0.27452868\nIteration 86, loss = 0.27488435\nIteration 87, loss = 0.27394497\nIteration 88, loss = 0.27310451\nIteration 89, loss = 0.27140902\nIteration 90, loss = 0.27345534\nIteration 91, loss = 0.27255995\nIteration 92, loss = 0.27092325\nIteration 93, loss = 0.27102997\nIteration 94, loss = 0.27067964\nIteration 95, loss = 0.26891469\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n  warnings.warn(\"Training interrupted by user.\")\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36635142\nIteration 2, loss = 0.34926659\nIteration 3, loss = 0.34467480\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n  warnings.warn(\"Training interrupted by user.\")\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36753868\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n  warnings.warn(\"Training interrupted by user.\")\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36677324\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n  warnings.warn(\"Training interrupted by user.\")\n/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n  warnings.warn(\"Training interrupted by user.\")\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36650725\nIteration 2, loss = 0.35208162\nIteration 3, loss = 0.34765546\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n  warnings.warn(\"Training interrupted by user.\")\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36580027\nIteration 2, loss = 0.35259201\nIteration 3, loss = 0.34855308\nIteration 4, loss = 0.34412867\nIteration 5, loss = 0.34223185\nIteration 6, loss = 0.34073042\nIteration 7, loss = 0.33912235\nIteration 8, loss = 0.33764572\nIteration 9, loss = 0.33606488\nIteration 10, loss = 0.33448688\nIteration 11, loss = 0.33380421\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n  warnings.warn(\"Training interrupted by user.\")\n","output_type":"stream"},{"name":"stdout","text":"Iteration 1, loss = 0.36696186\nIteration 2, loss = 0.35221777\nIteration 3, loss = 0.34722620\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n  warnings.warn(\"Training interrupted by user.\")\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/351439654.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparam_dicti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'hidden_layer_sizes'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'random_state'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m516\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'activation'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'logistic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'learning_rate_init'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'max_iter'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_dicti\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgrid_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_score\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \"\"\"\n\u001b[1;32m   1166\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pass_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_forward_pass_fast\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoefs_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mactivation\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercepts_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers_\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mhidden_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"raw","source":"\n","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import log_loss","metadata":{"execution":{"iopub.status.busy":"2022-12-04T21:07:43.193681Z","iopub.execute_input":"2022-12-04T21:07:43.194858Z","iopub.status.idle":"2022-12-04T21:07:43.201326Z","shell.execute_reply.started":"2022-12-04T21:07:43.194815Z","shell.execute_reply":"2022-12-04T21:07:43.199727Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"plt.plot(np.arange(0,10), np.arange(3,13))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-04T21:08:09.674588Z","iopub.execute_input":"2022-12-04T21:08:09.675028Z","iopub.status.idle":"2022-12-04T21:08:09.883167Z","shell.execute_reply.started":"2022-12-04T21:08:09.674994Z","shell.execute_reply":"2022-12-04T21:08:09.881999Z"},"trusted":true},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"def get_log_loss(y_true, y_pred):\n    loss_val = log_loss(y_true, y_pred, labels = np.unique(y_true))\n    return loss_val","metadata":{"execution":{"iopub.status.busy":"2022-12-04T20:51:47.369967Z","iopub.execute_input":"2022-12-04T20:51:47.370378Z","iopub.status.idle":"2022-12-04T20:51:47.375524Z","shell.execute_reply.started":"2022-12-04T20:51:47.370343Z","shell.execute_reply":"2022-12-04T20:51:47.374314Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"mlc = MLPClassifier(hidden_layer_sizes = (128, 64, 32), activation = 'relu', max_iter = 250, verbose = True)\n\nepochs = 150\ntrain_losses = []\ntest_losses = []\nfor i in range(epochs):\n    mlc = mlc.partial_fit(x_train, np.ravel(y_train), np.unique(y_train))\n    train_losses.append(mlc.loss_)\n    y_pred_proba = mlc.predict_proba(x_test)\n    test_losses.append(get_log_loss(y_test, y_pred_proba))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-04T21:10:58.556838Z","iopub.execute_input":"2022-12-04T21:10:58.557236Z","iopub.status.idle":"2022-12-04T21:18:55.623289Z","shell.execute_reply.started":"2022-12-04T21:10:58.557204Z","shell.execute_reply":"2022-12-04T21:18:55.621702Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Iteration 1, loss = 0.37573456\nIteration 2, loss = 0.34620268\nIteration 3, loss = 0.34131911\nIteration 4, loss = 0.33760161\nIteration 5, loss = 0.33606635\nIteration 6, loss = 0.33350334\nIteration 7, loss = 0.33152639\nIteration 8, loss = 0.33073955\nIteration 9, loss = 0.32900772\nIteration 10, loss = 0.32786743\nIteration 11, loss = 0.32637915\nIteration 12, loss = 0.32483682\nIteration 13, loss = 0.32401994\nIteration 14, loss = 0.32267116\nIteration 15, loss = 0.32237804\nIteration 16, loss = 0.32121839\nIteration 17, loss = 0.31985285\nIteration 18, loss = 0.31886177\nIteration 19, loss = 0.31776830\nIteration 20, loss = 0.31718006\nIteration 21, loss = 0.31594373\nIteration 22, loss = 0.31541283\nIteration 23, loss = 0.31429603\nIteration 24, loss = 0.31370895\nIteration 25, loss = 0.31262209\nIteration 26, loss = 0.31154491\nIteration 27, loss = 0.31135338\nIteration 28, loss = 0.30994330\nIteration 29, loss = 0.30894656\nIteration 30, loss = 0.30857514\nIteration 31, loss = 0.30770465\nIteration 32, loss = 0.30642900\nIteration 33, loss = 0.30647313\nIteration 34, loss = 0.30549552\nIteration 35, loss = 0.30402676\nIteration 36, loss = 0.30431431\nIteration 37, loss = 0.30274696\nIteration 38, loss = 0.30235021\nIteration 39, loss = 0.30197578\nIteration 40, loss = 0.30092598\nIteration 41, loss = 0.30028224\nIteration 42, loss = 0.29936889\nIteration 43, loss = 0.29904607\nIteration 44, loss = 0.29825328\nIteration 45, loss = 0.29715513\nIteration 46, loss = 0.29649103\nIteration 47, loss = 0.29579664\nIteration 48, loss = 0.29546125\nIteration 49, loss = 0.29477466\nIteration 50, loss = 0.29444637\nIteration 51, loss = 0.29388850\nIteration 52, loss = 0.29234595\nIteration 53, loss = 0.29253948\nIteration 54, loss = 0.29177324\nIteration 55, loss = 0.29031904\nIteration 56, loss = 0.29052333\nIteration 57, loss = 0.28945704\nIteration 58, loss = 0.28951327\nIteration 59, loss = 0.28911966\nIteration 60, loss = 0.28784548\nIteration 61, loss = 0.28736039\nIteration 62, loss = 0.28743308\nIteration 63, loss = 0.28605544\nIteration 64, loss = 0.28537778\nIteration 65, loss = 0.28502866\nIteration 66, loss = 0.28445714\nIteration 67, loss = 0.28420405\nIteration 68, loss = 0.28377890\nIteration 69, loss = 0.28375386\nIteration 70, loss = 0.28296762\nIteration 71, loss = 0.28169860\nIteration 72, loss = 0.28180775\nIteration 73, loss = 0.28177253\nIteration 74, loss = 0.28085010\nIteration 75, loss = 0.28014035\nIteration 76, loss = 0.27968195\nIteration 77, loss = 0.27936540\nIteration 78, loss = 0.27913563\nIteration 79, loss = 0.27899991\nIteration 80, loss = 0.27768436\nIteration 81, loss = 0.27808895\nIteration 82, loss = 0.27647656\nIteration 83, loss = 0.27632767\nIteration 84, loss = 0.27658090\nIteration 85, loss = 0.27558948\nIteration 86, loss = 0.27521934\nIteration 87, loss = 0.27451400\nIteration 88, loss = 0.27476489\nIteration 89, loss = 0.27420878\nIteration 90, loss = 0.27343776\nIteration 91, loss = 0.27349011\nIteration 92, loss = 0.27276374\nIteration 93, loss = 0.27298262\nIteration 94, loss = 0.27237402\nIteration 95, loss = 0.27197348\nIteration 96, loss = 0.27193506\nIteration 97, loss = 0.27160106\nIteration 98, loss = 0.27140792\nIteration 99, loss = 0.26948335\nIteration 100, loss = 0.27019925\nIteration 101, loss = 0.26988481\nIteration 102, loss = 0.26866214\nIteration 103, loss = 0.26925792\nIteration 104, loss = 0.26887765\nIteration 105, loss = 0.26849823\nIteration 106, loss = 0.26828642\nIteration 107, loss = 0.26745904\nIteration 108, loss = 0.26735213\nIteration 109, loss = 0.26728374\nIteration 110, loss = 0.26610519\nIteration 111, loss = 0.26675213\nIteration 112, loss = 0.26625007\nIteration 113, loss = 0.26512181\nIteration 114, loss = 0.26545217\nIteration 115, loss = 0.26506429\nIteration 116, loss = 0.26530844\nIteration 117, loss = 0.26457295\nIteration 118, loss = 0.26428865\nIteration 119, loss = 0.26403693\nIteration 120, loss = 0.26413362\nIteration 121, loss = 0.26364945\nIteration 122, loss = 0.26294632\nIteration 123, loss = 0.26282685\nIteration 124, loss = 0.26184849\nIteration 125, loss = 0.26276961\nIteration 126, loss = 0.26148221\nIteration 127, loss = 0.26189433\nIteration 128, loss = 0.26128264\nIteration 129, loss = 0.26163758\nIteration 130, loss = 0.26141028\nIteration 131, loss = 0.25972479\nIteration 132, loss = 0.25991550\nIteration 133, loss = 0.26063395\nIteration 134, loss = 0.25932474\nIteration 135, loss = 0.25888608\nIteration 136, loss = 0.25859431\nIteration 137, loss = 0.25905956\nIteration 138, loss = 0.25833112\nIteration 139, loss = 0.25777107\nIteration 140, loss = 0.25796645\nIteration 141, loss = 0.25730776\nIteration 142, loss = 0.25681805\nIteration 143, loss = 0.25741838\nIteration 144, loss = 0.25735179\nIteration 145, loss = 0.25684348\nIteration 146, loss = 0.25596596\nIteration 147, loss = 0.25675378\nIteration 148, loss = 0.25604036\nIteration 149, loss = 0.25474560\nIteration 150, loss = 0.25510343\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.ylabel('Log loss')\nplt.xlabel('Epochs')\nplt.plot(np.arange(1,151), train_losses, label = 'Train')\nplt.plot(np.arange(1,151), test_losses, label = 'Test')  \nplt.legend(['Train', 'Test'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-04T21:18:59.143502Z","iopub.execute_input":"2022-12-04T21:18:59.144816Z","iopub.status.idle":"2022-12-04T21:18:59.387116Z","shell.execute_reply.started":"2022-12-04T21:18:59.144760Z","shell.execute_reply":"2022-12-04T21:18:59.385737Z"},"trusted":true},"execution_count":54,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2022-12-04T21:19:14.011786Z","iopub.execute_input":"2022-12-04T21:19:14.012158Z","iopub.status.idle":"2022-12-04T21:19:14.017246Z","shell.execute_reply.started":"2022-12-04T21:19:14.012128Z","shell.execute_reply":"2022-12-04T21:19:14.016107Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"y_pred = mlc.predict(x_test)\nprint(classification_report(y_test, y_pred, target_names = ['No Rain', 'Rain']))","metadata":{"execution":{"iopub.status.busy":"2022-12-04T21:19:16.031025Z","iopub.execute_input":"2022-12-04T21:19:16.031438Z","iopub.status.idle":"2022-12-04T21:19:16.345925Z","shell.execute_reply.started":"2022-12-04T21:19:16.031403Z","shell.execute_reply":"2022-12-04T21:19:16.344374Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n     No Rain       0.88      0.91      0.90     22083\n        Rain       0.65      0.58      0.61      6346\n\n    accuracy                           0.84     28429\n   macro avg       0.77      0.74      0.75     28429\nweighted avg       0.83      0.84      0.83     28429\n\n","output_type":"stream"}]}]}